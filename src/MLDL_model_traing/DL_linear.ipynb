{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "from torch.nn import Linear, Dropout, ReLU\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import MSELoss, HuberLoss\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from Module.MySQL_connector import MySQLConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "class SimpleTrainer:\n",
    "    \"\"\"A model trainer for regression models.\"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = self.to_gpu(model)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def to_gpu(self, obj, device=\"cuda:0\"):\n",
    "        \"\"\"將張量或模型送至GPU。\"\"\"\n",
    "        return obj.to(device)\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"訓練一個epoch，並計算 R²。\"\"\"\n",
    "        self.model.train()  # 設定成訓練模式\n",
    "        total_loss = 0  # 累積所有 batch 的損失\n",
    "        total_r2 = 0  # 累積所有 batch 的 R²\n",
    "\n",
    "        for iteration, (batch_x, batch_y) in enumerate(dataloader):\n",
    "            batch_x = self.to_gpu(batch_x)\n",
    "            batch_y = self.to_gpu(batch_y)\n",
    "\n",
    "            self.optimizer.zero_grad()  # 清空模型內所有權重的梯度\n",
    "\n",
    "            pred_y = self.model(batch_x)  # 正向傳遞得到模型預測\n",
    "\n",
    "            loss_value = self.loss_fn(pred_y, batch_y)  # 計算損失\n",
    "\n",
    "            # for weight in self.model.parameters():\n",
    "            #     loss_value += 0.01* torch.sum(weight**2)\n",
    "            #     # print(loss_value,  torch.sum(weight)**2)\n",
    "\n",
    "            loss_value.backward()  # 反向傳遞\n",
    "            self.optimizer.step()  # 更新權重\n",
    "\n",
    "            total_loss += loss_value.item()  # 累積損失\n",
    "\n",
    "            # 计算 R²\n",
    "            ss_total = torch.sum((batch_y - torch.mean(batch_y)) ** 2)\n",
    "            ss_residual = torch.sum((batch_y - pred_y) ** 2)\n",
    "            # r2 = 1 - ss_residual / ss_total\n",
    "            \n",
    "            if ss_total.item() == 0:\n",
    "                r2 = 0  # 防止分母为0\n",
    "            else:\n",
    "                r2 = 1 - ss_residual / ss_total\n",
    "            total_r2 += r2.item()\n",
    "        avg_loss = total_loss / len(dataloader)  # 計算平均損失\n",
    "        avg_r2 = total_r2 / len(dataloader)  # 平均 R²\n",
    "        print(f\"Train loss: {avg_loss:.4f}, Train R²: {avg_r2:.4f}\")\n",
    "        return avg_loss, avg_r2\n",
    "\n",
    "    def test_step(self, dataloader, mode=\"test\"):\n",
    "        \"\"\"結束一個epoch的訓練後，測試模型表現。\"\"\"\n",
    "        self.model.eval()  # 設定成推理模式\n",
    "\n",
    "        size = len(dataloader.dataset)\n",
    "        test_loss = 0\n",
    "        total_r2 = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in dataloader:\n",
    "                batch_x = self.to_gpu(batch_x)\n",
    "                batch_y = self.to_gpu(batch_y)\n",
    "\n",
    "                pred_y = self.model(batch_x)\n",
    "\n",
    "                # 计算损失\n",
    "                test_loss += self.loss_fn(pred_y, batch_y).item()\n",
    "\n",
    "                # 计算 R²\n",
    "                ss_total = torch.sum((batch_y - torch.mean(batch_y)) ** 2)\n",
    "                ss_residual = torch.sum((batch_y - pred_y) ** 2)\n",
    "                r2 = 1 - ss_residual / ss_total\n",
    "                total_r2 += r2.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(dataloader)  # 計算平均測試損失\n",
    "        avg_r2 = total_r2 / len(dataloader)  # 平均 R²\n",
    "        print(f\"{mode}_loss={avg_test_loss:.4f}, {mode}_R²={avg_r2:.4f}\")\n",
    "        return avg_test_loss, avg_r2\n",
    "\n",
    "    def fit(self, dataloader_train, dataloader_test, num_epochs):\n",
    "        # 開始訓練\n",
    "        metrics = {\"train_loss\": [], \"test_loss\": [], \"train_r2\": [], \"test_r2\": []}\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            train_loss, train_r2 = self.train_step(dataloader_train)\n",
    "            test_loss, test_r2 = self.test_step(dataloader_test)\n",
    "\n",
    "            metrics[\"train_loss\"].append(train_loss)\n",
    "            metrics[\"test_loss\"].append(test_loss)\n",
    "            metrics[\"train_r2\"].append(train_r2)\n",
    "            metrics[\"test_r2\"].append(test_r2)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.model.eval()  # 啟動推理模式\n",
    "        return self.model(x)  # 執行推理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.5.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0,>1.20.0 in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from torchmetrics) (1.26.3)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from torchmetrics) (2.4.1+cu118)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Downloading torchmetrics-1.5.1-py3-none-any.whl (890 kB)\n",
      "   ---------------------------------------- 0.0/890.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/890.6 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 524.3/890.6 kB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 786.4/890.6 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 890.6/890.6 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.8 torchmetrics-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torchmetrics import R2Score  # 引入 torchmetrics\n",
    "\n",
    "class SimpleTrainer:\n",
    "    \"\"\"A model trainer for regression models.\"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = self.to_gpu(model)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.r2_metric = R2Score().to(\"cuda:0\")  # 初始化 R2Score\n",
    "\n",
    "    def to_gpu(self, obj, device=\"cuda:0\"):\n",
    "        \"\"\"將張量或模型送至GPU。\"\"\"\n",
    "        return obj.to(device)\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"訓練一個epoch，並計算 R²。\"\"\"\n",
    "        self.model.train()  # 設定成訓練模式\n",
    "        total_loss = 0  # 累積所有 batch 的損失\n",
    "        total_r2 = 0  # 累積所有 batch 的 R²\n",
    "\n",
    "        for iteration, (batch_x, batch_y) in enumerate(dataloader):\n",
    "            batch_x = self.to_gpu(batch_x)\n",
    "            batch_y = self.to_gpu(batch_y)\n",
    "\n",
    "            self.optimizer.zero_grad()  # 清空模型內所有權重的梯度\n",
    "\n",
    "            pred_y = self.model(batch_x)  # 正向傳遞得到模型預測\n",
    "              # 去掉多餘的維度\n",
    "            pred_y = pred_y.squeeze(-1)\n",
    "            loss_value = self.loss_fn(pred_y, batch_y)  # 計算損失\n",
    "\n",
    "            loss_value.backward()  # 反向傳遞\n",
    "            self.optimizer.step()  # 更新權重\n",
    "\n",
    "            total_loss += loss_value.item()  # 累積損失\n",
    "            \n",
    "            # 使用 torchmetrics 計算 R²\n",
    "            r2 = self.r2_metric(pred_y, batch_y)\n",
    "            total_r2 += r2.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)  # 計算平均損失\n",
    "        avg_r2 = total_r2 / len(dataloader)  # 平均 R²\n",
    "        print(f\"Train loss: {avg_loss:.4f}, Train R²: {avg_r2:.4f}\")\n",
    "        return avg_loss, avg_r2\n",
    "\n",
    "    def test_step(self, dataloader, mode=\"test\"):\n",
    "        \"\"\"結束一個epoch的訓練後，測試模型表現。\"\"\"\n",
    "        self.model.eval()  # 設定成推理模式\n",
    "\n",
    "        test_loss = 0\n",
    "        total_r2 = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in dataloader:\n",
    "                batch_x = self.to_gpu(batch_x)\n",
    "                batch_y = self.to_gpu(batch_y)\n",
    "\n",
    "                pred_y = self.model(batch_x)\n",
    "\n",
    "                # 计算损失\n",
    "                test_loss += self.loss_fn(pred_y, batch_y).item()\n",
    "\n",
    "                # 使用 torchmetrics 計算 R²\n",
    "                r2 = self.r2_metric(pred_y, batch_y)\n",
    "                total_r2 += r2.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(dataloader)  # 計算平均測試損失\n",
    "        avg_r2 = total_r2 / len(dataloader)  # 平均 R²\n",
    "        print(f\"{mode}_loss={avg_test_loss:.4f}, {mode}_R²={avg_r2:.4f}\")\n",
    "        return avg_test_loss, avg_r2\n",
    "\n",
    "    def fit(self, dataloader_train, dataloader_test, num_epochs):\n",
    "        # 開始訓練\n",
    "        metrics = {\"train_loss\": [], \"test_loss\": [], \"train_r2\": [], \"test_r2\": []}\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            train_loss, train_r2 = self.train_step(dataloader_train)\n",
    "            test_loss, test_r2 = self.test_step(dataloader_test)\n",
    "\n",
    "            metrics[\"train_loss\"].append(train_loss)\n",
    "            metrics[\"test_loss\"].append(test_loss)\n",
    "            metrics[\"train_r2\"].append(train_r2)\n",
    "            metrics[\"test_r2\"].append(test_r2)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.model.eval()  # 啟動推理模式\n",
    "        return self.model(x)  # 執行推理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to g3_db on 192.168.31.130:32010\n"
     ]
    }
   ],
   "source": [
    "# import data from MySQL\n",
    "sql_connector = MySQLConnector('192.168.31.130', 'bigred', 'bigred', 32010)\n",
    "sql_connector.connectDB('g3_db')\n",
    "# print(sql_connector.execute_query('show tables;'))\n",
    "sql_connector.execute_query('show tables;')\n",
    "main_table = pd.DataFrame(sql_connector.execute_query(\"select * from main_aa\"))\n",
    "genres_onehot = pd.DataFrame(sql_connector.execute_query(\"select * from genres_onehot_aa\"))\n",
    "features_onehot = pd.DataFrame(sql_connector.execute_query(\"select * from features_onehot_aa\"))\n",
    "tags_onehot = pd.DataFrame(sql_connector.execute_query(\"select * from tags_onehot_aa\"))\n",
    "id_publisher_aa = pd.DataFrame(sql_connector.execute_query(\"select * from id_publishers_aa\"))\n",
    "publishers_info_aa = pd.DataFrame(sql_connector.execute_query(\"select * from publishers_info_aa;\"))\n",
    "copiesold = pd.DataFrame(sql_connector.execute_query(\"select * from first30days_sales_2A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併時指定 suffixes 參數來處理重複列\n",
    "merged_table = pd.merge(main_table, genres_onehot, on='steamId', suffixes=('', '_genres'))\n",
    "merged_table = pd.merge(merged_table, tags_onehot, on='steamId', suffixes=('', '_tags'))\n",
    "# 再次合併時，處理更多的重複列\n",
    "merged_table = pd.merge(merged_table, features_onehot, on='steamId', suffixes=('', '_features'))\n",
    "# 找到所有帶有 '_tags' 或 '_features' 後綴的列名\n",
    "replicated_columns = [col for col in merged_table.columns if '_tags' in col or '_features' in col]\n",
    "\n",
    "# 刪除這些列\n",
    "merged_table.drop(columns=replicated_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_pub = pd.merge(copiesold,id_publisher_aa,on=\"steamId\")\n",
    "# cs_pub_info = pd.merge(cs_pub, publishers_info_aa, on=\"steamId\")\n",
    "cs_pub_info = pd.merge(cs_pub, publishers_info_aa, left_on='publisher', right_on=\"name\")\n",
    "cs_pub_info\n",
    "cs_pub_info=cs_pub_info.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = pd.merge(cs_pub_info, merged_table, on=\"steamId\",suffixes=('', '_pub'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['steamId','totalRevenue_pub','publisher','name_pub','name','reviews','reviewsSteam','followers','avgPlaytime','reviewScore','releaseDate','firstReleaseDate','copiesSold','revenue','totalRevenue','players','owners','wishlists']\n",
    "full_table=full_table.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1244 entries, 0 to 1243\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   publishedGames  1244 non-null   int64  \n",
      " 1   medianRevenue   1244 non-null   float64\n",
      " 2   averageRevenue  1244 non-null   float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 29.3 KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "full_table = full_table.dropna()\n",
    "full_table[['publishedGames','medianRevenue','averageRevenue']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sales            timeStamp  days_since_release  averageRevenue  \\\n",
      "0   0.000000  2019-09-02 15:00:00                  30        0.011630   \n",
      "1  10.949297  2019-09-02 15:00:00                  30        0.011630   \n",
      "2  12.098669  2015-07-17 13:00:00                  30        0.100861   \n",
      "3  13.910367  2015-05-05 09:00:00                  30        0.054163   \n",
      "4  10.895628  2016-06-01 20:00:00                  30        0.027177   \n",
      "\n",
      "   medianRevenue  publishedGames     price  releaseDate_pub  earlyAccess  \\\n",
      "0       0.313861        0.920853  0.074884    1567137600000            0   \n",
      "1       0.313861        0.920853  0.074884    1567137600000            0   \n",
      "2       0.729617        0.333030  0.249906    1435032000000            0   \n",
      "3       0.466836        0.396728  0.499937    1430107200000            0   \n",
      "4       0.484056        0.639981  0.249906    1463457600000            0   \n",
      "\n",
      "   Action  ...  Steam Cloud  Steam Leaderboards  Steam Trading Cards  \\\n",
      "0       1  ...            0                   0                    0   \n",
      "1       1  ...            0                   0                    0   \n",
      "2       1  ...            1                   1                    1   \n",
      "3       0  ...            1                   0                    1   \n",
      "4       1  ...            0                   0                    1   \n",
      "\n",
      "   Steam Turn Notifications  Steam Workshop  SteamVR Collectibles  \\\n",
      "0                         0               0                     0   \n",
      "1                         0               0                     0   \n",
      "2                         0               0                     0   \n",
      "3                         0               1                     0   \n",
      "4                         0               0                     0   \n",
      "\n",
      "   Tracked Controller Support  VR Only  VR Supported  Valve Anti-Cheat enabled  \n",
      "0                           0        0             0                         0  \n",
      "1                           0        0             0                         0  \n",
      "2                           0        0             0                         0  \n",
      "3                           0        0             0                         0  \n",
      "4                           0        0             0                         0  \n",
      "\n",
      "[5 rows x 469 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# 初始化 MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "full_table['sales'] = np.log1p(full_table['sales'])\n",
    "full_table['publishedGames'] = np.log1p(full_table['publishedGames'])\n",
    "full_table['medianRevenue'] = np.log1p(full_table['medianRevenue'])\n",
    "# 選擇要正規化的欄位\n",
    "columns_to_scale = [\"price\", \"publishedGames\", \"medianRevenue\", \"averageRevenue\"]\n",
    "\n",
    "# 對指定欄位進行 Min-Max 轉換\n",
    "full_table[columns_to_scale] = scaler.fit_transform(full_table[columns_to_scale])\n",
    "# full_table['sales'] = np.log1p(full_table['sales'])\n",
    "# full_table 中的這些欄位現在已經進行 Min-Max 正規化\n",
    "print(full_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = full_table.drop(columns=[\"days_since_release\",\"releaseDate_pub\",\"timeStamp\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = pd.read_csv(\"../../data/processed/full_data_for_ml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = full_table.drop(columns=\"sales\").values\n",
    "Y_full = full_table['sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>averageRevenue</th>\n",
       "      <th>medianRevenue</th>\n",
       "      <th>publishedGames</th>\n",
       "      <th>price</th>\n",
       "      <th>earlyAccess</th>\n",
       "      <th>Accounting</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation &amp; Modeling</th>\n",
       "      <th>...</th>\n",
       "      <th>Steam Timeline</th>\n",
       "      <th>Steam Trading Cards</th>\n",
       "      <th>Steam Turn Notifications</th>\n",
       "      <th>Steam Workshop</th>\n",
       "      <th>SteamVR Collectibles</th>\n",
       "      <th>Tracked Controller Support</th>\n",
       "      <th>VR Only</th>\n",
       "      <th>VR Supported</th>\n",
       "      <th>Valve Anti-Cheat enabled</th>\n",
       "      <th>indie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.404149</td>\n",
       "      <td>0.453316</td>\n",
       "      <td>0.587113</td>\n",
       "      <td>0.466248</td>\n",
       "      <td>0.451984</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.475520</td>\n",
       "      <td>0.762018</td>\n",
       "      <td>0.493235</td>\n",
       "      <td>0.227723</td>\n",
       "      <td>0.647464</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.207449</td>\n",
       "      <td>0.333886</td>\n",
       "      <td>0.521493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.663562</td>\n",
       "      <td>0.450807</td>\n",
       "      <td>0.620089</td>\n",
       "      <td>0.166560</td>\n",
       "      <td>0.451984</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.548621</td>\n",
       "      <td>0.762480</td>\n",
       "      <td>0.829376</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.573996</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17785</th>\n",
       "      <td>3.610918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.656380</td>\n",
       "      <td>0.129757</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17786</th>\n",
       "      <td>4.343805</td>\n",
       "      <td>0.234036</td>\n",
       "      <td>0.347340</td>\n",
       "      <td>0.479719</td>\n",
       "      <td>0.260932</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17787</th>\n",
       "      <td>7.483244</td>\n",
       "      <td>0.271885</td>\n",
       "      <td>0.492761</td>\n",
       "      <td>0.125998</td>\n",
       "      <td>0.303104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17788</th>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.214907</td>\n",
       "      <td>0.398674</td>\n",
       "      <td>0.498221</td>\n",
       "      <td>0.337546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17789</th>\n",
       "      <td>2.639057</td>\n",
       "      <td>0.222613</td>\n",
       "      <td>0.386061</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.510513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17790 rows × 488 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sales  averageRevenue  medianRevenue  publishedGames     price  \\\n",
       "0       8.404149        0.453316       0.587113        0.466248  0.451984   \n",
       "1      13.475520        0.762018       0.493235        0.227723  0.647464   \n",
       "2       6.207449        0.333886       0.521493        0.000000  0.337546   \n",
       "3       3.663562        0.450807       0.620089        0.166560  0.451984   \n",
       "4      11.548621        0.762480       0.829376        0.073704  0.573996   \n",
       "...          ...             ...            ...             ...       ...   \n",
       "17785   3.610918        0.000000       0.186124        0.656380  0.129757   \n",
       "17786   4.343805        0.234036       0.347340        0.479719  0.260932   \n",
       "17787   7.483244        0.271885       0.492761        0.125998  0.303104   \n",
       "17788   2.772589        0.214907       0.398674        0.498221  0.337546   \n",
       "17789   2.639057        0.222613       0.386061        1.000000  0.510513   \n",
       "\n",
       "       earlyAccess  Accounting  Action  Adventure  Animation & Modeling  ...  \\\n",
       "0                0           0       0          0                     0  ...   \n",
       "1                0           0       0          0                     0  ...   \n",
       "2                0           0       0          0                     0  ...   \n",
       "3                0           0       0          0                     0  ...   \n",
       "4                0           0       1          1                     0  ...   \n",
       "...            ...         ...     ...        ...                   ...  ...   \n",
       "17785            0           0       1          0                     0  ...   \n",
       "17786            0           0       0          0                     0  ...   \n",
       "17787            0           0       0          0                     0  ...   \n",
       "17788            0           0       0          0                     0  ...   \n",
       "17789            0           0       0          1                     0  ...   \n",
       "\n",
       "       Steam Timeline  Steam Trading Cards  Steam Turn Notifications  \\\n",
       "0                   0                    1                         0   \n",
       "1                   0                    1                         0   \n",
       "2                   0                    0                         0   \n",
       "3                   0                    0                         0   \n",
       "4                   0                    1                         0   \n",
       "...               ...                  ...                       ...   \n",
       "17785               0                    0                         0   \n",
       "17786               0                    0                         0   \n",
       "17787               0                    0                         0   \n",
       "17788               0                    0                         0   \n",
       "17789               0                    0                         0   \n",
       "\n",
       "       Steam Workshop  SteamVR Collectibles  Tracked Controller Support  \\\n",
       "0                   0                     0                           0   \n",
       "1                   1                     0                           0   \n",
       "2                   0                     0                           0   \n",
       "3                   0                     0                           0   \n",
       "4                   0                     0                           0   \n",
       "...               ...                   ...                         ...   \n",
       "17785               0                     0                           0   \n",
       "17786               0                     0                           0   \n",
       "17787               0                     0                           0   \n",
       "17788               0                     0                           0   \n",
       "17789               0                     0                           0   \n",
       "\n",
       "       VR Only  VR Supported  Valve Anti-Cheat enabled  indie  \n",
       "0            0             0                         0   True  \n",
       "1            0             0                         0   True  \n",
       "2            0             0                         0   True  \n",
       "3            0             0                         0   True  \n",
       "4            0             0                         0   True  \n",
       "...        ...           ...                       ...    ...  \n",
       "17785        0             0                         0   True  \n",
       "17786        0             0                         0   True  \n",
       "17787        0             0                         0   True  \n",
       "17788        0             0                         0   True  \n",
       "17789        0             0                         0   True  \n",
       "\n",
       "[17790 rows x 488 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = full_table.drop(columns=[\"steamId\",\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table_no_class = full_table.iloc[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_full = full_table_no_class.drop(columns=\"sales\").values\n",
    "Y_full = full_table_no_class['sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. 應用 PCA\n",
    "pca = PCA(n_components=0.95)  # 保留 95% 的變異性\n",
    "X_pca = pca.fit_transform(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.958634</td>\n",
       "      <td>1.381461</td>\n",
       "      <td>0.393285</td>\n",
       "      <td>0.331731</td>\n",
       "      <td>-1.208069</td>\n",
       "      <td>-0.459941</td>\n",
       "      <td>-0.229845</td>\n",
       "      <td>-0.986522</td>\n",
       "      <td>-0.502040</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>-0.137273</td>\n",
       "      <td>-0.119302</td>\n",
       "      <td>-0.065011</td>\n",
       "      <td>-0.051771</td>\n",
       "      <td>0.086889</td>\n",
       "      <td>0.018763</td>\n",
       "      <td>0.080927</td>\n",
       "      <td>0.104578</td>\n",
       "      <td>-0.010458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.958634</td>\n",
       "      <td>1.381461</td>\n",
       "      <td>0.393285</td>\n",
       "      <td>0.331731</td>\n",
       "      <td>-1.208069</td>\n",
       "      <td>-0.459941</td>\n",
       "      <td>-0.229845</td>\n",
       "      <td>-0.986522</td>\n",
       "      <td>-0.502040</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>-0.137273</td>\n",
       "      <td>-0.119302</td>\n",
       "      <td>-0.065011</td>\n",
       "      <td>-0.051771</td>\n",
       "      <td>0.086889</td>\n",
       "      <td>0.018763</td>\n",
       "      <td>0.080927</td>\n",
       "      <td>0.104578</td>\n",
       "      <td>-0.010458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.053934</td>\n",
       "      <td>0.743290</td>\n",
       "      <td>0.117340</td>\n",
       "      <td>-0.976509</td>\n",
       "      <td>-0.080199</td>\n",
       "      <td>-1.026414</td>\n",
       "      <td>0.939825</td>\n",
       "      <td>-0.474286</td>\n",
       "      <td>0.056530</td>\n",
       "      <td>0.736193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089179</td>\n",
       "      <td>0.041706</td>\n",
       "      <td>-0.052216</td>\n",
       "      <td>0.056904</td>\n",
       "      <td>0.047004</td>\n",
       "      <td>0.115403</td>\n",
       "      <td>-0.017493</td>\n",
       "      <td>-0.047417</td>\n",
       "      <td>-0.019444</td>\n",
       "      <td>-0.138522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.581212</td>\n",
       "      <td>-0.390878</td>\n",
       "      <td>0.897361</td>\n",
       "      <td>-0.611077</td>\n",
       "      <td>0.826489</td>\n",
       "      <td>-0.026942</td>\n",
       "      <td>-0.160297</td>\n",
       "      <td>-1.470252</td>\n",
       "      <td>-0.044766</td>\n",
       "      <td>0.811961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108880</td>\n",
       "      <td>0.137031</td>\n",
       "      <td>-0.007164</td>\n",
       "      <td>0.282209</td>\n",
       "      <td>-0.089450</td>\n",
       "      <td>-0.195839</td>\n",
       "      <td>0.171350</td>\n",
       "      <td>-0.052936</td>\n",
       "      <td>-0.050460</td>\n",
       "      <td>-0.092297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.142500</td>\n",
       "      <td>1.160684</td>\n",
       "      <td>-0.012679</td>\n",
       "      <td>-1.560976</td>\n",
       "      <td>-0.666818</td>\n",
       "      <td>-0.337330</td>\n",
       "      <td>0.640182</td>\n",
       "      <td>-0.088641</td>\n",
       "      <td>-0.303344</td>\n",
       "      <td>-0.196894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051213</td>\n",
       "      <td>0.023905</td>\n",
       "      <td>0.037628</td>\n",
       "      <td>-0.103024</td>\n",
       "      <td>-0.132994</td>\n",
       "      <td>0.039577</td>\n",
       "      <td>-0.112722</td>\n",
       "      <td>-0.005092</td>\n",
       "      <td>0.080923</td>\n",
       "      <td>-0.069105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>-0.295878</td>\n",
       "      <td>1.144732</td>\n",
       "      <td>1.196059</td>\n",
       "      <td>-1.025700</td>\n",
       "      <td>-0.858526</td>\n",
       "      <td>0.425925</td>\n",
       "      <td>0.260954</td>\n",
       "      <td>1.179354</td>\n",
       "      <td>-0.213090</td>\n",
       "      <td>0.025175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018486</td>\n",
       "      <td>0.347347</td>\n",
       "      <td>0.390136</td>\n",
       "      <td>0.033045</td>\n",
       "      <td>-0.091125</td>\n",
       "      <td>-0.068043</td>\n",
       "      <td>-0.134452</td>\n",
       "      <td>0.082871</td>\n",
       "      <td>-0.214248</td>\n",
       "      <td>-0.097932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>1.362531</td>\n",
       "      <td>0.195780</td>\n",
       "      <td>-0.030295</td>\n",
       "      <td>-0.700239</td>\n",
       "      <td>-0.034225</td>\n",
       "      <td>-0.494802</td>\n",
       "      <td>-0.223845</td>\n",
       "      <td>0.795331</td>\n",
       "      <td>0.749374</td>\n",
       "      <td>0.373240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094645</td>\n",
       "      <td>-0.057417</td>\n",
       "      <td>-0.141879</td>\n",
       "      <td>-0.072635</td>\n",
       "      <td>0.076430</td>\n",
       "      <td>-0.051944</td>\n",
       "      <td>0.133079</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.030252</td>\n",
       "      <td>-0.068081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>-1.086280</td>\n",
       "      <td>1.119930</td>\n",
       "      <td>-0.436171</td>\n",
       "      <td>-0.662856</td>\n",
       "      <td>-0.309996</td>\n",
       "      <td>0.034728</td>\n",
       "      <td>0.237060</td>\n",
       "      <td>0.388083</td>\n",
       "      <td>0.606347</td>\n",
       "      <td>-0.503221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248914</td>\n",
       "      <td>-0.294603</td>\n",
       "      <td>-0.086432</td>\n",
       "      <td>-0.123298</td>\n",
       "      <td>-0.099795</td>\n",
       "      <td>0.128806</td>\n",
       "      <td>0.052561</td>\n",
       "      <td>0.041710</td>\n",
       "      <td>-0.023201</td>\n",
       "      <td>-0.072632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>0.670831</td>\n",
       "      <td>0.672811</td>\n",
       "      <td>-0.279929</td>\n",
       "      <td>0.056136</td>\n",
       "      <td>0.131138</td>\n",
       "      <td>-0.163586</td>\n",
       "      <td>-0.706235</td>\n",
       "      <td>0.218882</td>\n",
       "      <td>1.231441</td>\n",
       "      <td>0.746694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>-0.148918</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>-0.000926</td>\n",
       "      <td>-0.023227</td>\n",
       "      <td>0.078115</td>\n",
       "      <td>-0.118050</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>0.007409</td>\n",
       "      <td>0.045811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>0.751070</td>\n",
       "      <td>-0.316942</td>\n",
       "      <td>1.420399</td>\n",
       "      <td>-0.795311</td>\n",
       "      <td>-0.075680</td>\n",
       "      <td>-0.904795</td>\n",
       "      <td>0.803759</td>\n",
       "      <td>0.863035</td>\n",
       "      <td>0.467163</td>\n",
       "      <td>-0.835325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040437</td>\n",
       "      <td>0.216892</td>\n",
       "      <td>0.189835</td>\n",
       "      <td>-0.089033</td>\n",
       "      <td>-0.175435</td>\n",
       "      <td>-0.054679</td>\n",
       "      <td>0.101485</td>\n",
       "      <td>-0.016919</td>\n",
       "      <td>-0.179628</td>\n",
       "      <td>0.167000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1244 rows × 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.958634  1.381461  0.393285  0.331731 -1.208069 -0.459941 -0.229845   \n",
       "1    -0.958634  1.381461  0.393285  0.331731 -1.208069 -0.459941 -0.229845   \n",
       "2     1.053934  0.743290  0.117340 -0.976509 -0.080199 -1.026414  0.939825   \n",
       "3    -0.581212 -0.390878  0.897361 -0.611077  0.826489 -0.026942 -0.160297   \n",
       "4    -0.142500  1.160684 -0.012679 -1.560976 -0.666818 -0.337330  0.640182   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1239 -0.295878  1.144732  1.196059 -1.025700 -0.858526  0.425925  0.260954   \n",
       "1240  1.362531  0.195780 -0.030295 -0.700239 -0.034225 -0.494802 -0.223845   \n",
       "1241 -1.086280  1.119930 -0.436171 -0.662856 -0.309996  0.034728  0.237060   \n",
       "1242  0.670831  0.672811 -0.279929  0.056136  0.131138 -0.163586 -0.706235   \n",
       "1243  0.751070 -0.316942  1.420399 -0.795311 -0.075680 -0.904795  0.803759   \n",
       "\n",
       "           7         8         9    ...       210       211       212  \\\n",
       "0    -0.986522 -0.502040  0.001096  ...  0.019197 -0.137273 -0.119302   \n",
       "1    -0.986522 -0.502040  0.001096  ...  0.019197 -0.137273 -0.119302   \n",
       "2    -0.474286  0.056530  0.736193  ... -0.089179  0.041706 -0.052216   \n",
       "3    -1.470252 -0.044766  0.811961  ... -0.108880  0.137031 -0.007164   \n",
       "4    -0.088641 -0.303344 -0.196894  ... -0.051213  0.023905  0.037628   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1239  1.179354 -0.213090  0.025175  ...  0.018486  0.347347  0.390136   \n",
       "1240  0.795331  0.749374  0.373240  ...  0.094645 -0.057417 -0.141879   \n",
       "1241  0.388083  0.606347 -0.503221  ... -0.248914 -0.294603 -0.086432   \n",
       "1242  0.218882  1.231441  0.746694  ...  0.018647 -0.148918  0.009707   \n",
       "1243  0.863035  0.467163 -0.835325  ...  0.040437  0.216892  0.189835   \n",
       "\n",
       "           213       214       215       216       217       218       219  \n",
       "0    -0.065011 -0.051771  0.086889  0.018763  0.080927  0.104578 -0.010458  \n",
       "1    -0.065011 -0.051771  0.086889  0.018763  0.080927  0.104578 -0.010458  \n",
       "2     0.056904  0.047004  0.115403 -0.017493 -0.047417 -0.019444 -0.138522  \n",
       "3     0.282209 -0.089450 -0.195839  0.171350 -0.052936 -0.050460 -0.092297  \n",
       "4    -0.103024 -0.132994  0.039577 -0.112722 -0.005092  0.080923 -0.069105  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1239  0.033045 -0.091125 -0.068043 -0.134452  0.082871 -0.214248 -0.097932  \n",
       "1240 -0.072635  0.076430 -0.051944  0.133079  0.063694  0.030252 -0.068081  \n",
       "1241 -0.123298 -0.099795  0.128806  0.052561  0.041710 -0.023201 -0.072632  \n",
       "1242 -0.000926 -0.023227  0.078115 -0.118050  0.090323  0.007409  0.045811  \n",
       "1243 -0.089033 -0.175435 -0.054679  0.101485 -0.016919 -0.179628  0.167000  \n",
       "\n",
       "[1244 rows x 220 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "float64\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 25)\n",
    "# X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, Y_full, test_size = 0.3, random_state = 25)。。\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, Y_full, test_size = 0.3, random_state = 25)\n",
    "print(X_train_full.dtype)\n",
    "print(y_train_full.dtype)\n",
    "X_train_full = np.array(X_train_full, dtype=np.float32)\n",
    "y_train_full = np.array(y_train_full, dtype=np.float32)\n",
    "X_test_full = np.array(X_test_full, dtype=np.float32)\n",
    "y_test_full = np.array(y_test_full, dtype=np.float32)\n",
    "print(X_train_full.dtype)\n",
    "print(y_train_full.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = torch.tensor(X_train_full), torch.tensor(y_train_full)\n",
    "x_test, y_test = torch.tensor(X_test_full), torch.tensor(y_test_full)\n",
    "x_train = x_train.to(torch.float32)\n",
    "x_test = x_test.to(torch.float32)\n",
    "y_train = y_train.to(torch.float32)\n",
    "y_test = y_test.to(torch.float32)\n",
    "# 將PyTorch tensor 製作成 Data Loader\n",
    "td = TensorDataset(x_train, y_train)\n",
    "dl_train = DataLoader(td, batch_size=128, shuffle=True)\n",
    "# Shuffle 隨機打亂數據，防止模型過度依賴數據的順序，從而提高模型的泛化能力。\n",
    "td = TensorDataset(x_test, y_test)\n",
    "dl_test = DataLoader(td, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 487]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in dl_train:\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:(12453, 487)\n",
      "Shape of X_test:(5337, 487)\n",
      "Shape of y_train:(12453,)\n",
      "Shape of y_test(5337,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X_train:{X_train_full.shape}\")\n",
    "print(f\"Shape of X_test:{X_test_full.shape}\")\n",
    "print(f\"Shape of y_train:{y_train_full.shape}\")\n",
    "print(f\"Shape of y_test{y_test_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集大小: 800\n",
      "測試集大小: 200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "# 生成測試數據 x = y\n",
    "n_samples = 1000  # 總樣本數量\n",
    "x_data = torch.linspace(0, 1, n_samples).reshape(-1, 1)  # 創建 x 為 0 到 1 的等差數列\n",
    "y_data = x_data.clone()  # y = x\n",
    "\n",
    "# 創建 TensorDataset\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "\n",
    "# 將數據集拆分為 80% 訓練集和 20% 測試集\n",
    "train_size = int(0.8 * n_samples)  # 訓練集大小\n",
    "test_size = n_samples - train_size  # 測試集大小\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "# 創建 DataLoader\n",
    "dl_train_t = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "dl_test_t = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 檢查數據集大小\n",
    "print(f\"訓練集大小: {len(dl_train.dataset)}\")\n",
    "print(f\"測試集大小: {len(dl_test.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "layers = [ \n",
    "    Linear(487, 1024),  # 增加神經元數量\n",
    "    ReLU(),\n",
    "    # nn.BatchNorm1d(1024),\n",
    "    Linear(1024, 512),\n",
    "    ReLU(),# 增加一層\n",
    "    # nn.BatchNorm1d(1024),\n",
    "    Linear(512, 256),\n",
    "    ReLU(),\n",
    "    # nn.BatchNorm1d(512),\n",
    "    Linear(256, 1)\n",
    "    # ReLU(),\n",
    "    # nn.BatchNorm1d(32),\n",
    "    # Linear(32, 32),\n",
    "    # ReLU(),\n",
    "    # nn.BatchNorm1d(32),\n",
    "    # Linear(32, 32),\n",
    "    # ReLU(),\n",
    "    # nn.BatchNorm1d(32),\n",
    "    # Linear(32, 32),\n",
    "    # ReLU(),\n",
    "    # nn.BatchNorm1d(32),\n",
    "    # Linear(32, 1)\n",
    "]\n",
    "# layers = [\n",
    "#     nn.Conv1d(in_channels=1, out_channels=50, kernel_size=3, padding=1),  # Conv1d 层替代 Linear 层\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv1d(in_channels=50, out_channels=32, kernel_size=3, padding=1), # 第二层 Conv1d\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv1d(in_channels=32, out_channels=1, kernel_size=1)  # 输出层 Conv1d\n",
    "# ]\n",
    "model = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(dl_train))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train loss: 1.2489, Train R²: 0.1476\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Predictions and targets are expected to have the same shape, but got torch.Size([128, 1]) and torch.Size([128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 開始訓練模|型\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleTrainer(model, loss, opt)\n\u001b[1;32m---> 13\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[125], line 81\u001b[0m, in \u001b[0;36mSimpleTrainer.fit\u001b[1;34m(self, dataloader_train, dataloader_test, num_epochs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m train_loss, train_r2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(dataloader_train)\n\u001b[1;32m---> 81\u001b[0m test_loss, test_r2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     84\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(test_loss)\n",
      "Cell \u001b[1;32mIn[125], line 67\u001b[0m, in \u001b[0;36mSimpleTrainer.test_step\u001b[1;34m(self, dataloader, mode)\u001b[0m\n\u001b[0;32m     64\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(pred_y, batch_y)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;66;03m# 使用 torchmetrics 計算 R²\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m         r2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr2_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m         total_r2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r2\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     70\u001b[0m avg_test_loss \u001b[38;5;241m=\u001b[39m test_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)  \u001b[38;5;66;03m# 計算平均測試損失\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages\\torchmetrics\\metric.py:312\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_reduce_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32mc:\\Users\\Nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages\\torchmetrics\\metric.py:381\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m--> 381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    382\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages\\torchmetrics\\metric.py:493\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[0;32m    486\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    487\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    488\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    491\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device corresponds to the device of the input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_on_cpu:\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[1;32mc:\\Users\\Nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages\\torchmetrics\\metric.py:483\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[1;32mc:\\Users\\Nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages\\torchmetrics\\regression\\r2.py:144\u001b[0m, in \u001b[0;36mR2Score.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     sum_squared_error, sum_error, residual, total \u001b[38;5;241m=\u001b[39m \u001b[43m_r2_score_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_squared_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_squared_error \u001b[38;5;241m+\u001b[39m sum_squared_error\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_error \u001b[38;5;241m+\u001b[39m sum_error\n",
      "File \u001b[1;32mc:\\Users\\Nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages\\torchmetrics\\functional\\regression\\r2.py:33\u001b[0m, in \u001b[0;36m_r2_score_update\u001b[1;34m(preds, target)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_r2_score_update\u001b[39m(preds: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update and returns variables required to compute R2 score.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    Check for same shape and 1D/2D input tensors.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[43m_check_same_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected both prediction and target to be 1D or 2D tensors,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but received tensors with dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Nico\\anaconda3\\envs\\dev310-mldl\\lib\\site-packages\\torchmetrics\\utilities\\checks.py:40\u001b[0m, in \u001b[0;36m_check_same_shape\u001b[1;34m(preds, target)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that predictions and target have the same shape, else raise error.\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and targets are expected to have the same shape, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Predictions and targets are expected to have the same shape, but got torch.Size([128, 1]) and torch.Size([128])."
     ]
    }
   ],
   "source": [
    "\n",
    "# 宣告模型訓練設定\n",
    "num_epochs = 50\n",
    "learning_rate = 0.005\n",
    "\n",
    "# 定義優化器, Loss函數\n",
    "# loss = MSELoss()\n",
    "loss= torch.nn.SmoothL1Loss()\n",
    "opt = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# 開始訓練模|型\n",
    "\n",
    "model = SimpleTrainer(model, loss, opt)\n",
    "metrics = model.fit(dl_train, dl_test, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200\n",
      "Train loss: 11.0782, Train R²: -242.3925\n",
      "test_loss=14.0417, test_R²=-333.0180\n",
      "Epoch 2/1200\n",
      "Train loss: 11.1800, Train R²: -240.6142\n",
      "test_loss=14.0059, test_R²=-332.1652\n",
      "Epoch 3/1200\n",
      "Train loss: 11.0563, Train R²: -239.3392\n",
      "test_loss=13.9704, test_R²=-331.3139\n",
      "Epoch 4/1200\n",
      "Train loss: 11.0908, Train R²: -239.8276\n",
      "test_loss=13.9350, test_R²=-330.4707\n",
      "Epoch 5/1200\n",
      "Train loss: 11.1064, Train R²: -242.1052\n",
      "test_loss=13.8992, test_R²=-329.6165\n",
      "Epoch 6/1200\n",
      "Train loss: 10.9891, Train R²: -239.4359\n",
      "test_loss=13.8637, test_R²=-328.7681\n",
      "Epoch 7/1200\n",
      "Train loss: 10.9774, Train R²: -242.6008\n",
      "test_loss=13.8280, test_R²=-327.9183\n",
      "Epoch 8/1200\n",
      "Train loss: 10.9930, Train R²: -236.4871\n",
      "test_loss=13.7927, test_R²=-327.0746\n",
      "Epoch 9/1200\n",
      "Train loss: 10.9515, Train R²: -236.7240\n",
      "test_loss=13.7562, test_R²=-326.2046\n",
      "Epoch 10/1200\n",
      "Train loss: 10.9469, Train R²: -236.8241\n",
      "test_loss=13.7212, test_R²=-325.3695\n",
      "Epoch 11/1200\n",
      "Train loss: 10.9958, Train R²: -238.7611\n",
      "test_loss=13.6863, test_R²=-324.5359\n",
      "Epoch 12/1200\n",
      "Train loss: 10.8666, Train R²: -236.7049\n",
      "test_loss=13.6499, test_R²=-323.6665\n",
      "Epoch 13/1200\n",
      "Train loss: 10.9106, Train R²: -232.5926\n",
      "test_loss=13.6145, test_R²=-322.8196\n",
      "Epoch 14/1200\n",
      "Train loss: 10.8505, Train R²: -236.2987\n",
      "test_loss=13.5802, test_R²=-322.0011\n",
      "Epoch 15/1200\n",
      "Train loss: 10.8174, Train R²: -235.3285\n",
      "test_loss=13.5450, test_R²=-321.1611\n",
      "Epoch 16/1200\n",
      "Train loss: 10.8790, Train R²: -234.1743\n",
      "test_loss=13.5102, test_R²=-320.3307\n",
      "Epoch 17/1200\n",
      "Train loss: 10.7701, Train R²: -236.8175\n",
      "test_loss=13.4760, test_R²=-319.5144\n",
      "Epoch 18/1200\n",
      "Train loss: 10.7650, Train R²: -235.0600\n",
      "test_loss=13.4417, test_R²=-318.6961\n",
      "Epoch 19/1200\n",
      "Train loss: 10.7053, Train R²: -231.1128\n",
      "test_loss=13.4071, test_R²=-317.8728\n",
      "Epoch 20/1200\n",
      "Train loss: 10.8022, Train R²: -229.9009\n",
      "test_loss=13.3730, test_R²=-317.0575\n",
      "Epoch 21/1200\n",
      "Train loss: 10.7242, Train R²: -232.1927\n",
      "test_loss=13.3391, test_R²=-316.2473\n",
      "Epoch 22/1200\n",
      "Train loss: 10.6954, Train R²: -230.8302\n",
      "test_loss=13.3053, test_R²=-315.4399\n",
      "Epoch 23/1200\n",
      "Train loss: 10.7012, Train R²: -233.2139\n",
      "test_loss=13.2716, test_R²=-314.6356\n",
      "Epoch 24/1200\n",
      "Train loss: 10.6243, Train R²: -228.7157\n",
      "test_loss=13.2376, test_R²=-313.8240\n",
      "Epoch 25/1200\n",
      "Train loss: 10.6318, Train R²: -229.0418\n",
      "test_loss=13.2036, test_R²=-313.0155\n",
      "Epoch 26/1200\n",
      "Train loss: 10.6335, Train R²: -232.6857\n",
      "test_loss=13.1704, test_R²=-312.2216\n",
      "Epoch 27/1200\n",
      "Train loss: 10.5765, Train R²: -229.9583\n",
      "test_loss=13.1360, test_R²=-311.4010\n",
      "Epoch 28/1200\n",
      "Train loss: 10.5606, Train R²: -229.7680\n",
      "test_loss=13.1019, test_R²=-310.5870\n",
      "Epoch 29/1200\n",
      "Train loss: 10.5721, Train R²: -227.1273\n",
      "test_loss=13.0684, test_R²=-309.7864\n",
      "Epoch 30/1200\n",
      "Train loss: 10.5598, Train R²: -226.5854\n",
      "test_loss=13.0349, test_R²=-308.9884\n",
      "Epoch 31/1200\n",
      "Train loss: 10.5679, Train R²: -226.2071\n",
      "test_loss=13.0015, test_R²=-308.1918\n",
      "Epoch 32/1200\n",
      "Train loss: 10.5176, Train R²: -225.4023\n",
      "test_loss=12.9683, test_R²=-307.4014\n",
      "Epoch 33/1200\n",
      "Train loss: 10.4942, Train R²: -225.7603\n",
      "test_loss=12.9357, test_R²=-306.6227\n",
      "Epoch 34/1200\n",
      "Train loss: 10.4185, Train R²: -223.7758\n",
      "test_loss=12.9027, test_R²=-305.8318\n",
      "Epoch 35/1200\n",
      "Train loss: 10.4475, Train R²: -224.4812\n",
      "test_loss=12.8688, test_R²=-305.0223\n",
      "Epoch 36/1200\n",
      "Train loss: 10.3877, Train R²: -223.8971\n",
      "test_loss=12.8356, test_R²=-304.2315\n",
      "Epoch 37/1200\n",
      "Train loss: 10.3676, Train R²: -222.5257\n",
      "test_loss=12.8030, test_R²=-303.4538\n",
      "Epoch 38/1200\n",
      "Train loss: 10.3975, Train R²: -222.2828\n",
      "test_loss=12.7708, test_R²=-302.6827\n",
      "Epoch 39/1200\n",
      "Train loss: 10.3187, Train R²: -221.0586\n",
      "test_loss=12.7378, test_R²=-301.8973\n",
      "Epoch 40/1200\n",
      "Train loss: 10.3463, Train R²: -220.9361\n",
      "test_loss=12.7047, test_R²=-301.1075\n",
      "Epoch 41/1200\n",
      "Train loss: 10.3057, Train R²: -224.6568\n",
      "test_loss=12.6721, test_R²=-300.3299\n",
      "Epoch 42/1200\n",
      "Train loss: 10.2765, Train R²: -222.0677\n",
      "test_loss=12.6405, test_R²=-299.5712\n",
      "Epoch 43/1200\n",
      "Train loss: 10.2842, Train R²: -220.6021\n",
      "test_loss=12.6082, test_R²=-298.8003\n",
      "Epoch 44/1200\n",
      "Train loss: 10.2395, Train R²: -220.9826\n",
      "test_loss=12.5767, test_R²=-298.0474\n",
      "Epoch 45/1200\n",
      "Train loss: 10.2302, Train R²: -219.4837\n",
      "test_loss=12.5439, test_R²=-297.2644\n",
      "Epoch 46/1200\n",
      "Train loss: 10.2121, Train R²: -222.6770\n",
      "test_loss=12.5125, test_R²=-296.5130\n",
      "Epoch 47/1200\n",
      "Train loss: 10.1865, Train R²: -221.2628\n",
      "test_loss=12.4809, test_R²=-295.7609\n",
      "Epoch 48/1200\n",
      "Train loss: 10.2181, Train R²: -218.6584\n",
      "test_loss=12.4491, test_R²=-295.0030\n",
      "Epoch 49/1200\n",
      "Train loss: 10.1725, Train R²: -216.9621\n",
      "test_loss=12.4174, test_R²=-294.2443\n",
      "Epoch 50/1200\n",
      "Train loss: 10.2015, Train R²: -216.8220\n",
      "test_loss=12.3854, test_R²=-293.4802\n",
      "Epoch 51/1200\n",
      "Train loss: 10.1574, Train R²: -218.7790\n",
      "test_loss=12.3540, test_R²=-292.7315\n",
      "Epoch 52/1200\n",
      "Train loss: 10.0474, Train R²: -217.9720\n",
      "test_loss=12.3224, test_R²=-291.9788\n",
      "Epoch 53/1200\n",
      "Train loss: 10.0285, Train R²: -215.8373\n",
      "test_loss=12.2902, test_R²=-291.2063\n",
      "Epoch 54/1200\n",
      "Train loss: 10.0813, Train R²: -214.3759\n",
      "test_loss=12.2597, test_R²=-290.4784\n",
      "Epoch 55/1200\n",
      "Train loss: 10.0137, Train R²: -220.2902\n",
      "test_loss=12.2285, test_R²=-289.7323\n",
      "Epoch 56/1200\n",
      "Train loss: 10.0793, Train R²: -217.4964\n",
      "test_loss=12.1980, test_R²=-289.0065\n",
      "Epoch 57/1200\n",
      "Train loss: 10.0185, Train R²: -215.8911\n",
      "test_loss=12.1666, test_R²=-288.2560\n",
      "Epoch 58/1200\n",
      "Train loss: 9.9787, Train R²: -217.4000\n",
      "test_loss=12.1352, test_R²=-287.5070\n",
      "Epoch 59/1200\n",
      "Train loss: 10.0010, Train R²: -214.4394\n",
      "test_loss=12.1041, test_R²=-286.7638\n",
      "Epoch 60/1200\n",
      "Train loss: 9.9701, Train R²: -213.5706\n",
      "test_loss=12.0728, test_R²=-286.0165\n",
      "Epoch 61/1200\n",
      "Train loss: 9.9935, Train R²: -215.9455\n",
      "test_loss=12.0415, test_R²=-285.2716\n",
      "Epoch 62/1200\n",
      "Train loss: 9.9543, Train R²: -212.9086\n",
      "test_loss=12.0124, test_R²=-284.5750\n",
      "Epoch 63/1200\n",
      "Train loss: 9.8926, Train R²: -214.3808\n",
      "test_loss=11.9807, test_R²=-283.8191\n",
      "Epoch 64/1200\n",
      "Train loss: 9.8754, Train R²: -212.5829\n",
      "test_loss=11.9507, test_R²=-283.1037\n",
      "Epoch 65/1200\n",
      "Train loss: 9.8871, Train R²: -212.1252\n",
      "test_loss=11.9208, test_R²=-282.3892\n",
      "Epoch 66/1200\n",
      "Train loss: 9.8797, Train R²: -210.5598\n",
      "test_loss=11.8903, test_R²=-281.6598\n",
      "Epoch 67/1200\n",
      "Train loss: 9.8722, Train R²: -210.0270\n",
      "test_loss=11.8601, test_R²=-280.9383\n",
      "Epoch 68/1200\n",
      "Train loss: 9.8287, Train R²: -208.4154\n",
      "test_loss=11.8296, test_R²=-280.2101\n",
      "Epoch 69/1200\n",
      "Train loss: 9.8187, Train R²: -212.7184\n",
      "test_loss=11.7993, test_R²=-279.4860\n",
      "Epoch 70/1200\n",
      "Train loss: 9.8066, Train R²: -207.9549\n",
      "test_loss=11.7698, test_R²=-278.7795\n",
      "Epoch 71/1200\n",
      "Train loss: 9.7584, Train R²: -208.8841\n",
      "test_loss=11.7412, test_R²=-278.0973\n",
      "Epoch 72/1200\n",
      "Train loss: 9.8163, Train R²: -211.1708\n",
      "test_loss=11.7111, test_R²=-277.3784\n",
      "Epoch 73/1200\n",
      "Train loss: 9.7207, Train R²: -207.7812\n",
      "test_loss=11.6802, test_R²=-276.6409\n",
      "Epoch 74/1200\n",
      "Train loss: 9.7386, Train R²: -209.6122\n",
      "test_loss=11.6510, test_R²=-275.9418\n",
      "Epoch 75/1200\n",
      "Train loss: 9.7173, Train R²: -205.7153\n",
      "test_loss=11.6212, test_R²=-275.2324\n",
      "Epoch 76/1200\n",
      "Train loss: 9.5926, Train R²: -208.1500\n",
      "test_loss=11.5924, test_R²=-274.5435\n",
      "Epoch 77/1200\n",
      "Train loss: 9.6610, Train R²: -206.4008\n",
      "test_loss=11.5626, test_R²=-273.8321\n",
      "Epoch 78/1200\n",
      "Train loss: 9.6704, Train R²: -207.5121\n",
      "test_loss=11.5328, test_R²=-273.1208\n",
      "Epoch 79/1200\n",
      "Train loss: 9.7081, Train R²: -208.8317\n",
      "test_loss=11.5041, test_R²=-272.4329\n",
      "Epoch 80/1200\n",
      "Train loss: 9.6227, Train R²: -206.9379\n",
      "test_loss=11.4751, test_R²=-271.7426\n",
      "Epoch 81/1200\n",
      "Train loss: 9.6005, Train R²: -206.7080\n",
      "test_loss=11.4451, test_R²=-271.0252\n",
      "Epoch 82/1200\n",
      "Train loss: 9.5951, Train R²: -204.9108\n",
      "test_loss=11.4161, test_R²=-270.3316\n",
      "Epoch 83/1200\n",
      "Train loss: 9.5600, Train R²: -206.3412\n",
      "test_loss=11.3867, test_R²=-269.6278\n",
      "Epoch 84/1200\n",
      "Train loss: 9.5689, Train R²: -202.3352\n",
      "test_loss=11.3574, test_R²=-268.9274\n",
      "Epoch 85/1200\n",
      "Train loss: 9.5702, Train R²: -204.0331\n",
      "test_loss=11.3290, test_R²=-268.2493\n",
      "Epoch 86/1200\n",
      "Train loss: 9.5609, Train R²: -203.1229\n",
      "test_loss=11.2996, test_R²=-267.5476\n",
      "Epoch 87/1200\n",
      "Train loss: 9.4613, Train R²: -202.8381\n",
      "test_loss=11.2708, test_R²=-266.8601\n",
      "Epoch 88/1200\n",
      "Train loss: 9.4859, Train R²: -206.8659\n",
      "test_loss=11.2431, test_R²=-266.1976\n",
      "Epoch 89/1200\n",
      "Train loss: 9.4921, Train R²: -201.4420\n",
      "test_loss=11.2148, test_R²=-265.5215\n",
      "Epoch 90/1200\n",
      "Train loss: 9.4410, Train R²: -201.2193\n",
      "test_loss=11.1866, test_R²=-264.8481\n",
      "Epoch 91/1200\n",
      "Train loss: 9.4096, Train R²: -200.9490\n",
      "test_loss=11.1583, test_R²=-264.1714\n",
      "Epoch 92/1200\n",
      "Train loss: 9.4219, Train R²: -201.5767\n",
      "test_loss=11.1297, test_R²=-263.4875\n",
      "Epoch 93/1200\n",
      "Train loss: 9.3697, Train R²: -198.9677\n",
      "test_loss=11.1013, test_R²=-262.8101\n",
      "Epoch 94/1200\n",
      "Train loss: 9.4298, Train R²: -199.2388\n",
      "test_loss=11.0733, test_R²=-262.1408\n",
      "Epoch 95/1200\n",
      "Train loss: 9.3270, Train R²: -200.3548\n",
      "test_loss=11.0456, test_R²=-261.4776\n",
      "Epoch 96/1200\n",
      "Train loss: 9.3591, Train R²: -198.3405\n",
      "test_loss=11.0175, test_R²=-260.8075\n",
      "Epoch 97/1200\n",
      "Train loss: 9.3809, Train R²: -200.1986\n",
      "test_loss=10.9900, test_R²=-260.1491\n",
      "Epoch 98/1200\n",
      "Train loss: 9.3255, Train R²: -197.7232\n",
      "test_loss=10.9622, test_R²=-259.4852\n",
      "Epoch 99/1200\n",
      "Train loss: 9.3427, Train R²: -197.8570\n",
      "test_loss=10.9349, test_R²=-258.8327\n",
      "Epoch 100/1200\n",
      "Train loss: 9.2936, Train R²: -198.2693\n",
      "test_loss=10.9067, test_R²=-258.1591\n",
      "Epoch 101/1200\n",
      "Train loss: 9.2860, Train R²: -197.0501\n",
      "test_loss=10.8792, test_R²=-257.5010\n",
      "Epoch 102/1200\n",
      "Train loss: 9.2999, Train R²: -197.9398\n",
      "test_loss=10.8514, test_R²=-256.8357\n",
      "Epoch 103/1200\n",
      "Train loss: 9.2609, Train R²: -197.9913\n",
      "test_loss=10.8239, test_R²=-256.1795\n",
      "Epoch 104/1200\n",
      "Train loss: 9.2725, Train R²: -196.6182\n",
      "test_loss=10.7968, test_R²=-255.5302\n",
      "Epoch 105/1200\n",
      "Train loss: 9.2858, Train R²: -196.4418\n",
      "test_loss=10.7693, test_R²=-254.8701\n",
      "Epoch 106/1200\n",
      "Train loss: 9.1674, Train R²: -197.9253\n",
      "test_loss=10.7421, test_R²=-254.2174\n",
      "Epoch 107/1200\n",
      "Train loss: 9.2336, Train R²: -195.9267\n",
      "test_loss=10.7147, test_R²=-253.5638\n",
      "Epoch 108/1200\n",
      "Train loss: 9.1599, Train R²: -195.4108\n",
      "test_loss=10.6881, test_R²=-252.9299\n",
      "Epoch 109/1200\n",
      "Train loss: 9.1501, Train R²: -196.4174\n",
      "test_loss=10.6618, test_R²=-252.2999\n",
      "Epoch 110/1200\n",
      "Train loss: 9.1727, Train R²: -193.5006\n",
      "test_loss=10.6349, test_R²=-251.6578\n",
      "Epoch 111/1200\n",
      "Train loss: 9.1343, Train R²: -194.7762\n",
      "test_loss=10.6084, test_R²=-251.0225\n",
      "Epoch 112/1200\n",
      "Train loss: 9.1281, Train R²: -194.9181\n",
      "test_loss=10.5823, test_R²=-250.3960\n",
      "Epoch 113/1200\n",
      "Train loss: 9.1482, Train R²: -192.3500\n",
      "test_loss=10.5554, test_R²=-249.7525\n",
      "Epoch 114/1200\n",
      "Train loss: 9.0731, Train R²: -192.6099\n",
      "test_loss=10.5288, test_R²=-249.1172\n",
      "Epoch 115/1200\n",
      "Train loss: 9.1029, Train R²: -192.8278\n",
      "test_loss=10.5023, test_R²=-248.4833\n",
      "Epoch 116/1200\n",
      "Train loss: 9.0390, Train R²: -193.5614\n",
      "test_loss=10.4755, test_R²=-247.8438\n",
      "Epoch 117/1200\n",
      "Train loss: 9.0341, Train R²: -191.4871\n",
      "test_loss=10.4492, test_R²=-247.2135\n",
      "Epoch 118/1200\n",
      "Train loss: 9.0347, Train R²: -191.0174\n",
      "test_loss=10.4229, test_R²=-246.5862\n",
      "Epoch 119/1200\n",
      "Train loss: 8.9974, Train R²: -193.5373\n",
      "test_loss=10.3974, test_R²=-245.9765\n",
      "Epoch 120/1200\n",
      "Train loss: 8.9388, Train R²: -190.8239\n",
      "test_loss=10.3710, test_R²=-245.3465\n",
      "Epoch 121/1200\n",
      "Train loss: 9.0210, Train R²: -190.4290\n",
      "test_loss=10.3441, test_R²=-244.7014\n",
      "Epoch 122/1200\n",
      "Train loss: 8.9351, Train R²: -191.7005\n",
      "test_loss=10.3180, test_R²=-244.0766\n",
      "Epoch 123/1200\n",
      "Train loss: 8.9911, Train R²: -190.2790\n",
      "test_loss=10.2930, test_R²=-243.4814\n",
      "Epoch 124/1200\n",
      "Train loss: 8.9846, Train R²: -189.3112\n",
      "test_loss=10.2676, test_R²=-242.8725\n",
      "Epoch 125/1200\n",
      "Train loss: 8.8949, Train R²: -189.1714\n",
      "test_loss=10.2410, test_R²=-242.2368\n",
      "Epoch 126/1200\n",
      "Train loss: 8.9254, Train R²: -187.8849\n",
      "test_loss=10.2160, test_R²=-241.6380\n",
      "Epoch 127/1200\n",
      "Train loss: 8.9524, Train R²: -188.9706\n",
      "test_loss=10.1903, test_R²=-241.0246\n",
      "Epoch 128/1200\n",
      "Train loss: 8.8764, Train R²: -189.8338\n",
      "test_loss=10.1655, test_R²=-240.4328\n",
      "Epoch 129/1200\n",
      "Train loss: 8.9073, Train R²: -189.0561\n",
      "test_loss=10.1406, test_R²=-239.8359\n",
      "Epoch 130/1200\n",
      "Train loss: 8.9053, Train R²: -186.5266\n",
      "test_loss=10.1154, test_R²=-239.2336\n",
      "Epoch 131/1200\n",
      "Train loss: 8.8218, Train R²: -189.2990\n",
      "test_loss=10.0903, test_R²=-238.6350\n",
      "Epoch 132/1200\n",
      "Train loss: 8.8856, Train R²: -189.4059\n",
      "test_loss=10.0655, test_R²=-238.0392\n",
      "Epoch 133/1200\n",
      "Train loss: 8.8107, Train R²: -186.2562\n",
      "test_loss=10.0403, test_R²=-237.4372\n",
      "Epoch 134/1200\n",
      "Train loss: 8.8163, Train R²: -188.0023\n",
      "test_loss=10.0161, test_R²=-236.8574\n",
      "Epoch 135/1200\n",
      "Train loss: 8.7832, Train R²: -185.4789\n",
      "test_loss=9.9913, test_R²=-236.2647\n",
      "Epoch 136/1200\n",
      "Train loss: 8.7272, Train R²: -187.1587\n",
      "test_loss=9.9667, test_R²=-235.6738\n",
      "Epoch 137/1200\n",
      "Train loss: 8.7498, Train R²: -184.8266\n",
      "test_loss=9.9418, test_R²=-235.0760\n",
      "Epoch 138/1200\n",
      "Train loss: 8.7596, Train R²: -184.0230\n",
      "test_loss=9.9175, test_R²=-234.4958\n",
      "Epoch 139/1200\n",
      "Train loss: 8.7145, Train R²: -185.7855\n",
      "test_loss=9.8929, test_R²=-233.9071\n",
      "Epoch 140/1200\n",
      "Train loss: 8.6739, Train R²: -184.0336\n",
      "test_loss=9.8688, test_R²=-233.3294\n",
      "Epoch 141/1200\n",
      "Train loss: 8.7328, Train R²: -187.6901\n",
      "test_loss=9.8451, test_R²=-232.7640\n",
      "Epoch 142/1200\n",
      "Train loss: 8.6729, Train R²: -185.7187\n",
      "test_loss=9.8217, test_R²=-232.2021\n",
      "Epoch 143/1200\n",
      "Train loss: 8.6973, Train R²: -184.8770\n",
      "test_loss=9.7980, test_R²=-231.6341\n",
      "Epoch 144/1200\n",
      "Train loss: 8.6077, Train R²: -182.6079\n",
      "test_loss=9.7740, test_R²=-231.0614\n",
      "Epoch 145/1200\n",
      "Train loss: 8.6444, Train R²: -184.2936\n",
      "test_loss=9.7502, test_R²=-230.4905\n",
      "Epoch 146/1200\n",
      "Train loss: 8.6685, Train R²: -183.4456\n",
      "test_loss=9.7266, test_R²=-229.9273\n",
      "Epoch 147/1200\n",
      "Train loss: 8.6187, Train R²: -181.5872\n",
      "test_loss=9.7024, test_R²=-229.3469\n",
      "Epoch 148/1200\n",
      "Train loss: 8.5722, Train R²: -180.9138\n",
      "test_loss=9.6787, test_R²=-228.7808\n",
      "Epoch 149/1200\n",
      "Train loss: 8.6410, Train R²: -179.8991\n",
      "test_loss=9.6550, test_R²=-228.2148\n",
      "Epoch 150/1200\n",
      "Train loss: 8.6426, Train R²: -181.2625\n",
      "test_loss=9.6322, test_R²=-227.6687\n",
      "Epoch 151/1200\n",
      "Train loss: 8.6081, Train R²: -182.6840\n",
      "test_loss=9.6084, test_R²=-227.0998\n",
      "Epoch 152/1200\n",
      "Train loss: 8.5684, Train R²: -179.9201\n",
      "test_loss=9.5845, test_R²=-226.5273\n",
      "Epoch 153/1200\n",
      "Train loss: 8.5740, Train R²: -181.5233\n",
      "test_loss=9.5616, test_R²=-225.9790\n",
      "Epoch 154/1200\n",
      "Train loss: 8.5333, Train R²: -180.0895\n",
      "test_loss=9.5386, test_R²=-225.4276\n",
      "Epoch 155/1200\n",
      "Train loss: 8.4438, Train R²: -181.3695\n",
      "test_loss=9.5147, test_R²=-224.8564\n",
      "Epoch 156/1200\n",
      "Train loss: 8.5021, Train R²: -179.1312\n",
      "test_loss=9.4916, test_R²=-224.3039\n",
      "Epoch 157/1200\n",
      "Train loss: 8.4903, Train R²: -177.2003\n",
      "test_loss=9.4687, test_R²=-223.7585\n",
      "Epoch 158/1200\n",
      "Train loss: 8.4934, Train R²: -178.2125\n",
      "test_loss=9.4458, test_R²=-223.2086\n",
      "Epoch 159/1200\n",
      "Train loss: 8.4453, Train R²: -178.6080\n",
      "test_loss=9.4225, test_R²=-222.6498\n",
      "Epoch 160/1200\n",
      "Train loss: 8.4595, Train R²: -180.5349\n",
      "test_loss=9.3991, test_R²=-222.0895\n",
      "Epoch 161/1200\n",
      "Train loss: 8.4177, Train R²: -179.0900\n",
      "test_loss=9.3770, test_R²=-221.5629\n",
      "Epoch 162/1200\n",
      "Train loss: 8.4244, Train R²: -177.4034\n",
      "test_loss=9.3550, test_R²=-221.0357\n",
      "Epoch 163/1200\n",
      "Train loss: 8.4425, Train R²: -177.8939\n",
      "test_loss=9.3329, test_R²=-220.5067\n",
      "Epoch 164/1200\n",
      "Train loss: 8.4370, Train R²: -175.4963\n",
      "test_loss=9.3111, test_R²=-219.9821\n",
      "Epoch 165/1200\n",
      "Train loss: 8.3694, Train R²: -177.1471\n",
      "test_loss=9.2888, test_R²=-219.4505\n",
      "Epoch 166/1200\n",
      "Train loss: 8.3881, Train R²: -174.6696\n",
      "test_loss=9.2660, test_R²=-218.9066\n",
      "Epoch 167/1200\n",
      "Train loss: 8.4065, Train R²: -176.7687\n",
      "test_loss=9.2435, test_R²=-218.3679\n",
      "Epoch 168/1200\n",
      "Train loss: 8.3228, Train R²: -175.0284\n",
      "test_loss=9.2215, test_R²=-217.8393\n",
      "Epoch 169/1200\n",
      "Train loss: 8.3440, Train R²: -176.4306\n",
      "test_loss=9.1992, test_R²=-217.3065\n",
      "Epoch 170/1200\n",
      "Train loss: 8.3111, Train R²: -174.4387\n",
      "test_loss=9.1775, test_R²=-216.7846\n",
      "Epoch 171/1200\n",
      "Train loss: 8.2596, Train R²: -174.0480\n",
      "test_loss=9.1558, test_R²=-216.2656\n",
      "Epoch 172/1200\n",
      "Train loss: 8.2967, Train R²: -176.2917\n",
      "test_loss=9.1342, test_R²=-215.7502\n",
      "Epoch 173/1200\n",
      "Train loss: 8.2971, Train R²: -172.7476\n",
      "test_loss=9.1124, test_R²=-215.2279\n",
      "Epoch 174/1200\n",
      "Train loss: 8.2673, Train R²: -173.5452\n",
      "test_loss=9.0907, test_R²=-214.7082\n",
      "Epoch 175/1200\n",
      "Train loss: 8.2259, Train R²: -172.8273\n",
      "test_loss=9.0688, test_R²=-214.1832\n",
      "Epoch 176/1200\n",
      "Train loss: 8.3010, Train R²: -175.6330\n",
      "test_loss=9.0476, test_R²=-213.6739\n",
      "Epoch 177/1200\n",
      "Train loss: 8.2429, Train R²: -172.1083\n",
      "test_loss=9.0266, test_R²=-213.1724\n",
      "Epoch 178/1200\n",
      "Train loss: 8.2403, Train R²: -172.8916\n",
      "test_loss=9.0054, test_R²=-212.6651\n",
      "Epoch 179/1200\n",
      "Train loss: 8.2248, Train R²: -172.7857\n",
      "test_loss=8.9840, test_R²=-212.1525\n",
      "Epoch 180/1200\n",
      "Train loss: 8.1812, Train R²: -173.4216\n",
      "test_loss=8.9629, test_R²=-211.6469\n",
      "Epoch 181/1200\n",
      "Train loss: 8.2243, Train R²: -171.6239\n",
      "test_loss=8.9413, test_R²=-211.1293\n",
      "Epoch 182/1200\n",
      "Train loss: 8.1575, Train R²: -172.3351\n",
      "test_loss=8.9201, test_R²=-210.6186\n",
      "Epoch 183/1200\n",
      "Train loss: 8.1229, Train R²: -172.1441\n",
      "test_loss=8.8995, test_R²=-210.1263\n",
      "Epoch 184/1200\n",
      "Train loss: 8.1813, Train R²: -169.9914\n",
      "test_loss=8.8785, test_R²=-209.6251\n",
      "Epoch 185/1200\n",
      "Train loss: 8.1386, Train R²: -170.6171\n",
      "test_loss=8.8580, test_R²=-209.1322\n",
      "Epoch 186/1200\n",
      "Train loss: 8.1028, Train R²: -170.5567\n",
      "test_loss=8.8381, test_R²=-208.6555\n",
      "Epoch 187/1200\n",
      "Train loss: 8.1105, Train R²: -169.6614\n",
      "test_loss=8.8176, test_R²=-208.1672\n",
      "Epoch 188/1200\n",
      "Train loss: 8.1092, Train R²: -169.3558\n",
      "test_loss=8.7971, test_R²=-207.6760\n",
      "Epoch 189/1200\n",
      "Train loss: 8.1201, Train R²: -170.9949\n",
      "test_loss=8.7766, test_R²=-207.1845\n",
      "Epoch 190/1200\n",
      "Train loss: 8.1235, Train R²: -169.8145\n",
      "test_loss=8.7558, test_R²=-206.6861\n",
      "Epoch 191/1200\n",
      "Train loss: 8.0791, Train R²: -168.0116\n",
      "test_loss=8.7357, test_R²=-206.2042\n",
      "Epoch 192/1200\n",
      "Train loss: 8.0551, Train R²: -168.1570\n",
      "test_loss=8.7153, test_R²=-205.7166\n",
      "Epoch 193/1200\n",
      "Train loss: 8.0788, Train R²: -169.3798\n",
      "test_loss=8.6954, test_R²=-205.2383\n",
      "Epoch 194/1200\n",
      "Train loss: 8.0712, Train R²: -171.3087\n",
      "test_loss=8.6762, test_R²=-204.7782\n",
      "Epoch 195/1200\n",
      "Train loss: 8.0457, Train R²: -167.9216\n",
      "test_loss=8.6558, test_R²=-204.2905\n",
      "Epoch 196/1200\n",
      "Train loss: 8.0586, Train R²: -168.2915\n",
      "test_loss=8.6359, test_R²=-203.8142\n",
      "Epoch 197/1200\n",
      "Train loss: 8.0188, Train R²: -166.2778\n",
      "test_loss=8.6159, test_R²=-203.3345\n",
      "Epoch 198/1200\n",
      "Train loss: 8.0049, Train R²: -168.1399\n",
      "test_loss=8.5960, test_R²=-202.8568\n",
      "Epoch 199/1200\n",
      "Train loss: 8.0474, Train R²: -167.3867\n",
      "test_loss=8.5764, test_R²=-202.3855\n",
      "Epoch 200/1200\n",
      "Train loss: 7.9743, Train R²: -166.9406\n",
      "test_loss=8.5560, test_R²=-201.8979\n",
      "Epoch 201/1200\n",
      "Train loss: 8.0008, Train R²: -166.6248\n",
      "test_loss=8.5361, test_R²=-201.4227\n",
      "Epoch 202/1200\n",
      "Train loss: 7.9685, Train R²: -166.2066\n",
      "test_loss=8.5163, test_R²=-200.9477\n",
      "Epoch 203/1200\n",
      "Train loss: 7.9990, Train R²: -165.1328\n",
      "test_loss=8.4967, test_R²=-200.4765\n",
      "Epoch 204/1200\n",
      "Train loss: 7.9239, Train R²: -165.4705\n",
      "test_loss=8.4772, test_R²=-200.0102\n",
      "Epoch 205/1200\n",
      "Train loss: 7.9275, Train R²: -165.1598\n",
      "test_loss=8.4580, test_R²=-199.5495\n",
      "Epoch 206/1200\n",
      "Train loss: 7.9397, Train R²: -165.1870\n",
      "test_loss=8.4386, test_R²=-199.0844\n",
      "Epoch 207/1200\n",
      "Train loss: 7.8728, Train R²: -166.0504\n",
      "test_loss=8.4196, test_R²=-198.6312\n",
      "Epoch 208/1200\n",
      "Train loss: 7.9252, Train R²: -164.0075\n",
      "test_loss=8.4008, test_R²=-198.1800\n",
      "Epoch 209/1200\n",
      "Train loss: 7.8841, Train R²: -163.9622\n",
      "test_loss=8.3818, test_R²=-197.7234\n",
      "Epoch 210/1200\n",
      "Train loss: 7.9000, Train R²: -162.8215\n",
      "test_loss=8.3628, test_R²=-197.2690\n",
      "Epoch 211/1200\n",
      "Train loss: 7.8697, Train R²: -163.0266\n",
      "test_loss=8.3441, test_R²=-196.8211\n",
      "Epoch 212/1200\n",
      "Train loss: 7.8521, Train R²: -162.5673\n",
      "test_loss=8.3251, test_R²=-196.3647\n",
      "Epoch 213/1200\n",
      "Train loss: 7.8458, Train R²: -162.7306\n",
      "test_loss=8.3063, test_R²=-195.9147\n",
      "Epoch 214/1200\n",
      "Train loss: 7.8399, Train R²: -162.3438\n",
      "test_loss=8.2880, test_R²=-195.4740\n",
      "Epoch 215/1200\n",
      "Train loss: 7.8564, Train R²: -163.5608\n",
      "test_loss=8.2696, test_R²=-195.0339\n",
      "Epoch 216/1200\n",
      "Train loss: 7.8270, Train R²: -163.1691\n",
      "test_loss=8.2510, test_R²=-194.5882\n",
      "Epoch 217/1200\n",
      "Train loss: 7.7731, Train R²: -162.1093\n",
      "test_loss=8.2328, test_R²=-194.1514\n",
      "Epoch 218/1200\n",
      "Train loss: 7.8255, Train R²: -161.6637\n",
      "test_loss=8.2146, test_R²=-193.7149\n",
      "Epoch 219/1200\n",
      "Train loss: 7.8483, Train R²: -161.3793\n",
      "test_loss=8.1964, test_R²=-193.2766\n",
      "Epoch 220/1200\n",
      "Train loss: 7.8213, Train R²: -162.2077\n",
      "test_loss=8.1783, test_R²=-192.8428\n",
      "Epoch 221/1200\n",
      "Train loss: 7.7868, Train R²: -161.4138\n",
      "test_loss=8.1604, test_R²=-192.4128\n",
      "Epoch 222/1200\n",
      "Train loss: 7.7543, Train R²: -160.5671\n",
      "test_loss=8.1427, test_R²=-191.9898\n",
      "Epoch 223/1200\n",
      "Train loss: 7.7453, Train R²: -161.2809\n",
      "test_loss=8.1252, test_R²=-191.5678\n",
      "Epoch 224/1200\n",
      "Train loss: 7.7044, Train R²: -162.3515\n",
      "test_loss=8.1073, test_R²=-191.1388\n",
      "Epoch 225/1200\n",
      "Train loss: 7.7615, Train R²: -161.5644\n",
      "test_loss=8.0893, test_R²=-190.7087\n",
      "Epoch 226/1200\n",
      "Train loss: 7.7258, Train R²: -159.9718\n",
      "test_loss=8.0716, test_R²=-190.2825\n",
      "Epoch 227/1200\n",
      "Train loss: 7.7212, Train R²: -160.4979\n",
      "test_loss=8.0542, test_R²=-189.8681\n",
      "Epoch 228/1200\n",
      "Train loss: 7.7217, Train R²: -159.4253\n",
      "test_loss=8.0373, test_R²=-189.4636\n",
      "Epoch 229/1200\n",
      "Train loss: 7.7447, Train R²: -159.8441\n",
      "test_loss=8.0200, test_R²=-189.0487\n",
      "Epoch 230/1200\n",
      "Train loss: 7.7279, Train R²: -159.0610\n",
      "test_loss=8.0020, test_R²=-188.6165\n",
      "Epoch 231/1200\n",
      "Train loss: 7.6632, Train R²: -159.2351\n",
      "test_loss=7.9846, test_R²=-188.1986\n",
      "Epoch 232/1200\n",
      "Train loss: 7.6993, Train R²: -160.0933\n",
      "test_loss=7.9671, test_R²=-187.7784\n",
      "Epoch 233/1200\n",
      "Train loss: 7.6494, Train R²: -159.3020\n",
      "test_loss=7.9498, test_R²=-187.3658\n",
      "Epoch 234/1200\n",
      "Train loss: 7.6464, Train R²: -160.4540\n",
      "test_loss=7.9332, test_R²=-186.9658\n",
      "Epoch 235/1200\n",
      "Train loss: 7.6642, Train R²: -157.1561\n",
      "test_loss=7.9162, test_R²=-186.5584\n",
      "Epoch 236/1200\n",
      "Train loss: 7.5989, Train R²: -158.0838\n",
      "test_loss=7.8993, test_R²=-186.1512\n",
      "Epoch 237/1200\n",
      "Train loss: 7.6222, Train R²: -158.7730\n",
      "test_loss=7.8824, test_R²=-185.7484\n",
      "Epoch 238/1200\n",
      "Train loss: 7.6077, Train R²: -158.3318\n",
      "test_loss=7.8657, test_R²=-185.3479\n",
      "Epoch 239/1200\n",
      "Train loss: 7.5763, Train R²: -157.4481\n",
      "test_loss=7.8495, test_R²=-184.9589\n",
      "Epoch 240/1200\n",
      "Train loss: 7.6173, Train R²: -157.2163\n",
      "test_loss=7.8328, test_R²=-184.5569\n",
      "Epoch 241/1200\n",
      "Train loss: 7.6076, Train R²: -156.9817\n",
      "test_loss=7.8164, test_R²=-184.1640\n",
      "Epoch 242/1200\n",
      "Train loss: 7.5303, Train R²: -156.9840\n",
      "test_loss=7.8000, test_R²=-183.7707\n",
      "Epoch 243/1200\n",
      "Train loss: 7.6148, Train R²: -155.7168\n",
      "test_loss=7.7834, test_R²=-183.3721\n",
      "Epoch 244/1200\n",
      "Train loss: 7.5908, Train R²: -156.3284\n",
      "test_loss=7.7669, test_R²=-182.9741\n",
      "Epoch 245/1200\n",
      "Train loss: 7.5603, Train R²: -155.4456\n",
      "test_loss=7.7506, test_R²=-182.5844\n",
      "Epoch 246/1200\n",
      "Train loss: 7.4860, Train R²: -155.3725\n",
      "test_loss=7.7340, test_R²=-182.1842\n",
      "Epoch 247/1200\n",
      "Train loss: 7.5342, Train R²: -155.5080\n",
      "test_loss=7.7182, test_R²=-181.8034\n",
      "Epoch 248/1200\n",
      "Train loss: 7.5139, Train R²: -155.4800\n",
      "test_loss=7.7022, test_R²=-181.4181\n",
      "Epoch 249/1200\n",
      "Train loss: 7.5024, Train R²: -154.6448\n",
      "test_loss=7.6863, test_R²=-181.0394\n",
      "Epoch 250/1200\n",
      "Train loss: 7.4679, Train R²: -155.2359\n",
      "test_loss=7.6704, test_R²=-180.6539\n",
      "Epoch 251/1200\n",
      "Train loss: 7.4590, Train R²: -155.3423\n",
      "test_loss=7.6547, test_R²=-180.2784\n",
      "Epoch 252/1200\n",
      "Train loss: 7.4617, Train R²: -154.4210\n",
      "test_loss=7.6389, test_R²=-179.9006\n",
      "Epoch 253/1200\n",
      "Train loss: 7.4959, Train R²: -154.9019\n",
      "test_loss=7.6234, test_R²=-179.5280\n",
      "Epoch 254/1200\n",
      "Train loss: 7.4546, Train R²: -155.1150\n",
      "test_loss=7.6075, test_R²=-179.1453\n",
      "Epoch 255/1200\n",
      "Train loss: 7.4344, Train R²: -154.5987\n",
      "test_loss=7.5919, test_R²=-178.7689\n",
      "Epoch 256/1200\n",
      "Train loss: 7.4705, Train R²: -153.7943\n",
      "test_loss=7.5762, test_R²=-178.3930\n",
      "Epoch 257/1200\n",
      "Train loss: 7.4444, Train R²: -153.5433\n",
      "test_loss=7.5613, test_R²=-178.0349\n",
      "Epoch 258/1200\n",
      "Train loss: 7.4912, Train R²: -152.4142\n",
      "test_loss=7.5459, test_R²=-177.6659\n",
      "Epoch 259/1200\n",
      "Train loss: 7.4316, Train R²: -155.5193\n",
      "test_loss=7.5307, test_R²=-177.3007\n",
      "Epoch 260/1200\n",
      "Train loss: 7.4482, Train R²: -153.1399\n",
      "test_loss=7.5156, test_R²=-176.9362\n",
      "Epoch 261/1200\n",
      "Train loss: 7.4191, Train R²: -152.8271\n",
      "test_loss=7.5004, test_R²=-176.5696\n",
      "Epoch 262/1200\n",
      "Train loss: 7.4116, Train R²: -152.3571\n",
      "test_loss=7.4856, test_R²=-176.2135\n",
      "Epoch 263/1200\n",
      "Train loss: 7.4590, Train R²: -151.9338\n",
      "test_loss=7.4708, test_R²=-175.8590\n",
      "Epoch 264/1200\n",
      "Train loss: 7.4160, Train R²: -152.2233\n",
      "test_loss=7.4560, test_R²=-175.5019\n",
      "Epoch 265/1200\n",
      "Train loss: 7.3421, Train R²: -151.9959\n",
      "test_loss=7.4416, test_R²=-175.1562\n",
      "Epoch 266/1200\n",
      "Train loss: 7.3803, Train R²: -151.5734\n",
      "test_loss=7.4267, test_R²=-174.7989\n",
      "Epoch 267/1200\n",
      "Train loss: 7.3770, Train R²: -151.0158\n",
      "test_loss=7.4121, test_R²=-174.4476\n",
      "Epoch 268/1200\n",
      "Train loss: 7.3314, Train R²: -151.4402\n",
      "test_loss=7.3976, test_R²=-174.0976\n",
      "Epoch 269/1200\n",
      "Train loss: 7.3406, Train R²: -152.0467\n",
      "test_loss=7.3832, test_R²=-173.7517\n",
      "Epoch 270/1200\n",
      "Train loss: 7.3512, Train R²: -153.9145\n",
      "test_loss=7.3688, test_R²=-173.4059\n",
      "Epoch 271/1200\n",
      "Train loss: 7.3291, Train R²: -151.2299\n",
      "test_loss=7.3543, test_R²=-173.0590\n",
      "Epoch 272/1200\n",
      "Train loss: 7.3447, Train R²: -150.7886\n",
      "test_loss=7.3403, test_R²=-172.7218\n",
      "Epoch 273/1200\n",
      "Train loss: 7.3295, Train R²: -149.8678\n",
      "test_loss=7.3263, test_R²=-172.3874\n",
      "Epoch 274/1200\n",
      "Train loss: 7.3374, Train R²: -150.2139\n",
      "test_loss=7.3124, test_R²=-172.0538\n",
      "Epoch 275/1200\n",
      "Train loss: 7.3237, Train R²: -149.2518\n",
      "test_loss=7.2977, test_R²=-171.7004\n",
      "Epoch 276/1200\n",
      "Train loss: 7.3236, Train R²: -150.3942\n",
      "test_loss=7.2843, test_R²=-171.3788\n",
      "Epoch 277/1200\n",
      "Train loss: 7.3530, Train R²: -149.0229\n",
      "test_loss=7.2706, test_R²=-171.0479\n",
      "Epoch 278/1200\n",
      "Train loss: 7.2669, Train R²: -149.8578\n",
      "test_loss=7.2566, test_R²=-170.7119\n",
      "Epoch 279/1200\n",
      "Train loss: 7.2938, Train R²: -148.6402\n",
      "test_loss=7.2428, test_R²=-170.3789\n",
      "Epoch 280/1200\n",
      "Train loss: 7.2508, Train R²: -149.7391\n",
      "test_loss=7.2289, test_R²=-170.0467\n",
      "Epoch 281/1200\n",
      "Train loss: 7.2551, Train R²: -148.8842\n",
      "test_loss=7.2152, test_R²=-169.7155\n",
      "Epoch 282/1200\n",
      "Train loss: 7.2428, Train R²: -148.7819\n",
      "test_loss=7.2017, test_R²=-169.3906\n",
      "Epoch 283/1200\n",
      "Train loss: 7.2293, Train R²: -148.5047\n",
      "test_loss=7.1885, test_R²=-169.0730\n",
      "Epoch 284/1200\n",
      "Train loss: 7.2768, Train R²: -148.8581\n",
      "test_loss=7.1750, test_R²=-168.7497\n",
      "Epoch 285/1200\n",
      "Train loss: 7.2133, Train R²: -148.4779\n",
      "test_loss=7.1614, test_R²=-168.4220\n",
      "Epoch 286/1200\n",
      "Train loss: 7.2352, Train R²: -147.3384\n",
      "test_loss=7.1480, test_R²=-168.1008\n",
      "Epoch 287/1200\n",
      "Train loss: 7.2146, Train R²: -149.8648\n",
      "test_loss=7.1348, test_R²=-167.7814\n",
      "Epoch 288/1200\n",
      "Train loss: 7.2551, Train R²: -146.9288\n",
      "test_loss=7.1218, test_R²=-167.4681\n",
      "Epoch 289/1200\n",
      "Train loss: 7.1807, Train R²: -148.1517\n",
      "test_loss=7.1089, test_R²=-167.1592\n",
      "Epoch 290/1200\n",
      "Train loss: 7.2199, Train R²: -147.7191\n",
      "test_loss=7.0959, test_R²=-166.8478\n",
      "Epoch 291/1200\n",
      "Train loss: 7.1537, Train R²: -148.8109\n",
      "test_loss=7.0826, test_R²=-166.5282\n",
      "Epoch 292/1200\n",
      "Train loss: 7.1617, Train R²: -147.1117\n",
      "test_loss=7.0696, test_R²=-166.2167\n",
      "Epoch 293/1200\n",
      "Train loss: 7.1600, Train R²: -146.7037\n",
      "test_loss=7.0568, test_R²=-165.9097\n",
      "Epoch 294/1200\n",
      "Train loss: 7.1372, Train R²: -148.9431\n",
      "test_loss=7.0440, test_R²=-165.6012\n",
      "Epoch 295/1200\n",
      "Train loss: 7.1785, Train R²: -147.3885\n",
      "test_loss=7.0313, test_R²=-165.2963\n",
      "Epoch 296/1200\n",
      "Train loss: 7.1498, Train R²: -148.8139\n",
      "test_loss=7.0180, test_R²=-164.9761\n",
      "Epoch 297/1200\n",
      "Train loss: 7.1357, Train R²: -145.7004\n",
      "test_loss=7.0050, test_R²=-164.6647\n",
      "Epoch 298/1200\n",
      "Train loss: 7.1456, Train R²: -146.2517\n",
      "test_loss=6.9924, test_R²=-164.3609\n",
      "Epoch 299/1200\n",
      "Train loss: 7.1192, Train R²: -146.4479\n",
      "test_loss=6.9798, test_R²=-164.0602\n",
      "Epoch 300/1200\n",
      "Train loss: 7.1122, Train R²: -147.2159\n",
      "test_loss=6.9667, test_R²=-163.7449\n",
      "Epoch 301/1200\n",
      "Train loss: 7.0771, Train R²: -148.5249\n",
      "test_loss=6.9550, test_R²=-163.4616\n",
      "Epoch 302/1200\n",
      "Train loss: 7.0905, Train R²: -145.2020\n",
      "test_loss=6.9429, test_R²=-163.1698\n",
      "Epoch 303/1200\n",
      "Train loss: 7.1424, Train R²: -145.7380\n",
      "test_loss=6.9308, test_R²=-162.8787\n",
      "Epoch 304/1200\n",
      "Train loss: 7.0970, Train R²: -145.2970\n",
      "test_loss=6.9186, test_R²=-162.5847\n",
      "Epoch 305/1200\n",
      "Train loss: 7.1064, Train R²: -145.1250\n",
      "test_loss=6.9068, test_R²=-162.3018\n",
      "Epoch 306/1200\n",
      "Train loss: 7.1307, Train R²: -143.9690\n",
      "test_loss=6.8948, test_R²=-162.0147\n",
      "Epoch 307/1200\n",
      "Train loss: 7.0847, Train R²: -145.2902\n",
      "test_loss=6.8824, test_R²=-161.7147\n",
      "Epoch 308/1200\n",
      "Train loss: 7.0707, Train R²: -144.6923\n",
      "test_loss=6.8702, test_R²=-161.4217\n",
      "Epoch 309/1200\n",
      "Train loss: 7.0745, Train R²: -143.7600\n",
      "test_loss=6.8583, test_R²=-161.1339\n",
      "Epoch 310/1200\n",
      "Train loss: 7.0482, Train R²: -144.1086\n",
      "test_loss=6.8465, test_R²=-160.8495\n",
      "Epoch 311/1200\n",
      "Train loss: 7.0393, Train R²: -143.5208\n",
      "test_loss=6.8348, test_R²=-160.5676\n",
      "Epoch 312/1200\n",
      "Train loss: 7.0802, Train R²: -144.6518\n",
      "test_loss=6.8232, test_R²=-160.2868\n",
      "Epoch 313/1200\n",
      "Train loss: 7.0171, Train R²: -144.0383\n",
      "test_loss=6.8112, test_R²=-159.9991\n",
      "Epoch 314/1200\n",
      "Train loss: 7.0455, Train R²: -144.6228\n",
      "test_loss=6.7997, test_R²=-159.7219\n",
      "Epoch 315/1200\n",
      "Train loss: 6.9987, Train R²: -143.6680\n",
      "test_loss=6.7883, test_R²=-159.4481\n",
      "Epoch 316/1200\n",
      "Train loss: 7.0568, Train R²: -143.7308\n",
      "test_loss=6.7768, test_R²=-159.1712\n",
      "Epoch 317/1200\n",
      "Train loss: 6.9877, Train R²: -144.3482\n",
      "test_loss=6.7655, test_R²=-158.8999\n",
      "Epoch 318/1200\n",
      "Train loss: 7.0071, Train R²: -146.4040\n",
      "test_loss=6.7544, test_R²=-158.6311\n",
      "Epoch 319/1200\n",
      "Train loss: 7.0114, Train R²: -142.8413\n",
      "test_loss=6.7430, test_R²=-158.3561\n",
      "Epoch 320/1200\n",
      "Train loss: 7.0296, Train R²: -142.5714\n",
      "test_loss=6.7321, test_R²=-158.0946\n",
      "Epoch 321/1200\n",
      "Train loss: 7.0327, Train R²: -143.0888\n",
      "test_loss=6.7208, test_R²=-157.8212\n",
      "Epoch 322/1200\n",
      "Train loss: 6.9829, Train R²: -144.1400\n",
      "test_loss=6.7097, test_R²=-157.5537\n",
      "Epoch 323/1200\n",
      "Train loss: 7.0248, Train R²: -144.4747\n",
      "test_loss=6.6986, test_R²=-157.2877\n",
      "Epoch 324/1200\n",
      "Train loss: 7.0036, Train R²: -143.0901\n",
      "test_loss=6.6878, test_R²=-157.0260\n",
      "Epoch 325/1200\n",
      "Train loss: 7.0012, Train R²: -142.7543\n",
      "test_loss=6.6770, test_R²=-156.7664\n",
      "Epoch 326/1200\n",
      "Train loss: 6.9962, Train R²: -142.2086\n",
      "test_loss=6.6661, test_R²=-156.5029\n",
      "Epoch 327/1200\n",
      "Train loss: 6.9982, Train R²: -141.9313\n",
      "test_loss=6.6549, test_R²=-156.2339\n",
      "Epoch 328/1200\n",
      "Train loss: 6.9460, Train R²: -142.2932\n",
      "test_loss=6.6443, test_R²=-155.9781\n",
      "Epoch 329/1200\n",
      "Train loss: 6.9392, Train R²: -142.1207\n",
      "test_loss=6.6334, test_R²=-155.7164\n",
      "Epoch 330/1200\n",
      "Train loss: 6.9384, Train R²: -142.9113\n",
      "test_loss=6.6230, test_R²=-155.4635\n",
      "Epoch 331/1200\n",
      "Train loss: 6.9451, Train R²: -141.3716\n",
      "test_loss=6.6129, test_R²=-155.2225\n",
      "Epoch 332/1200\n",
      "Train loss: 6.9166, Train R²: -141.7103\n",
      "test_loss=6.6027, test_R²=-154.9743\n",
      "Epoch 333/1200\n",
      "Train loss: 6.9059, Train R²: -141.7123\n",
      "test_loss=6.5923, test_R²=-154.7248\n",
      "Epoch 334/1200\n",
      "Train loss: 6.9384, Train R²: -140.5454\n",
      "test_loss=6.5820, test_R²=-154.4756\n",
      "Epoch 335/1200\n",
      "Train loss: 6.8947, Train R²: -142.3044\n",
      "test_loss=6.5720, test_R²=-154.2357\n",
      "Epoch 336/1200\n",
      "Train loss: 6.9454, Train R²: -141.6683\n",
      "test_loss=6.5616, test_R²=-153.9846\n",
      "Epoch 337/1200\n",
      "Train loss: 6.9359, Train R²: -139.8866\n",
      "test_loss=6.5513, test_R²=-153.7359\n",
      "Epoch 338/1200\n",
      "Train loss: 6.9170, Train R²: -142.0862\n",
      "test_loss=6.5414, test_R²=-153.4965\n",
      "Epoch 339/1200\n",
      "Train loss: 6.8820, Train R²: -140.9126\n",
      "test_loss=6.5312, test_R²=-153.2510\n",
      "Epoch 340/1200\n",
      "Train loss: 6.8795, Train R²: -139.4961\n",
      "test_loss=6.5211, test_R²=-153.0061\n",
      "Epoch 341/1200\n",
      "Train loss: 6.8807, Train R²: -140.0139\n",
      "test_loss=6.5113, test_R²=-152.7713\n",
      "Epoch 342/1200\n",
      "Train loss: 6.8961, Train R²: -139.7330\n",
      "test_loss=6.5016, test_R²=-152.5362\n",
      "Epoch 343/1200\n",
      "Train loss: 6.9283, Train R²: -139.9358\n",
      "test_loss=6.4916, test_R²=-152.2954\n",
      "Epoch 344/1200\n",
      "Train loss: 6.9151, Train R²: -141.5228\n",
      "test_loss=6.4815, test_R²=-152.0532\n",
      "Epoch 345/1200\n",
      "Train loss: 6.8774, Train R²: -139.2035\n",
      "test_loss=6.4718, test_R²=-151.8204\n",
      "Epoch 346/1200\n",
      "Train loss: 6.8514, Train R²: -139.1322\n",
      "test_loss=6.4623, test_R²=-151.5898\n",
      "Epoch 347/1200\n",
      "Train loss: 6.8668, Train R²: -139.1786\n",
      "test_loss=6.4529, test_R²=-151.3646\n",
      "Epoch 348/1200\n",
      "Train loss: 6.8563, Train R²: -139.1205\n",
      "test_loss=6.4434, test_R²=-151.1355\n",
      "Epoch 349/1200\n",
      "Train loss: 6.8808, Train R²: -142.1280\n",
      "test_loss=6.4338, test_R²=-150.9047\n",
      "Epoch 350/1200\n",
      "Train loss: 6.8271, Train R²: -138.8474\n",
      "test_loss=6.4245, test_R²=-150.6792\n",
      "Epoch 351/1200\n",
      "Train loss: 6.8335, Train R²: -138.8716\n",
      "test_loss=6.4151, test_R²=-150.4530\n",
      "Epoch 352/1200\n",
      "Train loss: 6.8014, Train R²: -137.9393\n",
      "test_loss=6.4057, test_R²=-150.2256\n",
      "Epoch 353/1200\n",
      "Train loss: 6.8459, Train R²: -138.6145\n",
      "test_loss=6.3962, test_R²=-149.9974\n",
      "Epoch 354/1200\n",
      "Train loss: 6.8440, Train R²: -139.8055\n",
      "test_loss=6.3873, test_R²=-149.7818\n",
      "Epoch 355/1200\n",
      "Train loss: 6.8146, Train R²: -138.2203\n",
      "test_loss=6.3783, test_R²=-149.5638\n",
      "Epoch 356/1200\n",
      "Train loss: 6.7938, Train R²: -137.8681\n",
      "test_loss=6.3694, test_R²=-149.3475\n",
      "Epoch 357/1200\n",
      "Train loss: 6.8329, Train R²: -138.3478\n",
      "test_loss=6.3604, test_R²=-149.1298\n",
      "Epoch 358/1200\n",
      "Train loss: 6.8154, Train R²: -137.8993\n",
      "test_loss=6.3516, test_R²=-148.9176\n",
      "Epoch 359/1200\n",
      "Train loss: 6.8226, Train R²: -139.7884\n",
      "test_loss=6.3429, test_R²=-148.7075\n",
      "Epoch 360/1200\n",
      "Train loss: 6.7670, Train R²: -136.9938\n",
      "test_loss=6.3341, test_R²=-148.4950\n",
      "Epoch 361/1200\n",
      "Train loss: 6.8214, Train R²: -139.7166\n",
      "test_loss=6.3254, test_R²=-148.2852\n",
      "Epoch 362/1200\n",
      "Train loss: 6.7853, Train R²: -137.3846\n",
      "test_loss=6.3169, test_R²=-148.0804\n",
      "Epoch 363/1200\n",
      "Train loss: 6.8265, Train R²: -137.2556\n",
      "test_loss=6.3083, test_R²=-147.8729\n",
      "Epoch 364/1200\n",
      "Train loss: 6.7712, Train R²: -138.1221\n",
      "test_loss=6.3000, test_R²=-147.6704\n",
      "Epoch 365/1200\n",
      "Train loss: 6.7850, Train R²: -137.6478\n",
      "test_loss=6.2917, test_R²=-147.4710\n",
      "Epoch 366/1200\n",
      "Train loss: 6.7442, Train R²: -136.6154\n",
      "test_loss=6.2832, test_R²=-147.2660\n",
      "Epoch 367/1200\n",
      "Train loss: 6.7778, Train R²: -137.0185\n",
      "test_loss=6.2746, test_R²=-147.0601\n",
      "Epoch 368/1200\n",
      "Train loss: 6.7733, Train R²: -136.9909\n",
      "test_loss=6.2659, test_R²=-146.8496\n",
      "Epoch 369/1200\n",
      "Train loss: 6.7381, Train R²: -136.3684\n",
      "test_loss=6.2576, test_R²=-146.6479\n",
      "Epoch 370/1200\n",
      "Train loss: 6.7929, Train R²: -137.7175\n",
      "test_loss=6.2494, test_R²=-146.4498\n",
      "Epoch 371/1200\n",
      "Train loss: 6.7321, Train R²: -136.4165\n",
      "test_loss=6.2412, test_R²=-146.2526\n",
      "Epoch 372/1200\n",
      "Train loss: 6.7459, Train R²: -136.7999\n",
      "test_loss=6.2330, test_R²=-146.0545\n",
      "Epoch 373/1200\n",
      "Train loss: 6.7324, Train R²: -135.9672\n",
      "test_loss=6.2250, test_R²=-145.8597\n",
      "Epoch 374/1200\n",
      "Train loss: 6.7548, Train R²: -135.8213\n",
      "test_loss=6.2168, test_R²=-145.6607\n",
      "Epoch 375/1200\n",
      "Train loss: 6.7261, Train R²: -137.3583\n",
      "test_loss=6.2089, test_R²=-145.4704\n",
      "Epoch 376/1200\n",
      "Train loss: 6.7010, Train R²: -136.0226\n",
      "test_loss=6.2010, test_R²=-145.2812\n",
      "Epoch 377/1200\n",
      "Train loss: 6.7166, Train R²: -135.1188\n",
      "test_loss=6.1932, test_R²=-145.0930\n",
      "Epoch 378/1200\n",
      "Train loss: 6.7197, Train R²: -135.5776\n",
      "test_loss=6.1856, test_R²=-144.9067\n",
      "Epoch 379/1200\n",
      "Train loss: 6.7446, Train R²: -136.5594\n",
      "test_loss=6.1779, test_R²=-144.7224\n",
      "Epoch 380/1200\n",
      "Train loss: 6.7208, Train R²: -134.9385\n",
      "test_loss=6.1702, test_R²=-144.5350\n",
      "Epoch 381/1200\n",
      "Train loss: 6.6869, Train R²: -134.8517\n",
      "test_loss=6.1623, test_R²=-144.3450\n",
      "Epoch 382/1200\n",
      "Train loss: 6.7159, Train R²: -134.7717\n",
      "test_loss=6.1548, test_R²=-144.1617\n",
      "Epoch 383/1200\n",
      "Train loss: 6.7096, Train R²: -134.8581\n",
      "test_loss=6.1475, test_R²=-143.9853\n",
      "Epoch 384/1200\n",
      "Train loss: 6.7281, Train R²: -137.8211\n",
      "test_loss=6.1397, test_R²=-143.7966\n",
      "Epoch 385/1200\n",
      "Train loss: 6.6974, Train R²: -135.5153\n",
      "test_loss=6.1321, test_R²=-143.6132\n",
      "Epoch 386/1200\n",
      "Train loss: 6.7409, Train R²: -134.8035\n",
      "test_loss=6.1246, test_R²=-143.4315\n",
      "Epoch 387/1200\n",
      "Train loss: 6.6767, Train R²: -134.3623\n",
      "test_loss=6.1175, test_R²=-143.2587\n",
      "Epoch 388/1200\n",
      "Train loss: 6.6873, Train R²: -134.8391\n",
      "test_loss=6.1101, test_R²=-143.0809\n",
      "Epoch 389/1200\n",
      "Train loss: 6.6919, Train R²: -133.9214\n",
      "test_loss=6.1028, test_R²=-142.9036\n",
      "Epoch 390/1200\n",
      "Train loss: 6.6499, Train R²: -134.4483\n",
      "test_loss=6.0959, test_R²=-142.7357\n",
      "Epoch 391/1200\n",
      "Train loss: 6.6810, Train R²: -134.9403\n",
      "test_loss=6.0890, test_R²=-142.5676\n",
      "Epoch 392/1200\n",
      "Train loss: 6.6988, Train R²: -135.0893\n",
      "test_loss=6.0821, test_R²=-142.3998\n",
      "Epoch 393/1200\n",
      "Train loss: 6.6573, Train R²: -135.1081\n",
      "test_loss=6.0750, test_R²=-142.2284\n",
      "Epoch 394/1200\n",
      "Train loss: 6.6551, Train R²: -135.2420\n",
      "test_loss=6.0680, test_R²=-142.0596\n",
      "Epoch 395/1200\n",
      "Train loss: 6.6360, Train R²: -136.0695\n",
      "test_loss=6.0611, test_R²=-141.8912\n",
      "Epoch 396/1200\n",
      "Train loss: 6.6695, Train R²: -134.4781\n",
      "test_loss=6.0540, test_R²=-141.7198\n",
      "Epoch 397/1200\n",
      "Train loss: 6.6484, Train R²: -134.4964\n",
      "test_loss=6.0474, test_R²=-141.5580\n",
      "Epoch 398/1200\n",
      "Train loss: 6.7000, Train R²: -134.1328\n",
      "test_loss=6.0408, test_R²=-141.3982\n",
      "Epoch 399/1200\n",
      "Train loss: 6.6104, Train R²: -134.7233\n",
      "test_loss=6.0340, test_R²=-141.2342\n",
      "Epoch 400/1200\n",
      "Train loss: 6.6315, Train R²: -135.7330\n",
      "test_loss=6.0277, test_R²=-141.0793\n",
      "Epoch 401/1200\n",
      "Train loss: 6.6453, Train R²: -134.1338\n",
      "test_loss=6.0208, test_R²=-140.9145\n",
      "Epoch 402/1200\n",
      "Train loss: 6.6137, Train R²: -133.5850\n",
      "test_loss=6.0146, test_R²=-140.7623\n",
      "Epoch 403/1200\n",
      "Train loss: 6.6351, Train R²: -133.5291\n",
      "test_loss=6.0080, test_R²=-140.6023\n",
      "Epoch 404/1200\n",
      "Train loss: 6.6480, Train R²: -134.3472\n",
      "test_loss=6.0015, test_R²=-140.4463\n",
      "Epoch 405/1200\n",
      "Train loss: 6.6338, Train R²: -133.5874\n",
      "test_loss=5.9951, test_R²=-140.2899\n",
      "Epoch 406/1200\n",
      "Train loss: 6.6605, Train R²: -134.7907\n",
      "test_loss=5.9887, test_R²=-140.1364\n",
      "Epoch 407/1200\n",
      "Train loss: 6.6459, Train R²: -133.2920\n",
      "test_loss=5.9824, test_R²=-139.9837\n",
      "Epoch 408/1200\n",
      "Train loss: 6.6205, Train R²: -132.4731\n",
      "test_loss=5.9763, test_R²=-139.8353\n",
      "Epoch 409/1200\n",
      "Train loss: 6.5920, Train R²: -132.9316\n",
      "test_loss=5.9699, test_R²=-139.6804\n",
      "Epoch 410/1200\n",
      "Train loss: 6.6025, Train R²: -133.8893\n",
      "test_loss=5.9639, test_R²=-139.5329\n",
      "Epoch 411/1200\n",
      "Train loss: 6.6416, Train R²: -132.6095\n",
      "test_loss=5.9579, test_R²=-139.3880\n",
      "Epoch 412/1200\n",
      "Train loss: 6.5703, Train R²: -133.2377\n",
      "test_loss=5.9519, test_R²=-139.2414\n",
      "Epoch 413/1200\n",
      "Train loss: 6.5712, Train R²: -131.9831\n",
      "test_loss=5.9457, test_R²=-139.0915\n",
      "Epoch 414/1200\n",
      "Train loss: 6.5753, Train R²: -132.1264\n",
      "test_loss=5.9396, test_R²=-138.9446\n",
      "Epoch 415/1200\n",
      "Train loss: 6.5673, Train R²: -132.5454\n",
      "test_loss=5.9335, test_R²=-138.7964\n",
      "Epoch 416/1200\n",
      "Train loss: 6.5482, Train R²: -132.2380\n",
      "test_loss=5.9277, test_R²=-138.6551\n",
      "Epoch 417/1200\n",
      "Train loss: 6.5957, Train R²: -132.1044\n",
      "test_loss=5.9217, test_R²=-138.5094\n",
      "Epoch 418/1200\n",
      "Train loss: 6.6125, Train R²: -132.0973\n",
      "test_loss=5.9159, test_R²=-138.3681\n",
      "Epoch 419/1200\n",
      "Train loss: 6.5808, Train R²: -132.0715\n",
      "test_loss=5.9101, test_R²=-138.2275\n",
      "Epoch 420/1200\n",
      "Train loss: 6.5377, Train R²: -132.7638\n",
      "test_loss=5.9043, test_R²=-138.0873\n",
      "Epoch 421/1200\n",
      "Train loss: 6.5728, Train R²: -131.9966\n",
      "test_loss=5.8984, test_R²=-137.9444\n",
      "Epoch 422/1200\n",
      "Train loss: 6.5944, Train R²: -132.4880\n",
      "test_loss=5.8929, test_R²=-137.8098\n",
      "Epoch 423/1200\n",
      "Train loss: 6.5444, Train R²: -131.5728\n",
      "test_loss=5.8872, test_R²=-137.6709\n",
      "Epoch 424/1200\n",
      "Train loss: 6.5634, Train R²: -133.0362\n",
      "test_loss=5.8816, test_R²=-137.5359\n",
      "Epoch 425/1200\n",
      "Train loss: 6.5863, Train R²: -132.1871\n",
      "test_loss=5.8763, test_R²=-137.4065\n",
      "Epoch 426/1200\n",
      "Train loss: 6.5806, Train R²: -131.8808\n",
      "test_loss=5.8711, test_R²=-137.2801\n",
      "Epoch 427/1200\n",
      "Train loss: 6.5384, Train R²: -132.1467\n",
      "test_loss=5.8657, test_R²=-137.1503\n",
      "Epoch 428/1200\n",
      "Train loss: 6.5657, Train R²: -132.5482\n",
      "test_loss=5.8604, test_R²=-137.0201\n",
      "Epoch 429/1200\n",
      "Train loss: 6.5362, Train R²: -131.9938\n",
      "test_loss=5.8551, test_R²=-136.8925\n",
      "Epoch 430/1200\n",
      "Train loss: 6.5898, Train R²: -132.4324\n",
      "test_loss=5.8498, test_R²=-136.7621\n",
      "Epoch 431/1200\n",
      "Train loss: 6.5737, Train R²: -131.0630\n",
      "test_loss=5.8444, test_R²=-136.6309\n",
      "Epoch 432/1200\n",
      "Train loss: 6.5253, Train R²: -132.8072\n",
      "test_loss=5.8391, test_R²=-136.5020\n",
      "Epoch 433/1200\n",
      "Train loss: 6.5191, Train R²: -130.8603\n",
      "test_loss=5.8339, test_R²=-136.3749\n",
      "Epoch 434/1200\n",
      "Train loss: 6.5485, Train R²: -130.7573\n",
      "test_loss=5.8288, test_R²=-136.2519\n",
      "Epoch 435/1200\n",
      "Train loss: 6.5514, Train R²: -130.7087\n",
      "test_loss=5.8236, test_R²=-136.1251\n",
      "Epoch 436/1200\n",
      "Train loss: 6.5158, Train R²: -130.7887\n",
      "test_loss=5.8187, test_R²=-136.0050\n",
      "Epoch 437/1200\n",
      "Train loss: 6.5195, Train R²: -130.9001\n",
      "test_loss=5.8136, test_R²=-135.8820\n",
      "Epoch 438/1200\n",
      "Train loss: 6.5315, Train R²: -131.4105\n",
      "test_loss=5.8086, test_R²=-135.7606\n",
      "Epoch 439/1200\n",
      "Train loss: 6.5383, Train R²: -130.4478\n",
      "test_loss=5.8035, test_R²=-135.6365\n",
      "Epoch 440/1200\n",
      "Train loss: 6.5018, Train R²: -130.5101\n",
      "test_loss=5.7986, test_R²=-135.5180\n",
      "Epoch 441/1200\n",
      "Train loss: 6.5347, Train R²: -130.3537\n",
      "test_loss=5.7938, test_R²=-135.3996\n",
      "Epoch 442/1200\n",
      "Train loss: 6.5229, Train R²: -130.5167\n",
      "test_loss=5.7890, test_R²=-135.2834\n",
      "Epoch 443/1200\n",
      "Train loss: 6.5145, Train R²: -130.5511\n",
      "test_loss=5.7840, test_R²=-135.1613\n",
      "Epoch 444/1200\n",
      "Train loss: 6.4837, Train R²: -130.2654\n",
      "test_loss=5.7792, test_R²=-135.0452\n",
      "Epoch 445/1200\n",
      "Train loss: 6.4949, Train R²: -130.3548\n",
      "test_loss=5.7745, test_R²=-134.9305\n",
      "Epoch 446/1200\n",
      "Train loss: 6.5033, Train R²: -130.3761\n",
      "test_loss=5.7697, test_R²=-134.8130\n",
      "Epoch 447/1200\n",
      "Train loss: 6.4543, Train R²: -129.7455\n",
      "test_loss=5.7651, test_R²=-134.7008\n",
      "Epoch 448/1200\n",
      "Train loss: 6.4810, Train R²: -132.2619\n",
      "test_loss=5.7606, test_R²=-134.5906\n",
      "Epoch 449/1200\n",
      "Train loss: 6.5372, Train R²: -129.9539\n",
      "test_loss=5.7562, test_R²=-134.4838\n",
      "Epoch 450/1200\n",
      "Train loss: 6.4870, Train R²: -131.1971\n",
      "test_loss=5.7517, test_R²=-134.3746\n",
      "Epoch 451/1200\n",
      "Train loss: 6.4730, Train R²: -130.2230\n",
      "test_loss=5.7474, test_R²=-134.2698\n",
      "Epoch 452/1200\n",
      "Train loss: 6.5255, Train R²: -129.8221\n",
      "test_loss=5.7430, test_R²=-134.1612\n",
      "Epoch 453/1200\n",
      "Train loss: 6.4938, Train R²: -130.0923\n",
      "test_loss=5.7387, test_R²=-134.0551\n",
      "Epoch 454/1200\n",
      "Train loss: 6.4695, Train R²: -129.4872\n",
      "test_loss=5.7343, test_R²=-133.9495\n",
      "Epoch 455/1200\n",
      "Train loss: 6.4706, Train R²: -130.1157\n",
      "test_loss=5.7299, test_R²=-133.8423\n",
      "Epoch 456/1200\n",
      "Train loss: 6.5192, Train R²: -129.8938\n",
      "test_loss=5.7258, test_R²=-133.7403\n",
      "Epoch 457/1200\n",
      "Train loss: 6.4682, Train R²: -130.0898\n",
      "test_loss=5.7214, test_R²=-133.6349\n",
      "Epoch 458/1200\n",
      "Train loss: 6.4544, Train R²: -129.2621\n",
      "test_loss=5.7173, test_R²=-133.5348\n",
      "Epoch 459/1200\n",
      "Train loss: 6.5080, Train R²: -129.7119\n",
      "test_loss=5.7131, test_R²=-133.4314\n",
      "Epoch 460/1200\n",
      "Train loss: 6.4643, Train R²: -130.3639\n",
      "test_loss=5.7088, test_R²=-133.3270\n",
      "Epoch 461/1200\n",
      "Train loss: 6.4794, Train R²: -129.2169\n",
      "test_loss=5.7047, test_R²=-133.2259\n",
      "Epoch 462/1200\n",
      "Train loss: 6.4909, Train R²: -130.4137\n",
      "test_loss=5.7005, test_R²=-133.1230\n",
      "Epoch 463/1200\n",
      "Train loss: 6.4897, Train R²: -129.3716\n",
      "test_loss=5.6965, test_R²=-133.0256\n",
      "Epoch 464/1200\n",
      "Train loss: 6.5004, Train R²: -131.2413\n",
      "test_loss=5.6927, test_R²=-132.9318\n",
      "Epoch 465/1200\n",
      "Train loss: 6.4731, Train R²: -130.0621\n",
      "test_loss=5.6886, test_R²=-132.8330\n",
      "Epoch 466/1200\n",
      "Train loss: 6.4706, Train R²: -128.4916\n",
      "test_loss=5.6846, test_R²=-132.7329\n",
      "Epoch 467/1200\n",
      "Train loss: 6.5263, Train R²: -129.5715\n",
      "test_loss=5.6806, test_R²=-132.6371\n",
      "Epoch 468/1200\n",
      "Train loss: 6.4684, Train R²: -128.7130\n",
      "test_loss=5.6768, test_R²=-132.5424\n",
      "Epoch 469/1200\n",
      "Train loss: 6.4562, Train R²: -129.2909\n",
      "test_loss=5.6729, test_R²=-132.4469\n",
      "Epoch 470/1200\n",
      "Train loss: 6.4707, Train R²: -129.0344\n",
      "test_loss=5.6692, test_R²=-132.3554\n",
      "Epoch 471/1200\n",
      "Train loss: 6.5372, Train R²: -128.7146\n",
      "test_loss=5.6654, test_R²=-132.2632\n",
      "Epoch 472/1200\n",
      "Train loss: 6.4656, Train R²: -128.6136\n",
      "test_loss=5.6616, test_R²=-132.1711\n",
      "Epoch 473/1200\n",
      "Train loss: 6.4464, Train R²: -128.9595\n",
      "test_loss=5.6579, test_R²=-132.0788\n",
      "Epoch 474/1200\n",
      "Train loss: 6.4576, Train R²: -129.1430\n",
      "test_loss=5.6543, test_R²=-131.9922\n",
      "Epoch 475/1200\n",
      "Train loss: 6.4553, Train R²: -128.9536\n",
      "test_loss=5.6508, test_R²=-131.9053\n",
      "Epoch 476/1200\n",
      "Train loss: 6.4552, Train R²: -129.5956\n",
      "test_loss=5.6475, test_R²=-131.8243\n",
      "Epoch 477/1200\n",
      "Train loss: 6.4809, Train R²: -128.4513\n",
      "test_loss=5.6442, test_R²=-131.7430\n",
      "Epoch 478/1200\n",
      "Train loss: 6.4738, Train R²: -128.6668\n",
      "test_loss=5.6407, test_R²=-131.6591\n",
      "Epoch 479/1200\n",
      "Train loss: 6.4747, Train R²: -128.6221\n",
      "test_loss=5.6374, test_R²=-131.5774\n",
      "Epoch 480/1200\n",
      "Train loss: 6.3970, Train R²: -129.4681\n",
      "test_loss=5.6341, test_R²=-131.4965\n",
      "Epoch 481/1200\n",
      "Train loss: 6.4446, Train R²: -128.4693\n",
      "test_loss=5.6308, test_R²=-131.4151\n",
      "Epoch 482/1200\n",
      "Train loss: 6.4462, Train R²: -128.6747\n",
      "test_loss=5.6275, test_R²=-131.3343\n",
      "Epoch 483/1200\n",
      "Train loss: 6.4620, Train R²: -128.3246\n",
      "test_loss=5.6241, test_R²=-131.2528\n",
      "Epoch 484/1200\n",
      "Train loss: 6.4826, Train R²: -129.7814\n",
      "test_loss=5.6208, test_R²=-131.1709\n",
      "Epoch 485/1200\n",
      "Train loss: 6.3867, Train R²: -127.7734\n",
      "test_loss=5.6174, test_R²=-131.0877\n",
      "Epoch 486/1200\n",
      "Train loss: 6.4087, Train R²: -127.9125\n",
      "test_loss=5.6141, test_R²=-131.0077\n",
      "Epoch 487/1200\n",
      "Train loss: 6.4268, Train R²: -128.4522\n",
      "test_loss=5.6110, test_R²=-130.9299\n",
      "Epoch 488/1200\n",
      "Train loss: 6.4437, Train R²: -128.1077\n",
      "test_loss=5.6078, test_R²=-130.8511\n",
      "Epoch 489/1200\n",
      "Train loss: 6.3981, Train R²: -128.6428\n",
      "test_loss=5.6044, test_R²=-130.7698\n",
      "Epoch 490/1200\n",
      "Train loss: 6.4149, Train R²: -128.6023\n",
      "test_loss=5.6013, test_R²=-130.6932\n",
      "Epoch 491/1200\n",
      "Train loss: 6.4211, Train R²: -127.9116\n",
      "test_loss=5.5983, test_R²=-130.6181\n",
      "Epoch 492/1200\n",
      "Train loss: 6.4219, Train R²: -127.6284\n",
      "test_loss=5.5952, test_R²=-130.5428\n",
      "Epoch 493/1200\n",
      "Train loss: 6.4793, Train R²: -127.9654\n",
      "test_loss=5.5921, test_R²=-130.4669\n",
      "Epoch 494/1200\n",
      "Train loss: 6.4121, Train R²: -127.5684\n",
      "test_loss=5.5892, test_R²=-130.3944\n",
      "Epoch 495/1200\n",
      "Train loss: 6.4086, Train R²: -128.2335\n",
      "test_loss=5.5862, test_R²=-130.3200\n",
      "Epoch 496/1200\n",
      "Train loss: 6.4704, Train R²: -127.9108\n",
      "test_loss=5.5831, test_R²=-130.2442\n",
      "Epoch 497/1200\n",
      "Train loss: 6.4072, Train R²: -127.1559\n",
      "test_loss=5.5799, test_R²=-130.1663\n",
      "Epoch 498/1200\n",
      "Train loss: 6.3954, Train R²: -127.9357\n",
      "test_loss=5.5770, test_R²=-130.0953\n",
      "Epoch 499/1200\n",
      "Train loss: 6.4237, Train R²: -128.0546\n",
      "test_loss=5.5742, test_R²=-130.0262\n",
      "Epoch 500/1200\n",
      "Train loss: 6.3833, Train R²: -128.9104\n",
      "test_loss=5.5713, test_R²=-129.9556\n",
      "Epoch 501/1200\n",
      "Train loss: 6.3935, Train R²: -128.2028\n",
      "test_loss=5.5684, test_R²=-129.8836\n",
      "Epoch 502/1200\n",
      "Train loss: 6.4211, Train R²: -127.6835\n",
      "test_loss=5.5655, test_R²=-129.8134\n",
      "Epoch 503/1200\n",
      "Train loss: 6.4041, Train R²: -127.1982\n",
      "test_loss=5.5628, test_R²=-129.7447\n",
      "Epoch 504/1200\n",
      "Train loss: 6.3742, Train R²: -128.0386\n",
      "test_loss=5.5600, test_R²=-129.6771\n",
      "Epoch 505/1200\n",
      "Train loss: 6.4064, Train R²: -127.9085\n",
      "test_loss=5.5574, test_R²=-129.6123\n",
      "Epoch 506/1200\n",
      "Train loss: 6.3894, Train R²: -127.3263\n",
      "test_loss=5.5548, test_R²=-129.5484\n",
      "Epoch 507/1200\n",
      "Train loss: 6.4437, Train R²: -127.4354\n",
      "test_loss=5.5521, test_R²=-129.4833\n",
      "Epoch 508/1200\n",
      "Train loss: 6.4468, Train R²: -127.6626\n",
      "test_loss=5.5496, test_R²=-129.4212\n",
      "Epoch 509/1200\n",
      "Train loss: 6.3906, Train R²: -127.2022\n",
      "test_loss=5.5471, test_R²=-129.3602\n",
      "Epoch 510/1200\n",
      "Train loss: 6.4317, Train R²: -127.3809\n",
      "test_loss=5.5447, test_R²=-129.3005\n",
      "Epoch 511/1200\n",
      "Train loss: 6.4182, Train R²: -126.9944\n",
      "test_loss=5.5420, test_R²=-129.2352\n",
      "Epoch 512/1200\n",
      "Train loss: 6.4084, Train R²: -126.9345\n",
      "test_loss=5.5395, test_R²=-129.1719\n",
      "Epoch 513/1200\n",
      "Train loss: 6.4205, Train R²: -127.1531\n",
      "test_loss=5.5370, test_R²=-129.1100\n",
      "Epoch 514/1200\n",
      "Train loss: 6.3772, Train R²: -126.7342\n",
      "test_loss=5.5344, test_R²=-129.0472\n",
      "Epoch 515/1200\n",
      "Train loss: 6.3769, Train R²: -126.5800\n",
      "test_loss=5.5320, test_R²=-128.9864\n",
      "Epoch 516/1200\n",
      "Train loss: 6.3764, Train R²: -128.1568\n",
      "test_loss=5.5296, test_R²=-128.9282\n",
      "Epoch 517/1200\n",
      "Train loss: 6.3631, Train R²: -128.5230\n",
      "test_loss=5.5272, test_R²=-128.8668\n",
      "Epoch 518/1200\n",
      "Train loss: 6.3953, Train R²: -126.6359\n",
      "test_loss=5.5248, test_R²=-128.8088\n",
      "Epoch 519/1200\n",
      "Train loss: 6.3500, Train R²: -126.6394\n",
      "test_loss=5.5225, test_R²=-128.7517\n",
      "Epoch 520/1200\n",
      "Train loss: 6.3736, Train R²: -127.9187\n",
      "test_loss=5.5203, test_R²=-128.6967\n",
      "Epoch 521/1200\n",
      "Train loss: 6.3835, Train R²: -126.6901\n",
      "test_loss=5.5180, test_R²=-128.6407\n",
      "Epoch 522/1200\n",
      "Train loss: 6.4039, Train R²: -128.0391\n",
      "test_loss=5.5157, test_R²=-128.5837\n",
      "Epoch 523/1200\n",
      "Train loss: 6.3682, Train R²: -127.0315\n",
      "test_loss=5.5135, test_R²=-128.5280\n",
      "Epoch 524/1200\n",
      "Train loss: 6.4072, Train R²: -127.3126\n",
      "test_loss=5.5113, test_R²=-128.4738\n",
      "Epoch 525/1200\n",
      "Train loss: 6.3731, Train R²: -126.2951\n",
      "test_loss=5.5092, test_R²=-128.4213\n",
      "Epoch 526/1200\n",
      "Train loss: 6.3339, Train R²: -126.7529\n",
      "test_loss=5.5070, test_R²=-128.3676\n",
      "Epoch 527/1200\n",
      "Train loss: 6.3735, Train R²: -127.6979\n",
      "test_loss=5.5049, test_R²=-128.3159\n",
      "Epoch 528/1200\n",
      "Train loss: 6.4123, Train R²: -126.8176\n",
      "test_loss=5.5028, test_R²=-128.2655\n",
      "Epoch 529/1200\n",
      "Train loss: 6.3887, Train R²: -126.2580\n",
      "test_loss=5.5008, test_R²=-128.2157\n",
      "Epoch 530/1200\n",
      "Train loss: 6.3373, Train R²: -126.5982\n",
      "test_loss=5.4988, test_R²=-128.1643\n",
      "Epoch 531/1200\n",
      "Train loss: 6.3559, Train R²: -126.8977\n",
      "test_loss=5.4968, test_R²=-128.1162\n",
      "Epoch 532/1200\n",
      "Train loss: 6.3216, Train R²: -126.6781\n",
      "test_loss=5.4948, test_R²=-128.0674\n",
      "Epoch 533/1200\n",
      "Train loss: 6.3795, Train R²: -126.6711\n",
      "test_loss=5.4929, test_R²=-128.0194\n",
      "Epoch 534/1200\n",
      "Train loss: 6.3952, Train R²: -126.1255\n",
      "test_loss=5.4910, test_R²=-127.9733\n",
      "Epoch 535/1200\n",
      "Train loss: 6.3372, Train R²: -126.5171\n",
      "test_loss=5.4891, test_R²=-127.9237\n",
      "Epoch 536/1200\n",
      "Train loss: 6.3934, Train R²: -126.7092\n",
      "test_loss=5.4871, test_R²=-127.8741\n",
      "Epoch 537/1200\n",
      "Train loss: 6.3805, Train R²: -126.9122\n",
      "test_loss=5.4852, test_R²=-127.8268\n",
      "Epoch 538/1200\n",
      "Train loss: 6.4193, Train R²: -126.1201\n",
      "test_loss=5.4832, test_R²=-127.7785\n",
      "Epoch 539/1200\n",
      "Train loss: 6.3747, Train R²: -127.5310\n",
      "test_loss=5.4814, test_R²=-127.7337\n",
      "Epoch 540/1200\n",
      "Train loss: 6.3650, Train R²: -126.2112\n",
      "test_loss=5.4797, test_R²=-127.6895\n",
      "Epoch 541/1200\n",
      "Train loss: 6.3643, Train R²: -126.2061\n",
      "test_loss=5.4780, test_R²=-127.6477\n",
      "Epoch 542/1200\n",
      "Train loss: 6.3561, Train R²: -125.8854\n",
      "test_loss=5.4762, test_R²=-127.6039\n",
      "Epoch 543/1200\n",
      "Train loss: 6.3625, Train R²: -126.2827\n",
      "test_loss=5.4745, test_R²=-127.5598\n",
      "Epoch 544/1200\n",
      "Train loss: 6.3378, Train R²: -126.4812\n",
      "test_loss=5.4728, test_R²=-127.5184\n",
      "Epoch 545/1200\n",
      "Train loss: 6.3738, Train R²: -128.6300\n",
      "test_loss=5.4712, test_R²=-127.4780\n",
      "Epoch 546/1200\n",
      "Train loss: 6.3089, Train R²: -126.0037\n",
      "test_loss=5.4695, test_R²=-127.4359\n",
      "Epoch 547/1200\n",
      "Train loss: 6.3755, Train R²: -126.6223\n",
      "test_loss=5.4679, test_R²=-127.3941\n",
      "Epoch 548/1200\n",
      "Train loss: 6.3651, Train R²: -128.2234\n",
      "test_loss=5.4663, test_R²=-127.3550\n",
      "Epoch 549/1200\n",
      "Train loss: 6.3656, Train R²: -125.4980\n",
      "test_loss=5.4646, test_R²=-127.3142\n",
      "Epoch 550/1200\n",
      "Train loss: 6.3439, Train R²: -126.3143\n",
      "test_loss=5.4631, test_R²=-127.2757\n",
      "Epoch 551/1200\n",
      "Train loss: 6.3399, Train R²: -126.2598\n",
      "test_loss=5.4616, test_R²=-127.2378\n",
      "Epoch 552/1200\n",
      "Train loss: 6.2957, Train R²: -125.6176\n",
      "test_loss=5.4600, test_R²=-127.1995\n",
      "Epoch 553/1200\n",
      "Train loss: 6.3219, Train R²: -125.9157\n",
      "test_loss=5.4585, test_R²=-127.1625\n",
      "Epoch 554/1200\n",
      "Train loss: 6.4030, Train R²: -125.9649\n",
      "test_loss=5.4570, test_R²=-127.1255\n",
      "Epoch 555/1200\n",
      "Train loss: 6.3828, Train R²: -126.5019\n",
      "test_loss=5.4556, test_R²=-127.0888\n",
      "Epoch 556/1200\n",
      "Train loss: 6.3565, Train R²: -126.5940\n",
      "test_loss=5.4541, test_R²=-127.0518\n",
      "Epoch 557/1200\n",
      "Train loss: 6.3486, Train R²: -126.2812\n",
      "test_loss=5.4528, test_R²=-127.0188\n",
      "Epoch 558/1200\n",
      "Train loss: 6.3351, Train R²: -125.5294\n",
      "test_loss=5.4513, test_R²=-126.9822\n",
      "Epoch 559/1200\n",
      "Train loss: 6.3497, Train R²: -125.8447\n",
      "test_loss=5.4499, test_R²=-126.9471\n",
      "Epoch 560/1200\n",
      "Train loss: 6.3648, Train R²: -125.7867\n",
      "test_loss=5.4485, test_R²=-126.9124\n",
      "Epoch 561/1200\n",
      "Train loss: 6.3839, Train R²: -125.6086\n",
      "test_loss=5.4470, test_R²=-126.8763\n",
      "Epoch 562/1200\n",
      "Train loss: 6.3332, Train R²: -125.6496\n",
      "test_loss=5.4457, test_R²=-126.8422\n",
      "Epoch 563/1200\n",
      "Train loss: 6.3256, Train R²: -126.2579\n",
      "test_loss=5.4444, test_R²=-126.8094\n",
      "Epoch 564/1200\n",
      "Train loss: 6.3774, Train R²: -125.3914\n",
      "test_loss=5.4430, test_R²=-126.7763\n",
      "Epoch 565/1200\n",
      "Train loss: 6.3576, Train R²: -127.0466\n",
      "test_loss=5.4417, test_R²=-126.7436\n",
      "Epoch 566/1200\n",
      "Train loss: 6.3481, Train R²: -126.8465\n",
      "test_loss=5.4406, test_R²=-126.7137\n",
      "Epoch 567/1200\n",
      "Train loss: 6.3673, Train R²: -125.6121\n",
      "test_loss=5.4394, test_R²=-126.6848\n",
      "Epoch 568/1200\n",
      "Train loss: 6.3831, Train R²: -125.9066\n",
      "test_loss=5.4382, test_R²=-126.6537\n",
      "Epoch 569/1200\n",
      "Train loss: 6.3659, Train R²: -127.0431\n",
      "test_loss=5.4370, test_R²=-126.6243\n",
      "Epoch 570/1200\n",
      "Train loss: 6.3941, Train R²: -125.8097\n",
      "test_loss=5.4359, test_R²=-126.5980\n",
      "Epoch 571/1200\n",
      "Train loss: 6.3585, Train R²: -125.5823\n",
      "test_loss=5.4349, test_R²=-126.5706\n",
      "Epoch 572/1200\n",
      "Train loss: 6.3533, Train R²: -125.6357\n",
      "test_loss=5.4337, test_R²=-126.5413\n",
      "Epoch 573/1200\n",
      "Train loss: 6.3399, Train R²: -125.8778\n",
      "test_loss=5.4325, test_R²=-126.5116\n",
      "Epoch 574/1200\n",
      "Train loss: 6.3642, Train R²: -125.7442\n",
      "test_loss=5.4313, test_R²=-126.4819\n",
      "Epoch 575/1200\n",
      "Train loss: 6.3270, Train R²: -126.5411\n",
      "test_loss=5.4301, test_R²=-126.4507\n",
      "Epoch 576/1200\n",
      "Train loss: 6.3238, Train R²: -125.4477\n",
      "test_loss=5.4289, test_R²=-126.4213\n",
      "Epoch 577/1200\n",
      "Train loss: 6.2928, Train R²: -125.4508\n",
      "test_loss=5.4277, test_R²=-126.3907\n",
      "Epoch 578/1200\n",
      "Train loss: 6.3513, Train R²: -125.5312\n",
      "test_loss=5.4266, test_R²=-126.3610\n",
      "Epoch 579/1200\n",
      "Train loss: 6.3356, Train R²: -125.5683\n",
      "test_loss=5.4253, test_R²=-126.3302\n",
      "Epoch 580/1200\n",
      "Train loss: 6.3603, Train R²: -125.5458\n",
      "test_loss=5.4243, test_R²=-126.3034\n",
      "Epoch 581/1200\n",
      "Train loss: 6.3518, Train R²: -125.7329\n",
      "test_loss=5.4233, test_R²=-126.2778\n",
      "Epoch 582/1200\n",
      "Train loss: 6.3250, Train R²: -125.0544\n",
      "test_loss=5.4222, test_R²=-126.2504\n",
      "Epoch 583/1200\n",
      "Train loss: 6.3456, Train R²: -127.3331\n",
      "test_loss=5.4211, test_R²=-126.2243\n",
      "Epoch 584/1200\n",
      "Train loss: 6.3263, Train R²: -126.9156\n",
      "test_loss=5.4201, test_R²=-126.1968\n",
      "Epoch 585/1200\n",
      "Train loss: 6.2840, Train R²: -125.4201\n",
      "test_loss=5.4190, test_R²=-126.1707\n",
      "Epoch 586/1200\n",
      "Train loss: 6.3608, Train R²: -125.9980\n",
      "test_loss=5.4180, test_R²=-126.1456\n",
      "Epoch 587/1200\n",
      "Train loss: 6.3321, Train R²: -127.6716\n",
      "test_loss=5.4171, test_R²=-126.1218\n",
      "Epoch 588/1200\n",
      "Train loss: 6.3236, Train R²: -125.5476\n",
      "test_loss=5.4162, test_R²=-126.0980\n",
      "Epoch 589/1200\n",
      "Train loss: 6.3170, Train R²: -126.3387\n",
      "test_loss=5.4152, test_R²=-126.0738\n",
      "Epoch 590/1200\n",
      "Train loss: 6.3619, Train R²: -124.9341\n",
      "test_loss=5.4142, test_R²=-126.0493\n",
      "Epoch 591/1200\n",
      "Train loss: 6.3352, Train R²: -124.9984\n",
      "test_loss=5.4133, test_R²=-126.0262\n",
      "Epoch 592/1200\n",
      "Train loss: 6.3225, Train R²: -125.5790\n",
      "test_loss=5.4123, test_R²=-126.0024\n",
      "Epoch 593/1200\n",
      "Train loss: 6.3211, Train R²: -125.1530\n",
      "test_loss=5.4114, test_R²=-125.9785\n",
      "Epoch 594/1200\n",
      "Train loss: 6.3487, Train R²: -126.5471\n",
      "test_loss=5.4105, test_R²=-125.9570\n",
      "Epoch 595/1200\n",
      "Train loss: 6.3270, Train R²: -125.0954\n",
      "test_loss=5.4098, test_R²=-125.9371\n",
      "Epoch 596/1200\n",
      "Train loss: 6.3696, Train R²: -125.3357\n",
      "test_loss=5.4088, test_R²=-125.9129\n",
      "Epoch 597/1200\n",
      "Train loss: 6.3461, Train R²: -127.2556\n",
      "test_loss=5.4079, test_R²=-125.8902\n",
      "Epoch 598/1200\n",
      "Train loss: 6.3209, Train R²: -125.0129\n",
      "test_loss=5.4070, test_R²=-125.8673\n",
      "Epoch 599/1200\n",
      "Train loss: 6.3159, Train R²: -125.2283\n",
      "test_loss=5.4061, test_R²=-125.8427\n",
      "Epoch 600/1200\n",
      "Train loss: 6.3458, Train R²: -125.5185\n",
      "test_loss=5.4052, test_R²=-125.8202\n",
      "Epoch 601/1200\n",
      "Train loss: 6.3280, Train R²: -125.6099\n",
      "test_loss=5.4043, test_R²=-125.7976\n",
      "Epoch 602/1200\n",
      "Train loss: 6.3360, Train R²: -125.2716\n",
      "test_loss=5.4034, test_R²=-125.7756\n",
      "Epoch 603/1200\n",
      "Train loss: 6.3761, Train R²: -126.0271\n",
      "test_loss=5.4026, test_R²=-125.7534\n",
      "Epoch 604/1200\n",
      "Train loss: 6.3266, Train R²: -125.2100\n",
      "test_loss=5.4017, test_R²=-125.7308\n",
      "Epoch 605/1200\n",
      "Train loss: 6.3621, Train R²: -126.1762\n",
      "test_loss=5.4009, test_R²=-125.7105\n",
      "Epoch 606/1200\n",
      "Train loss: 6.3623, Train R²: -125.5086\n",
      "test_loss=5.4001, test_R²=-125.6912\n",
      "Epoch 607/1200\n",
      "Train loss: 6.3504, Train R²: -126.2765\n",
      "test_loss=5.3994, test_R²=-125.6735\n",
      "Epoch 608/1200\n",
      "Train loss: 6.3133, Train R²: -124.6478\n",
      "test_loss=5.3988, test_R²=-125.6569\n",
      "Epoch 609/1200\n",
      "Train loss: 6.2664, Train R²: -125.1809\n",
      "test_loss=5.3981, test_R²=-125.6380\n",
      "Epoch 610/1200\n",
      "Train loss: 6.3096, Train R²: -125.1570\n",
      "test_loss=5.3973, test_R²=-125.6187\n",
      "Epoch 611/1200\n",
      "Train loss: 6.3469, Train R²: -124.7113\n",
      "test_loss=5.3966, test_R²=-125.6004\n",
      "Epoch 612/1200\n",
      "Train loss: 6.3533, Train R²: -125.4203\n",
      "test_loss=5.3959, test_R²=-125.5823\n",
      "Epoch 613/1200\n",
      "Train loss: 6.3480, Train R²: -124.8127\n",
      "test_loss=5.3953, test_R²=-125.5667\n",
      "Epoch 614/1200\n",
      "Train loss: 6.3447, Train R²: -125.5129\n",
      "test_loss=5.3945, test_R²=-125.5477\n",
      "Epoch 615/1200\n",
      "Train loss: 6.3034, Train R²: -125.5999\n",
      "test_loss=5.3939, test_R²=-125.5312\n",
      "Epoch 616/1200\n",
      "Train loss: 6.3426, Train R²: -124.7044\n",
      "test_loss=5.3932, test_R²=-125.5147\n",
      "Epoch 617/1200\n",
      "Train loss: 6.3133, Train R²: -124.9028\n",
      "test_loss=5.3925, test_R²=-125.4971\n",
      "Epoch 618/1200\n",
      "Train loss: 6.3270, Train R²: -125.6767\n",
      "test_loss=5.3918, test_R²=-125.4787\n",
      "Epoch 619/1200\n",
      "Train loss: 6.2962, Train R²: -124.7962\n",
      "test_loss=5.3912, test_R²=-125.4631\n",
      "Epoch 620/1200\n",
      "Train loss: 6.3568, Train R²: -124.9032\n",
      "test_loss=5.3907, test_R²=-125.4501\n",
      "Epoch 621/1200\n",
      "Train loss: 6.3995, Train R²: -124.7183\n",
      "test_loss=5.3902, test_R²=-125.4355\n",
      "Epoch 622/1200\n",
      "Train loss: 6.3225, Train R²: -124.6908\n",
      "test_loss=5.3896, test_R²=-125.4211\n",
      "Epoch 623/1200\n",
      "Train loss: 6.3156, Train R²: -124.9909\n",
      "test_loss=5.3890, test_R²=-125.4067\n",
      "Epoch 624/1200\n",
      "Train loss: 6.2893, Train R²: -125.3375\n",
      "test_loss=5.3885, test_R²=-125.3919\n",
      "Epoch 625/1200\n",
      "Train loss: 6.3159, Train R²: -125.7305\n",
      "test_loss=5.3880, test_R²=-125.3800\n",
      "Epoch 626/1200\n",
      "Train loss: 6.3571, Train R²: -124.6169\n",
      "test_loss=5.3875, test_R²=-125.3665\n",
      "Epoch 627/1200\n",
      "Train loss: 6.3936, Train R²: -125.3851\n",
      "test_loss=5.3869, test_R²=-125.3520\n",
      "Epoch 628/1200\n",
      "Train loss: 6.2925, Train R²: -126.1636\n",
      "test_loss=5.3865, test_R²=-125.3399\n",
      "Epoch 629/1200\n",
      "Train loss: 6.2976, Train R²: -125.4772\n",
      "test_loss=5.3859, test_R²=-125.3263\n",
      "Epoch 630/1200\n",
      "Train loss: 6.3163, Train R²: -124.7104\n",
      "test_loss=5.3855, test_R²=-125.3148\n",
      "Epoch 631/1200\n",
      "Train loss: 6.3121, Train R²: -125.1096\n",
      "test_loss=5.3850, test_R²=-125.3012\n",
      "Epoch 632/1200\n",
      "Train loss: 6.3364, Train R²: -125.1319\n",
      "test_loss=5.3844, test_R²=-125.2877\n",
      "Epoch 633/1200\n",
      "Train loss: 6.3483, Train R²: -125.0639\n",
      "test_loss=5.3840, test_R²=-125.2752\n",
      "Epoch 634/1200\n",
      "Train loss: 6.2990, Train R²: -125.4720\n",
      "test_loss=5.3835, test_R²=-125.2634\n",
      "Epoch 635/1200\n",
      "Train loss: 6.2934, Train R²: -124.6301\n",
      "test_loss=5.3830, test_R²=-125.2508\n",
      "Epoch 636/1200\n",
      "Train loss: 6.3678, Train R²: -125.6115\n",
      "test_loss=5.3826, test_R²=-125.2402\n",
      "Epoch 637/1200\n",
      "Train loss: 6.3226, Train R²: -125.8120\n",
      "test_loss=5.3822, test_R²=-125.2288\n",
      "Epoch 638/1200\n",
      "Train loss: 6.3348, Train R²: -124.9548\n",
      "test_loss=5.3818, test_R²=-125.2185\n",
      "Epoch 639/1200\n",
      "Train loss: 6.3364, Train R²: -125.9640\n",
      "test_loss=5.3814, test_R²=-125.2087\n",
      "Epoch 640/1200\n",
      "Train loss: 6.3131, Train R²: -125.1683\n",
      "test_loss=5.3810, test_R²=-125.1983\n",
      "Epoch 641/1200\n",
      "Train loss: 6.2981, Train R²: -124.9508\n",
      "test_loss=5.3806, test_R²=-125.1871\n",
      "Epoch 642/1200\n",
      "Train loss: 6.3134, Train R²: -124.5011\n",
      "test_loss=5.3801, test_R²=-125.1754\n",
      "Epoch 643/1200\n",
      "Train loss: 6.3145, Train R²: -125.1231\n",
      "test_loss=5.3796, test_R²=-125.1627\n",
      "Epoch 644/1200\n",
      "Train loss: 6.3484, Train R²: -124.8552\n",
      "test_loss=5.3792, test_R²=-125.1515\n",
      "Epoch 645/1200\n",
      "Train loss: 6.3220, Train R²: -124.6966\n",
      "test_loss=5.3789, test_R²=-125.1418\n",
      "Epoch 646/1200\n",
      "Train loss: 6.3508, Train R²: -124.4207\n",
      "test_loss=5.3785, test_R²=-125.1321\n",
      "Epoch 647/1200\n",
      "Train loss: 6.3115, Train R²: -125.0969\n",
      "test_loss=5.3781, test_R²=-125.1226\n",
      "Epoch 648/1200\n",
      "Train loss: 6.2875, Train R²: -124.1128\n",
      "test_loss=5.3778, test_R²=-125.1131\n",
      "Epoch 649/1200\n",
      "Train loss: 6.3533, Train R²: -124.9461\n",
      "test_loss=5.3774, test_R²=-125.1027\n",
      "Epoch 650/1200\n",
      "Train loss: 6.2997, Train R²: -124.8832\n",
      "test_loss=5.3770, test_R²=-125.0925\n",
      "Epoch 651/1200\n",
      "Train loss: 6.2990, Train R²: -124.4398\n",
      "test_loss=5.3766, test_R²=-125.0827\n",
      "Epoch 652/1200\n",
      "Train loss: 6.3440, Train R²: -126.6400\n",
      "test_loss=5.3763, test_R²=-125.0747\n",
      "Epoch 653/1200\n",
      "Train loss: 6.3307, Train R²: -124.5483\n",
      "test_loss=5.3760, test_R²=-125.0663\n",
      "Epoch 654/1200\n",
      "Train loss: 6.3286, Train R²: -125.2136\n",
      "test_loss=5.3757, test_R²=-125.0572\n",
      "Epoch 655/1200\n",
      "Train loss: 6.3240, Train R²: -124.1355\n",
      "test_loss=5.3753, test_R²=-125.0474\n",
      "Epoch 656/1200\n",
      "Train loss: 6.3290, Train R²: -124.6305\n",
      "test_loss=5.3749, test_R²=-125.0367\n",
      "Epoch 657/1200\n",
      "Train loss: 6.2880, Train R²: -124.2981\n",
      "test_loss=5.3745, test_R²=-125.0270\n",
      "Epoch 658/1200\n",
      "Train loss: 6.3023, Train R²: -125.1471\n",
      "test_loss=5.3741, test_R²=-125.0170\n",
      "Epoch 659/1200\n",
      "Train loss: 6.3313, Train R²: -124.8617\n",
      "test_loss=5.3737, test_R²=-125.0053\n",
      "Epoch 660/1200\n",
      "Train loss: 6.3030, Train R²: -125.1499\n",
      "test_loss=5.3733, test_R²=-124.9950\n",
      "Epoch 661/1200\n",
      "Train loss: 6.2831, Train R²: -124.4351\n",
      "test_loss=5.3730, test_R²=-124.9861\n",
      "Epoch 662/1200\n",
      "Train loss: 6.3052, Train R²: -124.1581\n",
      "test_loss=5.3727, test_R²=-124.9780\n",
      "Epoch 663/1200\n",
      "Train loss: 6.2839, Train R²: -125.6165\n",
      "test_loss=5.3724, test_R²=-124.9703\n",
      "Epoch 664/1200\n",
      "Train loss: 6.3446, Train R²: -124.4660\n",
      "test_loss=5.3721, test_R²=-124.9620\n",
      "Epoch 665/1200\n",
      "Train loss: 6.2892, Train R²: -124.4724\n",
      "test_loss=5.3718, test_R²=-124.9548\n",
      "Epoch 666/1200\n",
      "Train loss: 6.3185, Train R²: -124.6720\n",
      "test_loss=5.3715, test_R²=-124.9472\n",
      "Epoch 667/1200\n",
      "Train loss: 6.3011, Train R²: -124.4536\n",
      "test_loss=5.3712, test_R²=-124.9393\n",
      "Epoch 668/1200\n",
      "Train loss: 6.3268, Train R²: -125.0999\n",
      "test_loss=5.3709, test_R²=-124.9309\n",
      "Epoch 669/1200\n",
      "Train loss: 6.2911, Train R²: -125.6737\n",
      "test_loss=5.3707, test_R²=-124.9252\n",
      "Epoch 670/1200\n",
      "Train loss: 6.3409, Train R²: -124.4755\n",
      "test_loss=5.3705, test_R²=-124.9200\n",
      "Epoch 671/1200\n",
      "Train loss: 6.3102, Train R²: -124.7762\n",
      "test_loss=5.3703, test_R²=-124.9135\n",
      "Epoch 672/1200\n",
      "Train loss: 6.3036, Train R²: -124.2212\n",
      "test_loss=5.3700, test_R²=-124.9066\n",
      "Epoch 673/1200\n",
      "Train loss: 6.3109, Train R²: -124.3190\n",
      "test_loss=5.3698, test_R²=-124.8999\n",
      "Epoch 674/1200\n",
      "Train loss: 6.3072, Train R²: -124.2456\n",
      "test_loss=5.3695, test_R²=-124.8918\n",
      "Epoch 675/1200\n",
      "Train loss: 6.3200, Train R²: -124.6639\n",
      "test_loss=5.3691, test_R²=-124.8820\n",
      "Epoch 676/1200\n",
      "Train loss: 6.3013, Train R²: -124.6319\n",
      "test_loss=5.3688, test_R²=-124.8729\n",
      "Epoch 677/1200\n",
      "Train loss: 6.2868, Train R²: -124.4396\n",
      "test_loss=5.3685, test_R²=-124.8650\n",
      "Epoch 678/1200\n",
      "Train loss: 6.3121, Train R²: -124.4597\n",
      "test_loss=5.3682, test_R²=-124.8583\n",
      "Epoch 679/1200\n",
      "Train loss: 6.3363, Train R²: -124.6430\n",
      "test_loss=5.3679, test_R²=-124.8514\n",
      "Epoch 680/1200\n",
      "Train loss: 6.3571, Train R²: -124.6390\n",
      "test_loss=5.3677, test_R²=-124.8455\n",
      "Epoch 681/1200\n",
      "Train loss: 6.3079, Train R²: -124.2656\n",
      "test_loss=5.3676, test_R²=-124.8408\n",
      "Epoch 682/1200\n",
      "Train loss: 6.3315, Train R²: -125.2159\n",
      "test_loss=5.3674, test_R²=-124.8358\n",
      "Epoch 683/1200\n",
      "Train loss: 6.2738, Train R²: -125.5111\n",
      "test_loss=5.3672, test_R²=-124.8316\n",
      "Epoch 684/1200\n",
      "Train loss: 6.3152, Train R²: -124.5444\n",
      "test_loss=5.3670, test_R²=-124.8262\n",
      "Epoch 685/1200\n",
      "Train loss: 6.3116, Train R²: -124.1667\n",
      "test_loss=5.3668, test_R²=-124.8203\n",
      "Epoch 686/1200\n",
      "Train loss: 6.3223, Train R²: -124.7410\n",
      "test_loss=5.3666, test_R²=-124.8141\n",
      "Epoch 687/1200\n",
      "Train loss: 6.3204, Train R²: -124.1860\n",
      "test_loss=5.3664, test_R²=-124.8076\n",
      "Epoch 688/1200\n",
      "Train loss: 6.3329, Train R²: -124.4012\n",
      "test_loss=5.3661, test_R²=-124.8002\n",
      "Epoch 689/1200\n",
      "Train loss: 6.3249, Train R²: -124.8553\n",
      "test_loss=5.3659, test_R²=-124.7940\n",
      "Epoch 690/1200\n",
      "Train loss: 6.2973, Train R²: -124.3937\n",
      "test_loss=5.3657, test_R²=-124.7899\n",
      "Epoch 691/1200\n",
      "Train loss: 6.3280, Train R²: -123.9378\n",
      "test_loss=5.3655, test_R²=-124.7851\n",
      "Epoch 692/1200\n",
      "Train loss: 6.3176, Train R²: -125.3101\n",
      "test_loss=5.3654, test_R²=-124.7818\n",
      "Epoch 693/1200\n",
      "Train loss: 6.3551, Train R²: -124.3016\n",
      "test_loss=5.3653, test_R²=-124.7786\n",
      "Epoch 694/1200\n",
      "Train loss: 6.3280, Train R²: -124.8982\n",
      "test_loss=5.3652, test_R²=-124.7745\n",
      "Epoch 695/1200\n",
      "Train loss: 6.3445, Train R²: -124.6222\n",
      "test_loss=5.3650, test_R²=-124.7702\n",
      "Epoch 696/1200\n",
      "Train loss: 6.2809, Train R²: -124.1273\n",
      "test_loss=5.3648, test_R²=-124.7658\n",
      "Epoch 697/1200\n",
      "Train loss: 6.3228, Train R²: -125.5821\n",
      "test_loss=5.3647, test_R²=-124.7617\n",
      "Epoch 698/1200\n",
      "Train loss: 6.3422, Train R²: -124.1557\n",
      "test_loss=5.3645, test_R²=-124.7573\n",
      "Epoch 699/1200\n",
      "Train loss: 6.3636, Train R²: -126.0346\n",
      "test_loss=5.3643, test_R²=-124.7527\n",
      "Epoch 700/1200\n",
      "Train loss: 6.3121, Train R²: -124.1886\n",
      "test_loss=5.3642, test_R²=-124.7495\n",
      "Epoch 701/1200\n",
      "Train loss: 6.2922, Train R²: -123.8746\n",
      "test_loss=5.3641, test_R²=-124.7458\n",
      "Epoch 702/1200\n",
      "Train loss: 6.3762, Train R²: -124.2943\n",
      "test_loss=5.3640, test_R²=-124.7419\n",
      "Epoch 703/1200\n",
      "Train loss: 6.3744, Train R²: -124.3328\n",
      "test_loss=5.3638, test_R²=-124.7372\n",
      "Epoch 704/1200\n",
      "Train loss: 6.3168, Train R²: -124.0737\n",
      "test_loss=5.3636, test_R²=-124.7332\n",
      "Epoch 705/1200\n",
      "Train loss: 6.2644, Train R²: -125.5771\n",
      "test_loss=5.3635, test_R²=-124.7283\n",
      "Epoch 706/1200\n",
      "Train loss: 6.2869, Train R²: -124.9098\n",
      "test_loss=5.3633, test_R²=-124.7226\n",
      "Epoch 707/1200\n",
      "Train loss: 6.3075, Train R²: -124.2820\n",
      "test_loss=5.3631, test_R²=-124.7173\n",
      "Epoch 708/1200\n",
      "Train loss: 6.3208, Train R²: -124.5310\n",
      "test_loss=5.3629, test_R²=-124.7134\n",
      "Epoch 709/1200\n",
      "Train loss: 6.3282, Train R²: -124.5912\n",
      "test_loss=5.3628, test_R²=-124.7096\n",
      "Epoch 710/1200\n",
      "Train loss: 6.3377, Train R²: -124.0970\n",
      "test_loss=5.3627, test_R²=-124.7061\n",
      "Epoch 711/1200\n",
      "Train loss: 6.3141, Train R²: -124.8651\n",
      "test_loss=5.3625, test_R²=-124.7030\n",
      "Epoch 712/1200\n",
      "Train loss: 6.2697, Train R²: -125.3575\n",
      "test_loss=5.3624, test_R²=-124.6998\n",
      "Epoch 713/1200\n",
      "Train loss: 6.3168, Train R²: -124.6691\n",
      "test_loss=5.3623, test_R²=-124.6955\n",
      "Epoch 714/1200\n",
      "Train loss: 6.2891, Train R²: -124.5528\n",
      "test_loss=5.3621, test_R²=-124.6910\n",
      "Epoch 715/1200\n",
      "Train loss: 6.3088, Train R²: -124.6069\n",
      "test_loss=5.3620, test_R²=-124.6877\n",
      "Epoch 716/1200\n",
      "Train loss: 6.3091, Train R²: -124.5257\n",
      "test_loss=5.3618, test_R²=-124.6824\n",
      "Epoch 717/1200\n",
      "Train loss: 6.3237, Train R²: -123.9971\n",
      "test_loss=5.3617, test_R²=-124.6791\n",
      "Epoch 718/1200\n",
      "Train loss: 6.3220, Train R²: -123.9760\n",
      "test_loss=5.3616, test_R²=-124.6768\n",
      "Epoch 719/1200\n",
      "Train loss: 6.3232, Train R²: -124.8497\n",
      "test_loss=5.3615, test_R²=-124.6736\n",
      "Epoch 720/1200\n",
      "Train loss: 6.2905, Train R²: -125.2179\n",
      "test_loss=5.3614, test_R²=-124.6708\n",
      "Epoch 721/1200\n",
      "Train loss: 6.3269, Train R²: -125.3455\n",
      "test_loss=5.3613, test_R²=-124.6668\n",
      "Epoch 722/1200\n",
      "Train loss: 6.2906, Train R²: -123.9431\n",
      "test_loss=5.3612, test_R²=-124.6628\n",
      "Epoch 723/1200\n",
      "Train loss: 6.3267, Train R²: -124.2759\n",
      "test_loss=5.3610, test_R²=-124.6591\n",
      "Epoch 724/1200\n",
      "Train loss: 6.3204, Train R²: -123.7593\n",
      "test_loss=5.3609, test_R²=-124.6554\n",
      "Epoch 725/1200\n",
      "Train loss: 6.2886, Train R²: -126.2810\n",
      "test_loss=5.3607, test_R²=-124.6512\n",
      "Epoch 726/1200\n",
      "Train loss: 6.2944, Train R²: -124.7332\n",
      "test_loss=5.3606, test_R²=-124.6468\n",
      "Epoch 727/1200\n",
      "Train loss: 6.3045, Train R²: -124.3516\n",
      "test_loss=5.3605, test_R²=-124.6440\n",
      "Epoch 728/1200\n",
      "Train loss: 6.2986, Train R²: -124.4307\n",
      "test_loss=5.3605, test_R²=-124.6437\n",
      "Epoch 729/1200\n",
      "Train loss: 6.3170, Train R²: -124.2373\n",
      "test_loss=5.3605, test_R²=-124.6425\n",
      "Epoch 730/1200\n",
      "Train loss: 6.2782, Train R²: -124.3357\n",
      "test_loss=5.3604, test_R²=-124.6407\n",
      "Epoch 731/1200\n",
      "Train loss: 6.3213, Train R²: -124.4154\n",
      "test_loss=5.3603, test_R²=-124.6382\n",
      "Epoch 732/1200\n",
      "Train loss: 6.2998, Train R²: -124.0167\n",
      "test_loss=5.3602, test_R²=-124.6363\n",
      "Epoch 733/1200\n",
      "Train loss: 6.2960, Train R²: -124.3450\n",
      "test_loss=5.3601, test_R²=-124.6332\n",
      "Epoch 734/1200\n",
      "Train loss: 6.2878, Train R²: -124.6339\n",
      "test_loss=5.3600, test_R²=-124.6302\n",
      "Epoch 735/1200\n",
      "Train loss: 6.3137, Train R²: -124.3670\n",
      "test_loss=5.3600, test_R²=-124.6292\n",
      "Epoch 736/1200\n",
      "Train loss: 6.2752, Train R²: -124.1238\n",
      "test_loss=5.3599, test_R²=-124.6276\n",
      "Epoch 737/1200\n",
      "Train loss: 6.3059, Train R²: -124.1582\n",
      "test_loss=5.3598, test_R²=-124.6247\n",
      "Epoch 738/1200\n",
      "Train loss: 6.3403, Train R²: -124.7821\n",
      "test_loss=5.3597, test_R²=-124.6216\n",
      "Epoch 739/1200\n",
      "Train loss: 6.3061, Train R²: -125.3791\n",
      "test_loss=5.3596, test_R²=-124.6192\n",
      "Epoch 740/1200\n",
      "Train loss: 6.2777, Train R²: -124.3487\n",
      "test_loss=5.3596, test_R²=-124.6172\n",
      "Epoch 741/1200\n",
      "Train loss: 6.3217, Train R²: -124.3421\n",
      "test_loss=5.3594, test_R²=-124.6142\n",
      "Epoch 742/1200\n",
      "Train loss: 6.3088, Train R²: -124.4028\n",
      "test_loss=5.3594, test_R²=-124.6122\n",
      "Epoch 743/1200\n",
      "Train loss: 6.3015, Train R²: -124.3059\n",
      "test_loss=5.3593, test_R²=-124.6107\n",
      "Epoch 744/1200\n",
      "Train loss: 6.3356, Train R²: -124.3132\n",
      "test_loss=5.3593, test_R²=-124.6092\n",
      "Epoch 745/1200\n",
      "Train loss: 6.3536, Train R²: -124.5107\n",
      "test_loss=5.3592, test_R²=-124.6071\n",
      "Epoch 746/1200\n",
      "Train loss: 6.2832, Train R²: -126.6955\n",
      "test_loss=5.3592, test_R²=-124.6062\n",
      "Epoch 747/1200\n",
      "Train loss: 6.3381, Train R²: -124.7204\n",
      "test_loss=5.3591, test_R²=-124.6051\n",
      "Epoch 748/1200\n",
      "Train loss: 6.3353, Train R²: -124.3712\n",
      "test_loss=5.3592, test_R²=-124.6062\n",
      "Epoch 749/1200\n",
      "Train loss: 6.3054, Train R²: -124.5445\n",
      "test_loss=5.3592, test_R²=-124.6059\n",
      "Epoch 750/1200\n",
      "Train loss: 6.2896, Train R²: -123.8152\n",
      "test_loss=5.3591, test_R²=-124.6039\n",
      "Epoch 751/1200\n",
      "Train loss: 6.2987, Train R²: -124.2664\n",
      "test_loss=5.3591, test_R²=-124.6019\n",
      "Epoch 752/1200\n",
      "Train loss: 6.3139, Train R²: -123.9492\n",
      "test_loss=5.3590, test_R²=-124.5991\n",
      "Epoch 753/1200\n",
      "Train loss: 6.3013, Train R²: -124.6562\n",
      "test_loss=5.3589, test_R²=-124.5968\n",
      "Epoch 754/1200\n",
      "Train loss: 6.3199, Train R²: -124.5628\n",
      "test_loss=5.3588, test_R²=-124.5938\n",
      "Epoch 755/1200\n",
      "Train loss: 6.2905, Train R²: -124.1085\n",
      "test_loss=5.3587, test_R²=-124.5917\n",
      "Epoch 756/1200\n",
      "Train loss: 6.3437, Train R²: -124.7069\n",
      "test_loss=5.3586, test_R²=-124.5895\n",
      "Epoch 757/1200\n",
      "Train loss: 6.2778, Train R²: -124.9612\n",
      "test_loss=5.3586, test_R²=-124.5888\n",
      "Epoch 758/1200\n",
      "Train loss: 6.3404, Train R²: -124.2074\n",
      "test_loss=5.3586, test_R²=-124.5876\n",
      "Epoch 759/1200\n",
      "Train loss: 6.2832, Train R²: -124.1783\n",
      "test_loss=5.3585, test_R²=-124.5856\n",
      "Epoch 760/1200\n",
      "Train loss: 6.2987, Train R²: -124.3335\n",
      "test_loss=5.3584, test_R²=-124.5836\n",
      "Epoch 761/1200\n",
      "Train loss: 6.2719, Train R²: -124.6656\n",
      "test_loss=5.3585, test_R²=-124.5838\n",
      "Epoch 762/1200\n",
      "Train loss: 6.3253, Train R²: -124.1505\n",
      "test_loss=5.3585, test_R²=-124.5837\n",
      "Epoch 763/1200\n",
      "Train loss: 6.3280, Train R²: -124.4346\n",
      "test_loss=5.3584, test_R²=-124.5815\n",
      "Epoch 764/1200\n",
      "Train loss: 6.3129, Train R²: -124.1605\n",
      "test_loss=5.3583, test_R²=-124.5795\n",
      "Epoch 765/1200\n",
      "Train loss: 6.3033, Train R²: -124.2584\n",
      "test_loss=5.3582, test_R²=-124.5767\n",
      "Epoch 766/1200\n",
      "Train loss: 6.3298, Train R²: -125.5941\n",
      "test_loss=5.3581, test_R²=-124.5733\n",
      "Epoch 767/1200\n",
      "Train loss: 6.3099, Train R²: -124.5749\n",
      "test_loss=5.3580, test_R²=-124.5712\n",
      "Epoch 768/1200\n",
      "Train loss: 6.3349, Train R²: -123.9950\n",
      "test_loss=5.3580, test_R²=-124.5700\n",
      "Epoch 769/1200\n",
      "Train loss: 6.2925, Train R²: -124.1784\n",
      "test_loss=5.3580, test_R²=-124.5692\n",
      "Epoch 770/1200\n",
      "Train loss: 6.3459, Train R²: -123.9114\n",
      "test_loss=5.3579, test_R²=-124.5667\n",
      "Epoch 771/1200\n",
      "Train loss: 6.3027, Train R²: -123.8929\n",
      "test_loss=5.3577, test_R²=-124.5637\n",
      "Epoch 772/1200\n",
      "Train loss: 6.3115, Train R²: -123.8561\n",
      "test_loss=5.3577, test_R²=-124.5618\n",
      "Epoch 773/1200\n",
      "Train loss: 6.3025, Train R²: -125.2415\n",
      "test_loss=5.3577, test_R²=-124.5615\n",
      "Epoch 774/1200\n",
      "Train loss: 6.3259, Train R²: -124.4038\n",
      "test_loss=5.3577, test_R²=-124.5620\n",
      "Epoch 775/1200\n",
      "Train loss: 6.2753, Train R²: -123.8582\n",
      "test_loss=5.3577, test_R²=-124.5611\n",
      "Epoch 776/1200\n",
      "Train loss: 6.3066, Train R²: -123.7963\n",
      "test_loss=5.3577, test_R²=-124.5603\n",
      "Epoch 777/1200\n",
      "Train loss: 6.3054, Train R²: -125.0904\n",
      "test_loss=5.3576, test_R²=-124.5597\n",
      "Epoch 778/1200\n",
      "Train loss: 6.3069, Train R²: -123.8119\n",
      "test_loss=5.3576, test_R²=-124.5587\n",
      "Epoch 779/1200\n",
      "Train loss: 6.3504, Train R²: -124.6444\n",
      "test_loss=5.3576, test_R²=-124.5577\n",
      "Epoch 780/1200\n",
      "Train loss: 6.3036, Train R²: -124.2219\n",
      "test_loss=5.3576, test_R²=-124.5572\n",
      "Epoch 781/1200\n",
      "Train loss: 6.3098, Train R²: -124.1372\n",
      "test_loss=5.3575, test_R²=-124.5553\n",
      "Epoch 782/1200\n",
      "Train loss: 6.3170, Train R²: -124.9019\n",
      "test_loss=5.3575, test_R²=-124.5551\n",
      "Epoch 783/1200\n",
      "Train loss: 6.3226, Train R²: -123.9789\n",
      "test_loss=5.3575, test_R²=-124.5542\n",
      "Epoch 784/1200\n",
      "Train loss: 6.3145, Train R²: -124.5019\n",
      "test_loss=5.3575, test_R²=-124.5547\n",
      "Epoch 785/1200\n",
      "Train loss: 6.3298, Train R²: -124.5189\n",
      "test_loss=5.3575, test_R²=-124.5532\n",
      "Epoch 786/1200\n",
      "Train loss: 6.2787, Train R²: -124.2728\n",
      "test_loss=5.3574, test_R²=-124.5512\n",
      "Epoch 787/1200\n",
      "Train loss: 6.2923, Train R²: -124.3370\n",
      "test_loss=5.3574, test_R²=-124.5499\n",
      "Epoch 788/1200\n",
      "Train loss: 6.3385, Train R²: -123.8952\n",
      "test_loss=5.3573, test_R²=-124.5481\n",
      "Epoch 789/1200\n",
      "Train loss: 6.3265, Train R²: -125.4364\n",
      "test_loss=5.3572, test_R²=-124.5461\n",
      "Epoch 790/1200\n",
      "Train loss: 6.2784, Train R²: -124.7784\n",
      "test_loss=5.3572, test_R²=-124.5447\n",
      "Epoch 791/1200\n",
      "Train loss: 6.2900, Train R²: -124.2024\n",
      "test_loss=5.3572, test_R²=-124.5439\n",
      "Epoch 792/1200\n",
      "Train loss: 6.2919, Train R²: -124.3905\n",
      "test_loss=5.3572, test_R²=-124.5441\n",
      "Epoch 793/1200\n",
      "Train loss: 6.3844, Train R²: -124.7454\n",
      "test_loss=5.3572, test_R²=-124.5441\n",
      "Epoch 794/1200\n",
      "Train loss: 6.3218, Train R²: -123.8809\n",
      "test_loss=5.3572, test_R²=-124.5435\n",
      "Epoch 795/1200\n",
      "Train loss: 6.2948, Train R²: -124.8666\n",
      "test_loss=5.3571, test_R²=-124.5426\n",
      "Epoch 796/1200\n",
      "Train loss: 6.3147, Train R²: -124.0359\n",
      "test_loss=5.3571, test_R²=-124.5422\n",
      "Epoch 797/1200\n",
      "Train loss: 6.3063, Train R²: -124.4535\n",
      "test_loss=5.3571, test_R²=-124.5420\n",
      "Epoch 798/1200\n",
      "Train loss: 6.3149, Train R²: -125.3229\n",
      "test_loss=5.3572, test_R²=-124.5428\n",
      "Epoch 799/1200\n",
      "Train loss: 6.3511, Train R²: -124.3978\n",
      "test_loss=5.3572, test_R²=-124.5427\n",
      "Epoch 800/1200\n",
      "Train loss: 6.3166, Train R²: -124.4236\n",
      "test_loss=5.3572, test_R²=-124.5437\n",
      "Epoch 801/1200\n",
      "Train loss: 6.3405, Train R²: -124.3348\n",
      "test_loss=5.3572, test_R²=-124.5438\n",
      "Epoch 802/1200\n",
      "Train loss: 6.3159, Train R²: -124.1343\n",
      "test_loss=5.3572, test_R²=-124.5433\n",
      "Epoch 803/1200\n",
      "Train loss: 6.3189, Train R²: -125.4872\n",
      "test_loss=5.3572, test_R²=-124.5426\n",
      "Epoch 804/1200\n",
      "Train loss: 6.3216, Train R²: -124.4043\n",
      "test_loss=5.3572, test_R²=-124.5429\n",
      "Epoch 805/1200\n",
      "Train loss: 6.2903, Train R²: -124.2142\n",
      "test_loss=5.3572, test_R²=-124.5420\n",
      "Epoch 806/1200\n",
      "Train loss: 6.3373, Train R²: -123.9788\n",
      "test_loss=5.3572, test_R²=-124.5411\n",
      "Epoch 807/1200\n",
      "Train loss: 6.3189, Train R²: -124.2371\n",
      "test_loss=5.3571, test_R²=-124.5401\n",
      "Epoch 808/1200\n",
      "Train loss: 6.3258, Train R²: -123.7177\n",
      "test_loss=5.3571, test_R²=-124.5384\n",
      "Epoch 809/1200\n",
      "Train loss: 6.3370, Train R²: -124.3079\n",
      "test_loss=5.3571, test_R²=-124.5379\n",
      "Epoch 810/1200\n",
      "Train loss: 6.2807, Train R²: -123.8664\n",
      "test_loss=5.3570, test_R²=-124.5361\n",
      "Epoch 811/1200\n",
      "Train loss: 6.3252, Train R²: -124.8912\n",
      "test_loss=5.3569, test_R²=-124.5343\n",
      "Epoch 812/1200\n",
      "Train loss: 6.3103, Train R²: -125.9950\n",
      "test_loss=5.3571, test_R²=-124.5372\n",
      "Epoch 813/1200\n",
      "Train loss: 6.2709, Train R²: -124.2676\n",
      "test_loss=5.3571, test_R²=-124.5390\n",
      "Epoch 814/1200\n",
      "Train loss: 6.2955, Train R²: -123.8513\n",
      "test_loss=5.3571, test_R²=-124.5380\n",
      "Epoch 815/1200\n",
      "Train loss: 6.3280, Train R²: -124.2115\n",
      "test_loss=5.3570, test_R²=-124.5367\n",
      "Epoch 816/1200\n",
      "Train loss: 6.3407, Train R²: -124.1394\n",
      "test_loss=5.3570, test_R²=-124.5357\n",
      "Epoch 817/1200\n",
      "Train loss: 6.3013, Train R²: -124.7683\n",
      "test_loss=5.3570, test_R²=-124.5352\n",
      "Epoch 818/1200\n",
      "Train loss: 6.2956, Train R²: -124.1794\n",
      "test_loss=5.3570, test_R²=-124.5342\n",
      "Epoch 819/1200\n",
      "Train loss: 6.2773, Train R²: -124.2616\n",
      "test_loss=5.3570, test_R²=-124.5347\n",
      "Epoch 820/1200\n",
      "Train loss: 6.3309, Train R²: -124.5618\n",
      "test_loss=5.3571, test_R²=-124.5379\n",
      "Epoch 821/1200\n",
      "Train loss: 6.3218, Train R²: -124.2887\n",
      "test_loss=5.3572, test_R²=-124.5410\n",
      "Epoch 822/1200\n",
      "Train loss: 6.3311, Train R²: -123.9358\n",
      "test_loss=5.3573, test_R²=-124.5415\n",
      "Epoch 823/1200\n",
      "Train loss: 6.3443, Train R²: -124.4902\n",
      "test_loss=5.3572, test_R²=-124.5400\n",
      "Epoch 824/1200\n",
      "Train loss: 6.3157, Train R²: -125.5016\n",
      "test_loss=5.3571, test_R²=-124.5371\n",
      "Epoch 825/1200\n",
      "Train loss: 6.3044, Train R²: -124.3051\n",
      "test_loss=5.3570, test_R²=-124.5353\n",
      "Epoch 826/1200\n",
      "Train loss: 6.3324, Train R²: -125.3502\n",
      "test_loss=5.3570, test_R²=-124.5335\n",
      "Epoch 827/1200\n",
      "Train loss: 6.2880, Train R²: -125.3414\n",
      "test_loss=5.3570, test_R²=-124.5344\n",
      "Epoch 828/1200\n",
      "Train loss: 6.3508, Train R²: -124.6497\n",
      "test_loss=5.3570, test_R²=-124.5340\n",
      "Epoch 829/1200\n",
      "Train loss: 6.2734, Train R²: -123.8989\n",
      "test_loss=5.3569, test_R²=-124.5323\n",
      "Epoch 830/1200\n",
      "Train loss: 6.3384, Train R²: -123.7447\n",
      "test_loss=5.3569, test_R²=-124.5316\n",
      "Epoch 831/1200\n",
      "Train loss: 6.2723, Train R²: -124.3460\n",
      "test_loss=5.3569, test_R²=-124.5328\n",
      "Epoch 832/1200\n",
      "Train loss: 6.3397, Train R²: -124.6623\n",
      "test_loss=5.3569, test_R²=-124.5326\n",
      "Epoch 833/1200\n",
      "Train loss: 6.3443, Train R²: -124.3788\n",
      "test_loss=5.3569, test_R²=-124.5328\n",
      "Epoch 834/1200\n",
      "Train loss: 6.3237, Train R²: -124.0352\n",
      "test_loss=5.3570, test_R²=-124.5339\n",
      "Epoch 835/1200\n",
      "Train loss: 6.3352, Train R²: -124.2104\n",
      "test_loss=5.3570, test_R²=-124.5355\n",
      "Epoch 836/1200\n",
      "Train loss: 6.2851, Train R²: -123.8683\n",
      "test_loss=5.3570, test_R²=-124.5351\n",
      "Epoch 837/1200\n",
      "Train loss: 6.2974, Train R²: -124.1344\n",
      "test_loss=5.3569, test_R²=-124.5330\n",
      "Epoch 838/1200\n",
      "Train loss: 6.3672, Train R²: -124.0853\n",
      "test_loss=5.3569, test_R²=-124.5312\n",
      "Epoch 839/1200\n",
      "Train loss: 6.3203, Train R²: -123.9971\n",
      "test_loss=5.3568, test_R²=-124.5297\n",
      "Epoch 840/1200\n",
      "Train loss: 6.2893, Train R²: -124.1611\n",
      "test_loss=5.3568, test_R²=-124.5300\n",
      "Epoch 841/1200\n",
      "Train loss: 6.3139, Train R²: -124.7249\n",
      "test_loss=5.3569, test_R²=-124.5315\n",
      "Epoch 842/1200\n",
      "Train loss: 6.3412, Train R²: -124.2820\n",
      "test_loss=5.3569, test_R²=-124.5328\n",
      "Epoch 843/1200\n",
      "Train loss: 6.3002, Train R²: -124.8840\n",
      "test_loss=5.3569, test_R²=-124.5326\n",
      "Epoch 844/1200\n",
      "Train loss: 6.3327, Train R²: -125.0172\n",
      "test_loss=5.3570, test_R²=-124.5333\n",
      "Epoch 845/1200\n",
      "Train loss: 6.2955, Train R²: -124.4902\n",
      "test_loss=5.3570, test_R²=-124.5336\n",
      "Epoch 846/1200\n",
      "Train loss: 6.2862, Train R²: -124.4040\n",
      "test_loss=5.3570, test_R²=-124.5344\n",
      "Epoch 847/1200\n",
      "Train loss: 6.3142, Train R²: -124.2574\n",
      "test_loss=5.3571, test_R²=-124.5371\n",
      "Epoch 848/1200\n",
      "Train loss: 6.3260, Train R²: -123.6184\n",
      "test_loss=5.3571, test_R²=-124.5365\n",
      "Epoch 849/1200\n",
      "Train loss: 6.3144, Train R²: -124.2298\n",
      "test_loss=5.3571, test_R²=-124.5348\n",
      "Epoch 850/1200\n",
      "Train loss: 6.3127, Train R²: -125.7657\n",
      "test_loss=5.3570, test_R²=-124.5342\n",
      "Epoch 851/1200\n",
      "Train loss: 6.3241, Train R²: -124.7984\n",
      "test_loss=5.3570, test_R²=-124.5327\n",
      "Epoch 852/1200\n",
      "Train loss: 6.3088, Train R²: -124.7869\n",
      "test_loss=5.3569, test_R²=-124.5300\n",
      "Epoch 853/1200\n",
      "Train loss: 6.3764, Train R²: -124.3367\n",
      "test_loss=5.3569, test_R²=-124.5298\n",
      "Epoch 854/1200\n",
      "Train loss: 6.3120, Train R²: -123.7897\n",
      "test_loss=5.3569, test_R²=-124.5296\n",
      "Epoch 855/1200\n",
      "Train loss: 6.3147, Train R²: -124.0860\n",
      "test_loss=5.3568, test_R²=-124.5286\n",
      "Epoch 856/1200\n",
      "Train loss: 6.3330, Train R²: -124.2856\n",
      "test_loss=5.3568, test_R²=-124.5278\n",
      "Epoch 857/1200\n",
      "Train loss: 6.2814, Train R²: -124.6827\n",
      "test_loss=5.3568, test_R²=-124.5280\n",
      "Epoch 858/1200\n",
      "Train loss: 6.3294, Train R²: -125.5343\n",
      "test_loss=5.3568, test_R²=-124.5271\n",
      "Epoch 859/1200\n",
      "Train loss: 6.2879, Train R²: -125.3270\n",
      "test_loss=5.3567, test_R²=-124.5256\n",
      "Epoch 860/1200\n",
      "Train loss: 6.3181, Train R²: -123.9967\n",
      "test_loss=5.3567, test_R²=-124.5252\n",
      "Epoch 861/1200\n",
      "Train loss: 6.3207, Train R²: -125.3558\n",
      "test_loss=5.3566, test_R²=-124.5226\n",
      "Epoch 862/1200\n",
      "Train loss: 6.3657, Train R²: -124.5398\n",
      "test_loss=5.3565, test_R²=-124.5203\n",
      "Epoch 863/1200\n",
      "Train loss: 6.3159, Train R²: -124.6415\n",
      "test_loss=5.3565, test_R²=-124.5192\n",
      "Epoch 864/1200\n",
      "Train loss: 6.3259, Train R²: -124.8217\n",
      "test_loss=5.3565, test_R²=-124.5191\n",
      "Epoch 865/1200\n",
      "Train loss: 6.2983, Train R²: -124.5328\n",
      "test_loss=5.3565, test_R²=-124.5203\n",
      "Epoch 866/1200\n",
      "Train loss: 6.2977, Train R²: -123.8383\n",
      "test_loss=5.3566, test_R²=-124.5219\n",
      "Epoch 867/1200\n",
      "Train loss: 6.2817, Train R²: -124.1913\n",
      "test_loss=5.3567, test_R²=-124.5230\n",
      "Epoch 868/1200\n",
      "Train loss: 6.2676, Train R²: -124.5800\n",
      "test_loss=5.3567, test_R²=-124.5244\n",
      "Epoch 869/1200\n",
      "Train loss: 6.3085, Train R²: -124.4591\n",
      "test_loss=5.3568, test_R²=-124.5254\n",
      "Epoch 870/1200\n",
      "Train loss: 6.3362, Train R²: -123.7010\n",
      "test_loss=5.3568, test_R²=-124.5260\n",
      "Epoch 871/1200\n",
      "Train loss: 6.3150, Train R²: -123.9517\n",
      "test_loss=5.3568, test_R²=-124.5260\n",
      "Epoch 872/1200\n",
      "Train loss: 6.3504, Train R²: -124.3675\n",
      "test_loss=5.3567, test_R²=-124.5248\n",
      "Epoch 873/1200\n",
      "Train loss: 6.2790, Train R²: -123.9005\n",
      "test_loss=5.3567, test_R²=-124.5232\n",
      "Epoch 874/1200\n",
      "Train loss: 6.3315, Train R²: -124.0971\n",
      "test_loss=5.3566, test_R²=-124.5220\n",
      "Epoch 875/1200\n",
      "Train loss: 6.3008, Train R²: -124.2780\n",
      "test_loss=5.3565, test_R²=-124.5200\n",
      "Epoch 876/1200\n",
      "Train loss: 6.3003, Train R²: -124.0199\n",
      "test_loss=5.3565, test_R²=-124.5194\n",
      "Epoch 877/1200\n",
      "Train loss: 6.3214, Train R²: -125.9742\n",
      "test_loss=5.3564, test_R²=-124.5163\n",
      "Epoch 878/1200\n",
      "Train loss: 6.3530, Train R²: -124.9389\n",
      "test_loss=5.3563, test_R²=-124.5137\n",
      "Epoch 879/1200\n",
      "Train loss: 6.3100, Train R²: -124.5310\n",
      "test_loss=5.3563, test_R²=-124.5139\n",
      "Epoch 880/1200\n",
      "Train loss: 6.3071, Train R²: -124.2482\n",
      "test_loss=5.3563, test_R²=-124.5150\n",
      "Epoch 881/1200\n",
      "Train loss: 6.3278, Train R²: -125.1404\n",
      "test_loss=5.3564, test_R²=-124.5160\n",
      "Epoch 882/1200\n",
      "Train loss: 6.3289, Train R²: -124.5526\n",
      "test_loss=5.3563, test_R²=-124.5147\n",
      "Epoch 883/1200\n",
      "Train loss: 6.3370, Train R²: -124.4246\n",
      "test_loss=5.3562, test_R²=-124.5121\n",
      "Epoch 884/1200\n",
      "Train loss: 6.3091, Train R²: -125.0338\n",
      "test_loss=5.3561, test_R²=-124.5092\n",
      "Epoch 885/1200\n",
      "Train loss: 6.2951, Train R²: -124.0844\n",
      "test_loss=5.3560, test_R²=-124.5050\n",
      "Epoch 886/1200\n",
      "Train loss: 6.3156, Train R²: -123.9105\n",
      "test_loss=5.3559, test_R²=-124.5036\n",
      "Epoch 887/1200\n",
      "Train loss: 6.3288, Train R²: -124.0722\n",
      "test_loss=5.3559, test_R²=-124.5026\n",
      "Epoch 888/1200\n",
      "Train loss: 6.2923, Train R²: -124.5680\n",
      "test_loss=5.3560, test_R²=-124.5053\n",
      "Epoch 889/1200\n",
      "Train loss: 6.3260, Train R²: -125.0986\n",
      "test_loss=5.3560, test_R²=-124.5062\n",
      "Epoch 890/1200\n",
      "Train loss: 6.3203, Train R²: -124.1386\n",
      "test_loss=5.3560, test_R²=-124.5056\n",
      "Epoch 891/1200\n",
      "Train loss: 6.3104, Train R²: -123.7974\n",
      "test_loss=5.3560, test_R²=-124.5055\n",
      "Epoch 892/1200\n",
      "Train loss: 6.2893, Train R²: -125.0350\n",
      "test_loss=5.3561, test_R²=-124.5064\n",
      "Epoch 893/1200\n",
      "Train loss: 6.2694, Train R²: -124.4451\n",
      "test_loss=5.3561, test_R²=-124.5069\n",
      "Epoch 894/1200\n",
      "Train loss: 6.2689, Train R²: -124.5370\n",
      "test_loss=5.3561, test_R²=-124.5071\n",
      "Epoch 895/1200\n",
      "Train loss: 6.2830, Train R²: -124.2462\n",
      "test_loss=5.3561, test_R²=-124.5070\n",
      "Epoch 896/1200\n",
      "Train loss: 6.2989, Train R²: -123.6585\n",
      "test_loss=5.3561, test_R²=-124.5073\n",
      "Epoch 897/1200\n",
      "Train loss: 6.3164, Train R²: -124.3971\n",
      "test_loss=5.3562, test_R²=-124.5095\n",
      "Epoch 898/1200\n",
      "Train loss: 6.3050, Train R²: -124.3118\n",
      "test_loss=5.3562, test_R²=-124.5096\n",
      "Epoch 899/1200\n",
      "Train loss: 6.3148, Train R²: -124.1004\n",
      "test_loss=5.3562, test_R²=-124.5097\n",
      "Epoch 900/1200\n",
      "Train loss: 6.2890, Train R²: -124.5008\n",
      "test_loss=5.3563, test_R²=-124.5110\n",
      "Epoch 901/1200\n",
      "Train loss: 6.3362, Train R²: -124.8861\n",
      "test_loss=5.3562, test_R²=-124.5104\n",
      "Epoch 902/1200\n",
      "Train loss: 6.2731, Train R²: -124.1322\n",
      "test_loss=5.3562, test_R²=-124.5094\n",
      "Epoch 903/1200\n",
      "Train loss: 6.3067, Train R²: -125.6522\n",
      "test_loss=5.3562, test_R²=-124.5092\n",
      "Epoch 904/1200\n",
      "Train loss: 6.3259, Train R²: -124.5795\n",
      "test_loss=5.3562, test_R²=-124.5089\n",
      "Epoch 905/1200\n",
      "Train loss: 6.3555, Train R²: -124.4469\n",
      "test_loss=5.3561, test_R²=-124.5070\n",
      "Epoch 906/1200\n",
      "Train loss: 6.3248, Train R²: -124.0513\n",
      "test_loss=5.3560, test_R²=-124.5047\n",
      "Epoch 907/1200\n",
      "Train loss: 6.3193, Train R²: -124.1854\n",
      "test_loss=5.3559, test_R²=-124.5031\n",
      "Epoch 908/1200\n",
      "Train loss: 6.3070, Train R²: -123.6668\n",
      "test_loss=5.3559, test_R²=-124.5025\n",
      "Epoch 909/1200\n",
      "Train loss: 6.3417, Train R²: -124.7573\n",
      "test_loss=5.3559, test_R²=-124.5021\n",
      "Epoch 910/1200\n",
      "Train loss: 6.3388, Train R²: -124.2710\n",
      "test_loss=5.3559, test_R²=-124.5035\n",
      "Epoch 911/1200\n",
      "Train loss: 6.3299, Train R²: -124.0382\n",
      "test_loss=5.3560, test_R²=-124.5047\n",
      "Epoch 912/1200\n",
      "Train loss: 6.3116, Train R²: -124.0855\n",
      "test_loss=5.3560, test_R²=-124.5059\n",
      "Epoch 913/1200\n",
      "Train loss: 6.3369, Train R²: -123.8948\n",
      "test_loss=5.3561, test_R²=-124.5067\n",
      "Epoch 914/1200\n",
      "Train loss: 6.3266, Train R²: -123.9503\n",
      "test_loss=5.3561, test_R²=-124.5070\n",
      "Epoch 915/1200\n",
      "Train loss: 6.3135, Train R²: -124.3128\n",
      "test_loss=5.3560, test_R²=-124.5055\n",
      "Epoch 916/1200\n",
      "Train loss: 6.3221, Train R²: -124.0474\n",
      "test_loss=5.3560, test_R²=-124.5050\n",
      "Epoch 917/1200\n",
      "Train loss: 6.3128, Train R²: -124.7131\n",
      "test_loss=5.3560, test_R²=-124.5052\n",
      "Epoch 918/1200\n",
      "Train loss: 6.2868, Train R²: -123.7187\n",
      "test_loss=5.3560, test_R²=-124.5054\n",
      "Epoch 919/1200\n",
      "Train loss: 6.2955, Train R²: -124.2293\n",
      "test_loss=5.3560, test_R²=-124.5055\n",
      "Epoch 920/1200\n",
      "Train loss: 6.2627, Train R²: -124.1030\n",
      "test_loss=5.3560, test_R²=-124.5063\n",
      "Epoch 921/1200\n",
      "Train loss: 6.2979, Train R²: -124.5669\n",
      "test_loss=5.3561, test_R²=-124.5074\n",
      "Epoch 922/1200\n",
      "Train loss: 6.3088, Train R²: -125.2716\n",
      "test_loss=5.3561, test_R²=-124.5081\n",
      "Epoch 923/1200\n",
      "Train loss: 6.2869, Train R²: -123.8240\n",
      "test_loss=5.3562, test_R²=-124.5099\n",
      "Epoch 924/1200\n",
      "Train loss: 6.3239, Train R²: -124.2938\n",
      "test_loss=5.3562, test_R²=-124.5116\n",
      "Epoch 925/1200\n",
      "Train loss: 6.2921, Train R²: -124.3645\n",
      "test_loss=5.3563, test_R²=-124.5123\n",
      "Epoch 926/1200\n",
      "Train loss: 6.3099, Train R²: -125.2737\n",
      "test_loss=5.3563, test_R²=-124.5146\n",
      "Epoch 927/1200\n",
      "Train loss: 6.3060, Train R²: -124.3920\n",
      "test_loss=5.3566, test_R²=-124.5197\n",
      "Epoch 928/1200\n",
      "Train loss: 6.2966, Train R²: -124.2203\n",
      "test_loss=5.3567, test_R²=-124.5231\n",
      "Epoch 929/1200\n",
      "Train loss: 6.3270, Train R²: -123.7350\n",
      "test_loss=5.3567, test_R²=-124.5232\n",
      "Epoch 930/1200\n",
      "Train loss: 6.3283, Train R²: -124.5439\n",
      "test_loss=5.3567, test_R²=-124.5231\n",
      "Epoch 931/1200\n",
      "Train loss: 6.3065, Train R²: -126.3410\n",
      "test_loss=5.3568, test_R²=-124.5245\n",
      "Epoch 932/1200\n",
      "Train loss: 6.3012, Train R²: -123.9303\n",
      "test_loss=5.3569, test_R²=-124.5282\n",
      "Epoch 933/1200\n",
      "Train loss: 6.2773, Train R²: -124.2957\n",
      "test_loss=5.3569, test_R²=-124.5282\n",
      "Epoch 934/1200\n",
      "Train loss: 6.3316, Train R²: -123.9819\n",
      "test_loss=5.3569, test_R²=-124.5264\n",
      "Epoch 935/1200\n",
      "Train loss: 6.3263, Train R²: -124.2737\n",
      "test_loss=5.3568, test_R²=-124.5246\n",
      "Epoch 936/1200\n",
      "Train loss: 6.3128, Train R²: -123.8560\n",
      "test_loss=5.3567, test_R²=-124.5223\n",
      "Epoch 937/1200\n",
      "Train loss: 6.3166, Train R²: -124.9129\n",
      "test_loss=5.3566, test_R²=-124.5192\n",
      "Epoch 938/1200\n",
      "Train loss: 6.3578, Train R²: -124.4483\n",
      "test_loss=5.3564, test_R²=-124.5162\n",
      "Epoch 939/1200\n",
      "Train loss: 6.3459, Train R²: -124.8241\n",
      "test_loss=5.3564, test_R²=-124.5147\n",
      "Epoch 940/1200\n",
      "Train loss: 6.3389, Train R²: -124.8769\n",
      "test_loss=5.3564, test_R²=-124.5145\n",
      "Epoch 941/1200\n",
      "Train loss: 6.2893, Train R²: -124.8155\n",
      "test_loss=5.3564, test_R²=-124.5147\n",
      "Epoch 942/1200\n",
      "Train loss: 6.3053, Train R²: -125.8559\n",
      "test_loss=5.3563, test_R²=-124.5112\n",
      "Epoch 943/1200\n",
      "Train loss: 6.3290, Train R²: -124.0820\n",
      "test_loss=5.3562, test_R²=-124.5081\n",
      "Epoch 944/1200\n",
      "Train loss: 6.3205, Train R²: -124.6581\n",
      "test_loss=5.3561, test_R²=-124.5069\n",
      "Epoch 945/1200\n",
      "Train loss: 6.3411, Train R²: -124.8161\n",
      "test_loss=5.3562, test_R²=-124.5087\n",
      "Epoch 946/1200\n",
      "Train loss: 6.3002, Train R²: -124.6861\n",
      "test_loss=5.3563, test_R²=-124.5103\n",
      "Epoch 947/1200\n",
      "Train loss: 6.2861, Train R²: -124.7436\n",
      "test_loss=5.3563, test_R²=-124.5125\n",
      "Epoch 948/1200\n",
      "Train loss: 6.3217, Train R²: -123.9215\n",
      "test_loss=5.3566, test_R²=-124.5178\n",
      "Epoch 949/1200\n",
      "Train loss: 6.2877, Train R²: -124.8933\n",
      "test_loss=5.3566, test_R²=-124.5201\n",
      "Epoch 950/1200\n",
      "Train loss: 6.3199, Train R²: -124.7715\n",
      "test_loss=5.3566, test_R²=-124.5199\n",
      "Epoch 951/1200\n",
      "Train loss: 6.2675, Train R²: -124.3930\n",
      "test_loss=5.3566, test_R²=-124.5190\n",
      "Epoch 952/1200\n",
      "Train loss: 6.2945, Train R²: -124.3042\n",
      "test_loss=5.3565, test_R²=-124.5179\n",
      "Epoch 953/1200\n",
      "Train loss: 6.3462, Train R²: -125.1745\n",
      "test_loss=5.3564, test_R²=-124.5162\n",
      "Epoch 954/1200\n",
      "Train loss: 6.3142, Train R²: -124.9057\n",
      "test_loss=5.3565, test_R²=-124.5169\n",
      "Epoch 955/1200\n",
      "Train loss: 6.2922, Train R²: -124.6611\n",
      "test_loss=5.3565, test_R²=-124.5182\n",
      "Epoch 956/1200\n",
      "Train loss: 6.3062, Train R²: -123.6577\n",
      "test_loss=5.3565, test_R²=-124.5175\n",
      "Epoch 957/1200\n",
      "Train loss: 6.2997, Train R²: -124.2213\n",
      "test_loss=5.3564, test_R²=-124.5158\n",
      "Epoch 958/1200\n",
      "Train loss: 6.3209, Train R²: -123.8357\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 959/1200\n",
      "Train loss: 6.2957, Train R²: -124.6142\n",
      "test_loss=5.3564, test_R²=-124.5153\n",
      "Epoch 960/1200\n",
      "Train loss: 6.3202, Train R²: -124.0300\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 961/1200\n",
      "Train loss: 6.3084, Train R²: -124.4000\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 962/1200\n",
      "Train loss: 6.3053, Train R²: -124.1680\n",
      "test_loss=5.3565, test_R²=-124.5186\n",
      "Epoch 963/1200\n",
      "Train loss: 6.3351, Train R²: -124.0471\n",
      "test_loss=5.3565, test_R²=-124.5172\n",
      "Epoch 964/1200\n",
      "Train loss: 6.3028, Train R²: -124.9683\n",
      "test_loss=5.3566, test_R²=-124.5189\n",
      "Epoch 965/1200\n",
      "Train loss: 6.3473, Train R²: -124.6132\n",
      "test_loss=5.3566, test_R²=-124.5206\n",
      "Epoch 966/1200\n",
      "Train loss: 6.3399, Train R²: -124.1476\n",
      "test_loss=5.3566, test_R²=-124.5205\n",
      "Epoch 967/1200\n",
      "Train loss: 6.3251, Train R²: -124.5624\n",
      "test_loss=5.3566, test_R²=-124.5206\n",
      "Epoch 968/1200\n",
      "Train loss: 6.3101, Train R²: -124.0145\n",
      "test_loss=5.3566, test_R²=-124.5206\n",
      "Epoch 969/1200\n",
      "Train loss: 6.2840, Train R²: -124.1731\n",
      "test_loss=5.3565, test_R²=-124.5177\n",
      "Epoch 970/1200\n",
      "Train loss: 6.2844, Train R²: -123.9750\n",
      "test_loss=5.3564, test_R²=-124.5160\n",
      "Epoch 971/1200\n",
      "Train loss: 6.3141, Train R²: -124.6809\n",
      "test_loss=5.3564, test_R²=-124.5150\n",
      "Epoch 972/1200\n",
      "Train loss: 6.3061, Train R²: -123.7893\n",
      "test_loss=5.3563, test_R²=-124.5144\n",
      "Epoch 973/1200\n",
      "Train loss: 6.3367, Train R²: -125.2491\n",
      "test_loss=5.3564, test_R²=-124.5165\n",
      "Epoch 974/1200\n",
      "Train loss: 6.2939, Train R²: -125.3231\n",
      "test_loss=5.3565, test_R²=-124.5190\n",
      "Epoch 975/1200\n",
      "Train loss: 6.3136, Train R²: -124.6668\n",
      "test_loss=5.3567, test_R²=-124.5238\n",
      "Epoch 976/1200\n",
      "Train loss: 6.3192, Train R²: -124.1697\n",
      "test_loss=5.3568, test_R²=-124.5252\n",
      "Epoch 977/1200\n",
      "Train loss: 6.3391, Train R²: -124.0742\n",
      "test_loss=5.3567, test_R²=-124.5230\n",
      "Epoch 978/1200\n",
      "Train loss: 6.3080, Train R²: -124.2934\n",
      "test_loss=5.3567, test_R²=-124.5220\n",
      "Epoch 979/1200\n",
      "Train loss: 6.2900, Train R²: -124.1665\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 980/1200\n",
      "Train loss: 6.2972, Train R²: -123.7629\n",
      "test_loss=5.3565, test_R²=-124.5161\n",
      "Epoch 981/1200\n",
      "Train loss: 6.3109, Train R²: -124.4419\n",
      "test_loss=5.3564, test_R²=-124.5147\n",
      "Epoch 982/1200\n",
      "Train loss: 6.3418, Train R²: -125.4262\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 983/1200\n",
      "Train loss: 6.3159, Train R²: -124.5144\n",
      "test_loss=5.3567, test_R²=-124.5211\n",
      "Epoch 984/1200\n",
      "Train loss: 6.3329, Train R²: -124.0172\n",
      "test_loss=5.3567, test_R²=-124.5233\n",
      "Epoch 985/1200\n",
      "Train loss: 6.2937, Train R²: -124.9274\n",
      "test_loss=5.3567, test_R²=-124.5224\n",
      "Epoch 986/1200\n",
      "Train loss: 6.3070, Train R²: -124.2346\n",
      "test_loss=5.3567, test_R²=-124.5224\n",
      "Epoch 987/1200\n",
      "Train loss: 6.2733, Train R²: -124.1592\n",
      "test_loss=5.3567, test_R²=-124.5223\n",
      "Epoch 988/1200\n",
      "Train loss: 6.3211, Train R²: -125.2092\n",
      "test_loss=5.3567, test_R²=-124.5219\n",
      "Epoch 989/1200\n",
      "Train loss: 6.3361, Train R²: -123.8333\n",
      "test_loss=5.3567, test_R²=-124.5229\n",
      "Epoch 990/1200\n",
      "Train loss: 6.2730, Train R²: -124.3704\n",
      "test_loss=5.3568, test_R²=-124.5240\n",
      "Epoch 991/1200\n",
      "Train loss: 6.3439, Train R²: -124.1464\n",
      "test_loss=5.3568, test_R²=-124.5243\n",
      "Epoch 992/1200\n",
      "Train loss: 6.3243, Train R²: -124.3819\n",
      "test_loss=5.3567, test_R²=-124.5227\n",
      "Epoch 993/1200\n",
      "Train loss: 6.3079, Train R²: -124.3862\n",
      "test_loss=5.3566, test_R²=-124.5204\n",
      "Epoch 994/1200\n",
      "Train loss: 6.3196, Train R²: -125.6579\n",
      "test_loss=5.3566, test_R²=-124.5191\n",
      "Epoch 995/1200\n",
      "Train loss: 6.3597, Train R²: -124.2268\n",
      "test_loss=5.3566, test_R²=-124.5196\n",
      "Epoch 996/1200\n",
      "Train loss: 6.2977, Train R²: -124.5596\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 997/1200\n",
      "Train loss: 6.2798, Train R²: -123.7935\n",
      "test_loss=5.3565, test_R²=-124.5174\n",
      "Epoch 998/1200\n",
      "Train loss: 6.2943, Train R²: -125.8479\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 999/1200\n",
      "Train loss: 6.3389, Train R²: -124.0495\n",
      "test_loss=5.3565, test_R²=-124.5179\n",
      "Epoch 1000/1200\n",
      "Train loss: 6.3227, Train R²: -125.7765\n",
      "test_loss=5.3565, test_R²=-124.5176\n",
      "Epoch 1001/1200\n",
      "Train loss: 6.3316, Train R²: -124.3894\n",
      "test_loss=5.3565, test_R²=-124.5160\n",
      "Epoch 1002/1200\n",
      "Train loss: 6.2933, Train R²: -124.8840\n",
      "test_loss=5.3565, test_R²=-124.5165\n",
      "Epoch 1003/1200\n",
      "Train loss: 6.3275, Train R²: -124.4027\n",
      "test_loss=5.3565, test_R²=-124.5181\n",
      "Epoch 1004/1200\n",
      "Train loss: 6.2816, Train R²: -124.4351\n",
      "test_loss=5.3565, test_R²=-124.5186\n",
      "Epoch 1005/1200\n",
      "Train loss: 6.3287, Train R²: -124.8465\n",
      "test_loss=5.3567, test_R²=-124.5223\n",
      "Epoch 1006/1200\n",
      "Train loss: 6.3083, Train R²: -123.7232\n",
      "test_loss=5.3568, test_R²=-124.5237\n",
      "Epoch 1007/1200\n",
      "Train loss: 6.3002, Train R²: -123.7664\n",
      "test_loss=5.3567, test_R²=-124.5232\n",
      "Epoch 1008/1200\n",
      "Train loss: 6.3182, Train R²: -123.7310\n",
      "test_loss=5.3567, test_R²=-124.5212\n",
      "Epoch 1009/1200\n",
      "Train loss: 6.3269, Train R²: -124.7622\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 1010/1200\n",
      "Train loss: 6.3162, Train R²: -125.3756\n",
      "test_loss=5.3564, test_R²=-124.5141\n",
      "Epoch 1011/1200\n",
      "Train loss: 6.2891, Train R²: -124.0485\n",
      "test_loss=5.3563, test_R²=-124.5105\n",
      "Epoch 1012/1200\n",
      "Train loss: 6.3296, Train R²: -124.4705\n",
      "test_loss=5.3561, test_R²=-124.5079\n",
      "Epoch 1013/1200\n",
      "Train loss: 6.3005, Train R²: -124.3594\n",
      "test_loss=5.3560, test_R²=-124.5039\n",
      "Epoch 1014/1200\n",
      "Train loss: 6.3134, Train R²: -124.1014\n",
      "test_loss=5.3560, test_R²=-124.5036\n",
      "Epoch 1015/1200\n",
      "Train loss: 6.3584, Train R²: -124.5089\n",
      "test_loss=5.3560, test_R²=-124.5049\n",
      "Epoch 1016/1200\n",
      "Train loss: 6.3494, Train R²: -124.1143\n",
      "test_loss=5.3560, test_R²=-124.5051\n",
      "Epoch 1017/1200\n",
      "Train loss: 6.3098, Train R²: -124.7286\n",
      "test_loss=5.3561, test_R²=-124.5061\n",
      "Epoch 1018/1200\n",
      "Train loss: 6.3005, Train R²: -125.3231\n",
      "test_loss=5.3562, test_R²=-124.5085\n",
      "Epoch 1019/1200\n",
      "Train loss: 6.3031, Train R²: -124.7925\n",
      "test_loss=5.3563, test_R²=-124.5117\n",
      "Epoch 1020/1200\n",
      "Train loss: 6.3372, Train R²: -124.0869\n",
      "test_loss=5.3565, test_R²=-124.5178\n",
      "Epoch 1021/1200\n",
      "Train loss: 6.3396, Train R²: -124.1045\n",
      "test_loss=5.3567, test_R²=-124.5213\n",
      "Epoch 1022/1200\n",
      "Train loss: 6.3365, Train R²: -123.8294\n",
      "test_loss=5.3567, test_R²=-124.5222\n",
      "Epoch 1023/1200\n",
      "Train loss: 6.3250, Train R²: -124.7248\n",
      "test_loss=5.3567, test_R²=-124.5211\n",
      "Epoch 1024/1200\n",
      "Train loss: 6.3269, Train R²: -123.7269\n",
      "test_loss=5.3566, test_R²=-124.5196\n",
      "Epoch 1025/1200\n",
      "Train loss: 6.2822, Train R²: -123.8960\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 1026/1200\n",
      "Train loss: 6.2991, Train R²: -124.3923\n",
      "test_loss=5.3565, test_R²=-124.5175\n",
      "Epoch 1027/1200\n",
      "Train loss: 6.3462, Train R²: -124.4010\n",
      "test_loss=5.3564, test_R²=-124.5159\n",
      "Epoch 1028/1200\n",
      "Train loss: 6.3097, Train R²: -124.3040\n",
      "test_loss=5.3565, test_R²=-124.5175\n",
      "Epoch 1029/1200\n",
      "Train loss: 6.3231, Train R²: -124.4351\n",
      "test_loss=5.3565, test_R²=-124.5187\n",
      "Epoch 1030/1200\n",
      "Train loss: 6.3246, Train R²: -123.8826\n",
      "test_loss=5.3566, test_R²=-124.5196\n",
      "Epoch 1031/1200\n",
      "Train loss: 6.2960, Train R²: -126.3676\n",
      "test_loss=5.3566, test_R²=-124.5215\n",
      "Epoch 1032/1200\n",
      "Train loss: 6.2765, Train R²: -124.0931\n",
      "test_loss=5.3567, test_R²=-124.5231\n",
      "Epoch 1033/1200\n",
      "Train loss: 6.2759, Train R²: -124.3769\n",
      "test_loss=5.3568, test_R²=-124.5244\n",
      "Epoch 1034/1200\n",
      "Train loss: 6.3413, Train R²: -124.2775\n",
      "test_loss=5.3568, test_R²=-124.5241\n",
      "Epoch 1035/1200\n",
      "Train loss: 6.3148, Train R²: -126.3044\n",
      "test_loss=5.3569, test_R²=-124.5268\n",
      "Epoch 1036/1200\n",
      "Train loss: 6.2732, Train R²: -123.7752\n",
      "test_loss=5.3570, test_R²=-124.5297\n",
      "Epoch 1037/1200\n",
      "Train loss: 6.2631, Train R²: -123.9303\n",
      "test_loss=5.3570, test_R²=-124.5300\n",
      "Epoch 1038/1200\n",
      "Train loss: 6.3242, Train R²: -125.0098\n",
      "test_loss=5.3570, test_R²=-124.5295\n",
      "Epoch 1039/1200\n",
      "Train loss: 6.3062, Train R²: -125.4008\n",
      "test_loss=5.3569, test_R²=-124.5271\n",
      "Epoch 1040/1200\n",
      "Train loss: 6.3466, Train R²: -124.0594\n",
      "test_loss=5.3569, test_R²=-124.5261\n",
      "Epoch 1041/1200\n",
      "Train loss: 6.2959, Train R²: -124.5782\n",
      "test_loss=5.3566, test_R²=-124.5204\n",
      "Epoch 1042/1200\n",
      "Train loss: 6.2967, Train R²: -124.3709\n",
      "test_loss=5.3567, test_R²=-124.5204\n",
      "Epoch 1043/1200\n",
      "Train loss: 6.2819, Train R²: -125.7645\n",
      "test_loss=5.3568, test_R²=-124.5247\n",
      "Epoch 1044/1200\n",
      "Train loss: 6.3227, Train R²: -124.2119\n",
      "test_loss=5.3569, test_R²=-124.5277\n",
      "Epoch 1045/1200\n",
      "Train loss: 6.3093, Train R²: -123.9194\n",
      "test_loss=5.3569, test_R²=-124.5269\n",
      "Epoch 1046/1200\n",
      "Train loss: 6.3010, Train R²: -124.6795\n",
      "test_loss=5.3569, test_R²=-124.5263\n",
      "Epoch 1047/1200\n",
      "Train loss: 6.3225, Train R²: -124.6506\n",
      "test_loss=5.3569, test_R²=-124.5273\n",
      "Epoch 1048/1200\n",
      "Train loss: 6.3301, Train R²: -124.3334\n",
      "test_loss=5.3569, test_R²=-124.5276\n",
      "Epoch 1049/1200\n",
      "Train loss: 6.3303, Train R²: -124.8515\n",
      "test_loss=5.3569, test_R²=-124.5276\n",
      "Epoch 1050/1200\n",
      "Train loss: 6.3020, Train R²: -124.0874\n",
      "test_loss=5.3569, test_R²=-124.5274\n",
      "Epoch 1051/1200\n",
      "Train loss: 6.3096, Train R²: -123.7519\n",
      "test_loss=5.3568, test_R²=-124.5247\n",
      "Epoch 1052/1200\n",
      "Train loss: 6.2905, Train R²: -123.9785\n",
      "test_loss=5.3567, test_R²=-124.5224\n",
      "Epoch 1053/1200\n",
      "Train loss: 6.3240, Train R²: -124.6572\n",
      "test_loss=5.3565, test_R²=-124.5183\n",
      "Epoch 1054/1200\n",
      "Train loss: 6.3338, Train R²: -124.0650\n",
      "test_loss=5.3565, test_R²=-124.5161\n",
      "Epoch 1055/1200\n",
      "Train loss: 6.2949, Train R²: -124.1829\n",
      "test_loss=5.3564, test_R²=-124.5147\n",
      "Epoch 1056/1200\n",
      "Train loss: 6.3262, Train R²: -125.1641\n",
      "test_loss=5.3563, test_R²=-124.5125\n",
      "Epoch 1057/1200\n",
      "Train loss: 6.2975, Train R²: -124.3774\n",
      "test_loss=5.3565, test_R²=-124.5156\n",
      "Epoch 1058/1200\n",
      "Train loss: 6.3148, Train R²: -124.3377\n",
      "test_loss=5.3566, test_R²=-124.5186\n",
      "Epoch 1059/1200\n",
      "Train loss: 6.2889, Train R²: -124.0500\n",
      "test_loss=5.3566, test_R²=-124.5186\n",
      "Epoch 1060/1200\n",
      "Train loss: 6.2975, Train R²: -123.9310\n",
      "test_loss=5.3565, test_R²=-124.5183\n",
      "Epoch 1061/1200\n",
      "Train loss: 6.2713, Train R²: -124.1932\n",
      "test_loss=5.3565, test_R²=-124.5189\n",
      "Epoch 1062/1200\n",
      "Train loss: 6.2858, Train R²: -124.0141\n",
      "test_loss=5.3565, test_R²=-124.5185\n",
      "Epoch 1063/1200\n",
      "Train loss: 6.3165, Train R²: -124.1920\n",
      "test_loss=5.3565, test_R²=-124.5188\n",
      "Epoch 1064/1200\n",
      "Train loss: 6.2988, Train R²: -124.4959\n",
      "test_loss=5.3566, test_R²=-124.5193\n",
      "Epoch 1065/1200\n",
      "Train loss: 6.3474, Train R²: -125.3025\n",
      "test_loss=5.3567, test_R²=-124.5237\n",
      "Epoch 1066/1200\n",
      "Train loss: 6.3152, Train R²: -125.2319\n",
      "test_loss=5.3568, test_R²=-124.5241\n",
      "Epoch 1067/1200\n",
      "Train loss: 6.3047, Train R²: -124.1280\n",
      "test_loss=5.3568, test_R²=-124.5250\n",
      "Epoch 1068/1200\n",
      "Train loss: 6.3016, Train R²: -124.3619\n",
      "test_loss=5.3569, test_R²=-124.5266\n",
      "Epoch 1069/1200\n",
      "Train loss: 6.3098, Train R²: -124.1888\n",
      "test_loss=5.3571, test_R²=-124.5307\n",
      "Epoch 1070/1200\n",
      "Train loss: 6.3062, Train R²: -124.2528\n",
      "test_loss=5.3571, test_R²=-124.5323\n",
      "Epoch 1071/1200\n",
      "Train loss: 6.3172, Train R²: -124.6997\n",
      "test_loss=5.3571, test_R²=-124.5319\n",
      "Epoch 1072/1200\n",
      "Train loss: 6.3265, Train R²: -123.9531\n",
      "test_loss=5.3572, test_R²=-124.5328\n",
      "Epoch 1073/1200\n",
      "Train loss: 6.3079, Train R²: -125.2444\n",
      "test_loss=5.3571, test_R²=-124.5315\n",
      "Epoch 1074/1200\n",
      "Train loss: 6.2906, Train R²: -123.8441\n",
      "test_loss=5.3571, test_R²=-124.5316\n",
      "Epoch 1075/1200\n",
      "Train loss: 6.2801, Train R²: -124.1347\n",
      "test_loss=5.3571, test_R²=-124.5314\n",
      "Epoch 1076/1200\n",
      "Train loss: 6.3313, Train R²: -123.9591\n",
      "test_loss=5.3571, test_R²=-124.5312\n",
      "Epoch 1077/1200\n",
      "Train loss: 6.3112, Train R²: -124.4782\n",
      "test_loss=5.3569, test_R²=-124.5270\n",
      "Epoch 1078/1200\n",
      "Train loss: 6.3369, Train R²: -124.7991\n",
      "test_loss=5.3566, test_R²=-124.5180\n",
      "Epoch 1079/1200\n",
      "Train loss: 6.3374, Train R²: -123.6559\n",
      "test_loss=5.3564, test_R²=-124.5125\n",
      "Epoch 1080/1200\n",
      "Train loss: 6.3012, Train R²: -125.1732\n",
      "test_loss=5.3562, test_R²=-124.5093\n",
      "Epoch 1081/1200\n",
      "Train loss: 6.2981, Train R²: -124.7474\n",
      "test_loss=5.3563, test_R²=-124.5110\n",
      "Epoch 1082/1200\n",
      "Train loss: 6.2796, Train R²: -124.1900\n",
      "test_loss=5.3565, test_R²=-124.5161\n",
      "Epoch 1083/1200\n",
      "Train loss: 6.3413, Train R²: -125.4893\n",
      "test_loss=5.3566, test_R²=-124.5192\n",
      "Epoch 1084/1200\n",
      "Train loss: 6.3124, Train R²: -124.0309\n",
      "test_loss=5.3567, test_R²=-124.5227\n",
      "Epoch 1085/1200\n",
      "Train loss: 6.3301, Train R²: -124.2639\n",
      "test_loss=5.3567, test_R²=-124.5209\n",
      "Epoch 1086/1200\n",
      "Train loss: 6.3344, Train R²: -124.5670\n",
      "test_loss=5.3567, test_R²=-124.5217\n",
      "Epoch 1087/1200\n",
      "Train loss: 6.2974, Train R²: -124.9266\n",
      "test_loss=5.3568, test_R²=-124.5232\n",
      "Epoch 1088/1200\n",
      "Train loss: 6.3346, Train R²: -124.4772\n",
      "test_loss=5.3567, test_R²=-124.5225\n",
      "Epoch 1089/1200\n",
      "Train loss: 6.3414, Train R²: -124.9336\n",
      "test_loss=5.3567, test_R²=-124.5218\n",
      "Epoch 1090/1200\n",
      "Train loss: 6.3111, Train R²: -125.0642\n",
      "test_loss=5.3566, test_R²=-124.5190\n",
      "Epoch 1091/1200\n",
      "Train loss: 6.3173, Train R²: -123.8615\n",
      "test_loss=5.3566, test_R²=-124.5183\n",
      "Epoch 1092/1200\n",
      "Train loss: 6.3287, Train R²: -124.6300\n",
      "test_loss=5.3566, test_R²=-124.5177\n",
      "Epoch 1093/1200\n",
      "Train loss: 6.3001, Train R²: -124.1615\n",
      "test_loss=5.3565, test_R²=-124.5159\n",
      "Epoch 1094/1200\n",
      "Train loss: 6.2890, Train R²: -124.5428\n",
      "test_loss=5.3564, test_R²=-124.5129\n",
      "Epoch 1095/1200\n",
      "Train loss: 6.2858, Train R²: -124.1944\n",
      "test_loss=5.3563, test_R²=-124.5106\n",
      "Epoch 1096/1200\n",
      "Train loss: 6.3131, Train R²: -124.2465\n",
      "test_loss=5.3563, test_R²=-124.5097\n",
      "Epoch 1097/1200\n",
      "Train loss: 6.3171, Train R²: -123.8515\n",
      "test_loss=5.3563, test_R²=-124.5106\n",
      "Epoch 1098/1200\n",
      "Train loss: 6.3073, Train R²: -124.0310\n",
      "test_loss=5.3563, test_R²=-124.5109\n",
      "Epoch 1099/1200\n",
      "Train loss: 6.3065, Train R²: -123.9704\n",
      "test_loss=5.3563, test_R²=-124.5118\n",
      "Epoch 1100/1200\n",
      "Train loss: 6.3024, Train R²: -123.9468\n",
      "test_loss=5.3563, test_R²=-124.5117\n",
      "Epoch 1101/1200\n",
      "Train loss: 6.2939, Train R²: -123.7651\n",
      "test_loss=5.3563, test_R²=-124.5107\n",
      "Epoch 1102/1200\n",
      "Train loss: 6.2790, Train R²: -124.6941\n",
      "test_loss=5.3563, test_R²=-124.5101\n",
      "Epoch 1103/1200\n",
      "Train loss: 6.2883, Train R²: -124.2650\n",
      "test_loss=5.3563, test_R²=-124.5099\n",
      "Epoch 1104/1200\n",
      "Train loss: 6.2965, Train R²: -124.1932\n",
      "test_loss=5.3563, test_R²=-124.5103\n",
      "Epoch 1105/1200\n",
      "Train loss: 6.3535, Train R²: -124.3687\n",
      "test_loss=5.3562, test_R²=-124.5097\n",
      "Epoch 1106/1200\n",
      "Train loss: 6.3003, Train R²: -124.4711\n",
      "test_loss=5.3561, test_R²=-124.5071\n",
      "Epoch 1107/1200\n",
      "Train loss: 6.3207, Train R²: -125.1554\n",
      "test_loss=5.3560, test_R²=-124.5057\n",
      "Epoch 1108/1200\n",
      "Train loss: 6.2958, Train R²: -124.6445\n",
      "test_loss=5.3560, test_R²=-124.5043\n",
      "Epoch 1109/1200\n",
      "Train loss: 6.2765, Train R²: -124.3701\n",
      "test_loss=5.3561, test_R²=-124.5074\n",
      "Epoch 1110/1200\n",
      "Train loss: 6.3104, Train R²: -125.1045\n",
      "test_loss=5.3561, test_R²=-124.5082\n",
      "Epoch 1111/1200\n",
      "Train loss: 6.3231, Train R²: -124.3671\n",
      "test_loss=5.3561, test_R²=-124.5080\n",
      "Epoch 1112/1200\n",
      "Train loss: 6.2849, Train R²: -123.9271\n",
      "test_loss=5.3561, test_R²=-124.5076\n",
      "Epoch 1113/1200\n",
      "Train loss: 6.3140, Train R²: -124.0251\n",
      "test_loss=5.3561, test_R²=-124.5058\n",
      "Epoch 1114/1200\n",
      "Train loss: 6.3740, Train R²: -125.1378\n",
      "test_loss=5.3561, test_R²=-124.5067\n",
      "Epoch 1115/1200\n",
      "Train loss: 6.3322, Train R²: -124.4948\n",
      "test_loss=5.3564, test_R²=-124.5130\n",
      "Epoch 1116/1200\n",
      "Train loss: 6.3070, Train R²: -123.8616\n",
      "test_loss=5.3566, test_R²=-124.5185\n",
      "Epoch 1117/1200\n",
      "Train loss: 6.3466, Train R²: -124.3413\n",
      "test_loss=5.3567, test_R²=-124.5206\n",
      "Epoch 1118/1200\n",
      "Train loss: 6.3195, Train R²: -123.9894\n",
      "test_loss=5.3567, test_R²=-124.5213\n",
      "Epoch 1119/1200\n",
      "Train loss: 6.2995, Train R²: -124.7690\n",
      "test_loss=5.3567, test_R²=-124.5215\n",
      "Epoch 1120/1200\n",
      "Train loss: 6.3443, Train R²: -125.5630\n",
      "test_loss=5.3568, test_R²=-124.5244\n",
      "Epoch 1121/1200\n",
      "Train loss: 6.2936, Train R²: -124.2067\n",
      "test_loss=5.3570, test_R²=-124.5279\n",
      "Epoch 1122/1200\n",
      "Train loss: 6.3283, Train R²: -124.5970\n",
      "test_loss=5.3570, test_R²=-124.5295\n",
      "Epoch 1123/1200\n",
      "Train loss: 6.2865, Train R²: -124.3912\n",
      "test_loss=5.3569, test_R²=-124.5259\n",
      "Epoch 1124/1200\n",
      "Train loss: 6.3063, Train R²: -124.6421\n",
      "test_loss=5.3567, test_R²=-124.5207\n",
      "Epoch 1125/1200\n",
      "Train loss: 6.3450, Train R²: -124.5871\n",
      "test_loss=5.3565, test_R²=-124.5158\n",
      "Epoch 1126/1200\n",
      "Train loss: 6.2960, Train R²: -125.3291\n",
      "test_loss=5.3564, test_R²=-124.5160\n",
      "Epoch 1127/1200\n",
      "Train loss: 6.3225, Train R²: -124.2158\n",
      "test_loss=5.3565, test_R²=-124.5165\n",
      "Epoch 1128/1200\n",
      "Train loss: 6.3424, Train R²: -124.7695\n",
      "test_loss=5.3565, test_R²=-124.5182\n",
      "Epoch 1129/1200\n",
      "Train loss: 6.3409, Train R²: -126.0769\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 1130/1200\n",
      "Train loss: 6.3091, Train R²: -125.0964\n",
      "test_loss=5.3567, test_R²=-124.5221\n",
      "Epoch 1131/1200\n",
      "Train loss: 6.3708, Train R²: -124.5671\n",
      "test_loss=5.3569, test_R²=-124.5259\n",
      "Epoch 1132/1200\n",
      "Train loss: 6.2851, Train R²: -124.2076\n",
      "test_loss=5.3569, test_R²=-124.5274\n",
      "Epoch 1133/1200\n",
      "Train loss: 6.3144, Train R²: -124.4422\n",
      "test_loss=5.3568, test_R²=-124.5247\n",
      "Epoch 1134/1200\n",
      "Train loss: 6.3169, Train R²: -123.6280\n",
      "test_loss=5.3568, test_R²=-124.5227\n",
      "Epoch 1135/1200\n",
      "Train loss: 6.3203, Train R²: -124.1916\n",
      "test_loss=5.3566, test_R²=-124.5198\n",
      "Epoch 1136/1200\n",
      "Train loss: 6.2956, Train R²: -124.9660\n",
      "test_loss=5.3566, test_R²=-124.5206\n",
      "Epoch 1137/1200\n",
      "Train loss: 6.3640, Train R²: -124.1867\n",
      "test_loss=5.3567, test_R²=-124.5212\n",
      "Epoch 1138/1200\n",
      "Train loss: 6.3218, Train R²: -124.0845\n",
      "test_loss=5.3566, test_R²=-124.5206\n",
      "Epoch 1139/1200\n",
      "Train loss: 6.2469, Train R²: -123.8956\n",
      "test_loss=5.3566, test_R²=-124.5205\n",
      "Epoch 1140/1200\n",
      "Train loss: 6.3083, Train R²: -124.7876\n",
      "test_loss=5.3565, test_R²=-124.5187\n",
      "Epoch 1141/1200\n",
      "Train loss: 6.2954, Train R²: -123.8360\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 1142/1200\n",
      "Train loss: 6.2930, Train R²: -124.4279\n",
      "test_loss=5.3564, test_R²=-124.5154\n",
      "Epoch 1143/1200\n",
      "Train loss: 6.3177, Train R²: -123.7740\n",
      "test_loss=5.3564, test_R²=-124.5168\n",
      "Epoch 1144/1200\n",
      "Train loss: 6.2966, Train R²: -123.8056\n",
      "test_loss=5.3564, test_R²=-124.5171\n",
      "Epoch 1145/1200\n",
      "Train loss: 6.3097, Train R²: -124.3178\n",
      "test_loss=5.3563, test_R²=-124.5150\n",
      "Epoch 1146/1200\n",
      "Train loss: 6.3219, Train R²: -124.1904\n",
      "test_loss=5.3563, test_R²=-124.5131\n",
      "Epoch 1147/1200\n",
      "Train loss: 6.3118, Train R²: -125.1355\n",
      "test_loss=5.3563, test_R²=-124.5131\n",
      "Epoch 1148/1200\n",
      "Train loss: 6.3439, Train R²: -124.4538\n",
      "test_loss=5.3562, test_R²=-124.5115\n",
      "Epoch 1149/1200\n",
      "Train loss: 6.2972, Train R²: -123.8662\n",
      "test_loss=5.3562, test_R²=-124.5101\n",
      "Epoch 1150/1200\n",
      "Train loss: 6.2908, Train R²: -124.1698\n",
      "test_loss=5.3562, test_R²=-124.5108\n",
      "Epoch 1151/1200\n",
      "Train loss: 6.2998, Train R²: -124.1888\n",
      "test_loss=5.3562, test_R²=-124.5110\n",
      "Epoch 1152/1200\n",
      "Train loss: 6.3498, Train R²: -124.0211\n",
      "test_loss=5.3562, test_R²=-124.5119\n",
      "Epoch 1153/1200\n",
      "Train loss: 6.3170, Train R²: -124.2172\n",
      "test_loss=5.3564, test_R²=-124.5150\n",
      "Epoch 1154/1200\n",
      "Train loss: 6.3183, Train R²: -124.2597\n",
      "test_loss=5.3564, test_R²=-124.5164\n",
      "Epoch 1155/1200\n",
      "Train loss: 6.3108, Train R²: -124.0372\n",
      "test_loss=5.3564, test_R²=-124.5163\n",
      "Epoch 1156/1200\n",
      "Train loss: 6.3482, Train R²: -125.2399\n",
      "test_loss=5.3564, test_R²=-124.5151\n",
      "Epoch 1157/1200\n",
      "Train loss: 6.3418, Train R²: -124.6329\n",
      "test_loss=5.3563, test_R²=-124.5129\n",
      "Epoch 1158/1200\n",
      "Train loss: 6.2991, Train R²: -125.0200\n",
      "test_loss=5.3562, test_R²=-124.5112\n",
      "Epoch 1159/1200\n",
      "Train loss: 6.3204, Train R²: -124.6441\n",
      "test_loss=5.3562, test_R²=-124.5106\n",
      "Epoch 1160/1200\n",
      "Train loss: 6.3401, Train R²: -123.9946\n",
      "test_loss=5.3563, test_R²=-124.5118\n",
      "Epoch 1161/1200\n",
      "Train loss: 6.2888, Train R²: -124.3522\n",
      "test_loss=5.3563, test_R²=-124.5136\n",
      "Epoch 1162/1200\n",
      "Train loss: 6.2942, Train R²: -124.7613\n",
      "test_loss=5.3564, test_R²=-124.5156\n",
      "Epoch 1163/1200\n",
      "Train loss: 6.3151, Train R²: -125.2377\n",
      "test_loss=5.3566, test_R²=-124.5189\n",
      "Epoch 1164/1200\n",
      "Train loss: 6.2789, Train R²: -124.1984\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 1165/1200\n",
      "Train loss: 6.2715, Train R²: -124.2684\n",
      "test_loss=5.3565, test_R²=-124.5170\n",
      "Epoch 1166/1200\n",
      "Train loss: 6.3425, Train R²: -124.5838\n",
      "test_loss=5.3565, test_R²=-124.5164\n",
      "Epoch 1167/1200\n",
      "Train loss: 6.2605, Train R²: -123.6177\n",
      "test_loss=5.3564, test_R²=-124.5154\n",
      "Epoch 1168/1200\n",
      "Train loss: 6.3392, Train R²: -124.2323\n",
      "test_loss=5.3564, test_R²=-124.5144\n",
      "Epoch 1169/1200\n",
      "Train loss: 6.3390, Train R²: -125.3195\n",
      "test_loss=5.3564, test_R²=-124.5148\n",
      "Epoch 1170/1200\n",
      "Train loss: 6.2829, Train R²: -124.8930\n",
      "test_loss=5.3566, test_R²=-124.5188\n",
      "Epoch 1171/1200\n",
      "Train loss: 6.3488, Train R²: -124.5885\n",
      "test_loss=5.3566, test_R²=-124.5199\n",
      "Epoch 1172/1200\n",
      "Train loss: 6.3093, Train R²: -123.7643\n",
      "test_loss=5.3566, test_R²=-124.5196\n",
      "Epoch 1173/1200\n",
      "Train loss: 6.3577, Train R²: -124.0826\n",
      "test_loss=5.3565, test_R²=-124.5180\n",
      "Epoch 1174/1200\n",
      "Train loss: 6.3363, Train R²: -124.1243\n",
      "test_loss=5.3564, test_R²=-124.5160\n",
      "Epoch 1175/1200\n",
      "Train loss: 6.2767, Train R²: -124.5935\n",
      "test_loss=5.3564, test_R²=-124.5158\n",
      "Epoch 1176/1200\n",
      "Train loss: 6.3419, Train R²: -125.0233\n",
      "test_loss=5.3563, test_R²=-124.5120\n",
      "Epoch 1177/1200\n",
      "Train loss: 6.3052, Train R²: -124.4350\n",
      "test_loss=5.3563, test_R²=-124.5134\n",
      "Epoch 1178/1200\n",
      "Train loss: 6.3264, Train R²: -125.2216\n",
      "test_loss=5.3566, test_R²=-124.5198\n",
      "Epoch 1179/1200\n",
      "Train loss: 6.3248, Train R²: -123.6819\n",
      "test_loss=5.3568, test_R²=-124.5250\n",
      "Epoch 1180/1200\n",
      "Train loss: 6.3194, Train R²: -124.8718\n",
      "test_loss=5.3569, test_R²=-124.5267\n",
      "Epoch 1181/1200\n",
      "Train loss: 6.3203, Train R²: -124.8588\n",
      "test_loss=5.3568, test_R²=-124.5245\n",
      "Epoch 1182/1200\n",
      "Train loss: 6.3024, Train R²: -123.9389\n",
      "test_loss=5.3567, test_R²=-124.5234\n",
      "Epoch 1183/1200\n",
      "Train loss: 6.3427, Train R²: -124.3521\n",
      "test_loss=5.3567, test_R²=-124.5230\n",
      "Epoch 1184/1200\n",
      "Train loss: 6.3390, Train R²: -125.3143\n",
      "test_loss=5.3566, test_R²=-124.5209\n",
      "Epoch 1185/1200\n",
      "Train loss: 6.3954, Train R²: -124.2369\n",
      "test_loss=5.3567, test_R²=-124.5222\n",
      "Epoch 1186/1200\n",
      "Train loss: 6.3002, Train R²: -124.1846\n",
      "test_loss=5.3568, test_R²=-124.5250\n",
      "Epoch 1187/1200\n",
      "Train loss: 6.3309, Train R²: -124.0924\n",
      "test_loss=5.3569, test_R²=-124.5278\n",
      "Epoch 1188/1200\n",
      "Train loss: 6.3129, Train R²: -125.0727\n",
      "test_loss=5.3569, test_R²=-124.5287\n",
      "Epoch 1189/1200\n",
      "Train loss: 6.3066, Train R²: -125.0340\n",
      "test_loss=5.3568, test_R²=-124.5256\n",
      "Epoch 1190/1200\n",
      "Train loss: 6.2664, Train R²: -123.6000\n",
      "test_loss=5.3567, test_R²=-124.5223\n",
      "Epoch 1191/1200\n",
      "Train loss: 6.3036, Train R²: -123.8359\n",
      "test_loss=5.3566, test_R²=-124.5196\n",
      "Epoch 1192/1200\n",
      "Train loss: 6.2886, Train R²: -124.2024\n",
      "test_loss=5.3566, test_R²=-124.5201\n",
      "Epoch 1193/1200\n",
      "Train loss: 6.2891, Train R²: -123.9660\n",
      "test_loss=5.3567, test_R²=-124.5224\n",
      "Epoch 1194/1200\n",
      "Train loss: 6.3098, Train R²: -124.9124\n",
      "test_loss=5.3568, test_R²=-124.5256\n",
      "Epoch 1195/1200\n",
      "Train loss: 6.3060, Train R²: -124.3346\n",
      "test_loss=5.3568, test_R²=-124.5253\n",
      "Epoch 1196/1200\n",
      "Train loss: 6.2690, Train R²: -124.5861\n",
      "test_loss=5.3569, test_R²=-124.5275\n",
      "Epoch 1197/1200\n",
      "Train loss: 6.3131, Train R²: -124.2267\n",
      "test_loss=5.3570, test_R²=-124.5291\n",
      "Epoch 1198/1200\n",
      "Train loss: 6.3162, Train R²: -123.5668\n",
      "test_loss=5.3570, test_R²=-124.5290\n",
      "Epoch 1199/1200\n",
      "Train loss: 6.3130, Train R²: -124.8150\n",
      "test_loss=5.3569, test_R²=-124.5255\n",
      "Epoch 1200/1200\n",
      "Train loss: 6.3268, Train R²: -124.2863\n",
      "test_loss=5.3567, test_R²=-124.5221\n"
     ]
    }
   ],
   "source": [
    "metrics = model.fit(dl_train, dl_test, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200\n",
      "Train loss: 6.3398, Train R²: -124.0948\n",
      "test_loss=5.3566, test_R²=-124.5194\n",
      "Epoch 2/1200\n",
      "Train loss: 6.2844, Train R²: -124.6494\n",
      "test_loss=5.3565, test_R²=-124.5162\n",
      "Epoch 3/1200\n",
      "Train loss: 6.2779, Train R²: -124.6495\n",
      "test_loss=5.3564, test_R²=-124.5136\n",
      "Epoch 4/1200\n",
      "Train loss: 6.3428, Train R²: -124.2243\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 5/1200\n",
      "Train loss: 6.3442, Train R²: -124.0692\n",
      "test_loss=5.3564, test_R²=-124.5156\n",
      "Epoch 6/1200\n",
      "Train loss: 6.3082, Train R²: -124.6401\n",
      "test_loss=5.3565, test_R²=-124.5163\n",
      "Epoch 7/1200\n",
      "Train loss: 6.2989, Train R²: -124.2561\n",
      "test_loss=5.3565, test_R²=-124.5171\n",
      "Epoch 8/1200\n",
      "Train loss: 6.3303, Train R²: -124.1977\n",
      "test_loss=5.3565, test_R²=-124.5163\n",
      "Epoch 9/1200\n",
      "Train loss: 6.3260, Train R²: -124.0054\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 10/1200\n",
      "Train loss: 6.2949, Train R²: -123.9358\n",
      "test_loss=5.3564, test_R²=-124.5148\n",
      "Epoch 11/1200\n",
      "Train loss: 6.2956, Train R²: -124.0175\n",
      "test_loss=5.3564, test_R²=-124.5165\n",
      "Epoch 12/1200\n",
      "Train loss: 6.3052, Train R²: -124.5394\n",
      "test_loss=5.3565, test_R²=-124.5176\n",
      "Epoch 13/1200\n",
      "Train loss: 6.2925, Train R²: -123.9776\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 14/1200\n",
      "Train loss: 6.3462, Train R²: -124.1659\n",
      "test_loss=5.3564, test_R²=-124.5160\n",
      "Epoch 15/1200\n",
      "Train loss: 6.3011, Train R²: -124.2274\n",
      "test_loss=5.3564, test_R²=-124.5153\n",
      "Epoch 16/1200\n",
      "Train loss: 6.2922, Train R²: -123.9373\n",
      "test_loss=5.3563, test_R²=-124.5144\n",
      "Epoch 17/1200\n",
      "Train loss: 6.3126, Train R²: -123.9706\n",
      "test_loss=5.3564, test_R²=-124.5152\n",
      "Epoch 18/1200\n",
      "Train loss: 6.2718, Train R²: -124.4527\n",
      "test_loss=5.3563, test_R²=-124.5139\n",
      "Epoch 19/1200\n",
      "Train loss: 6.3219, Train R²: -123.8999\n",
      "test_loss=5.3563, test_R²=-124.5147\n",
      "Epoch 20/1200\n",
      "Train loss: 6.3197, Train R²: -123.9015\n",
      "test_loss=5.3564, test_R²=-124.5169\n",
      "Epoch 21/1200\n",
      "Train loss: 6.3027, Train R²: -123.7842\n",
      "test_loss=5.3564, test_R²=-124.5173\n",
      "Epoch 22/1200\n",
      "Train loss: 6.3458, Train R²: -124.4021\n",
      "test_loss=5.3565, test_R²=-124.5177\n",
      "Epoch 23/1200\n",
      "Train loss: 6.2944, Train R²: -124.1493\n",
      "test_loss=5.3565, test_R²=-124.5190\n",
      "Epoch 24/1200\n",
      "Train loss: 6.3357, Train R²: -123.8623\n",
      "test_loss=5.3566, test_R²=-124.5203\n",
      "Epoch 25/1200\n",
      "Train loss: 6.3130, Train R²: -123.9669\n",
      "test_loss=5.3566, test_R²=-124.5200\n",
      "Epoch 26/1200\n",
      "Train loss: 6.3486, Train R²: -124.0563\n",
      "test_loss=5.3567, test_R²=-124.5224\n",
      "Epoch 27/1200\n",
      "Train loss: 6.3069, Train R²: -124.4566\n",
      "test_loss=5.3567, test_R²=-124.5219\n",
      "Epoch 28/1200\n",
      "Train loss: 6.3291, Train R²: -123.7121\n",
      "test_loss=5.3566, test_R²=-124.5202\n",
      "Epoch 29/1200\n",
      "Train loss: 6.3051, Train R²: -124.9855\n",
      "test_loss=5.3564, test_R²=-124.5153\n",
      "Epoch 30/1200\n",
      "Train loss: 6.3338, Train R²: -124.8731\n",
      "test_loss=5.3562, test_R²=-124.5095\n",
      "Epoch 31/1200\n",
      "Train loss: 6.3470, Train R²: -124.9856\n",
      "test_loss=5.3560, test_R²=-124.5053\n",
      "Epoch 32/1200\n",
      "Train loss: 6.3185, Train R²: -124.2608\n",
      "test_loss=5.3560, test_R²=-124.5042\n",
      "Epoch 33/1200\n",
      "Train loss: 6.2972, Train R²: -123.9131\n",
      "test_loss=5.3561, test_R²=-124.5058\n",
      "Epoch 34/1200\n",
      "Train loss: 6.3149, Train R²: -124.3977\n",
      "test_loss=5.3561, test_R²=-124.5068\n",
      "Epoch 35/1200\n",
      "Train loss: 6.3494, Train R²: -124.2272\n",
      "test_loss=5.3562, test_R²=-124.5079\n",
      "Epoch 36/1200\n",
      "Train loss: 6.2975, Train R²: -124.4253\n",
      "test_loss=5.3562, test_R²=-124.5091\n",
      "Epoch 37/1200\n",
      "Train loss: 6.3361, Train R²: -124.3610\n",
      "test_loss=5.3562, test_R²=-124.5080\n",
      "Epoch 38/1200\n",
      "Train loss: 6.2998, Train R²: -124.2944\n",
      "test_loss=5.3562, test_R²=-124.5096\n",
      "Epoch 39/1200\n",
      "Train loss: 6.3659, Train R²: -124.4721\n",
      "test_loss=5.3563, test_R²=-124.5117\n",
      "Epoch 40/1200\n",
      "Train loss: 6.3314, Train R²: -124.5715\n",
      "test_loss=5.3563, test_R²=-124.5127\n",
      "Epoch 41/1200\n",
      "Train loss: 6.3156, Train R²: -124.5109\n",
      "test_loss=5.3563, test_R²=-124.5121\n",
      "Epoch 42/1200\n",
      "Train loss: 6.3010, Train R²: -125.4375\n",
      "test_loss=5.3563, test_R²=-124.5130\n",
      "Epoch 43/1200\n",
      "Train loss: 6.3094, Train R²: -124.4826\n",
      "test_loss=5.3563, test_R²=-124.5118\n",
      "Epoch 44/1200\n",
      "Train loss: 6.2907, Train R²: -123.9555\n",
      "test_loss=5.3563, test_R²=-124.5125\n",
      "Epoch 45/1200\n",
      "Train loss: 6.3130, Train R²: -124.6277\n",
      "test_loss=5.3564, test_R²=-124.5152\n",
      "Epoch 46/1200\n",
      "Train loss: 6.3433, Train R²: -123.9962\n",
      "test_loss=5.3565, test_R²=-124.5170\n",
      "Epoch 47/1200\n",
      "Train loss: 6.2967, Train R²: -124.1349\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 48/1200\n",
      "Train loss: 6.3047, Train R²: -124.3952\n",
      "test_loss=5.3565, test_R²=-124.5168\n",
      "Epoch 49/1200\n",
      "Train loss: 6.2906, Train R²: -124.0922\n",
      "test_loss=5.3564, test_R²=-124.5154\n",
      "Epoch 50/1200\n",
      "Train loss: 6.3049, Train R²: -124.0043\n",
      "test_loss=5.3564, test_R²=-124.5150\n",
      "Epoch 51/1200\n",
      "Train loss: 6.3028, Train R²: -124.1645\n",
      "test_loss=5.3564, test_R²=-124.5157\n",
      "Epoch 52/1200\n",
      "Train loss: 6.2919, Train R²: -124.9887\n",
      "test_loss=5.3565, test_R²=-124.5177\n",
      "Epoch 53/1200\n",
      "Train loss: 6.3394, Train R²: -123.8245\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 54/1200\n",
      "Train loss: 6.3048, Train R²: -124.1592\n",
      "test_loss=5.3566, test_R²=-124.5193\n",
      "Epoch 55/1200\n",
      "Train loss: 6.3382, Train R²: -124.3152\n",
      "test_loss=5.3566, test_R²=-124.5193\n",
      "Epoch 56/1200\n",
      "Train loss: 6.3025, Train R²: -123.9813\n",
      "test_loss=5.3567, test_R²=-124.5226\n",
      "Epoch 57/1200\n",
      "Train loss: 6.2774, Train R²: -124.2402\n",
      "test_loss=5.3568, test_R²=-124.5235\n",
      "Epoch 58/1200\n",
      "Train loss: 6.3433, Train R²: -124.0734\n",
      "test_loss=5.3567, test_R²=-124.5223\n",
      "Epoch 59/1200\n",
      "Train loss: 6.3262, Train R²: -124.9147\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 60/1200\n",
      "Train loss: 6.3388, Train R²: -123.9023\n",
      "test_loss=5.3566, test_R²=-124.5189\n",
      "Epoch 61/1200\n",
      "Train loss: 6.3059, Train R²: -124.3453\n",
      "test_loss=5.3567, test_R²=-124.5215\n",
      "Epoch 62/1200\n",
      "Train loss: 6.3578, Train R²: -124.7709\n",
      "test_loss=5.3568, test_R²=-124.5240\n",
      "Epoch 63/1200\n",
      "Train loss: 6.3112, Train R²: -124.3010\n",
      "test_loss=5.3568, test_R²=-124.5241\n",
      "Epoch 64/1200\n",
      "Train loss: 6.2735, Train R²: -124.7608\n",
      "test_loss=5.3567, test_R²=-124.5210\n",
      "Epoch 65/1200\n",
      "Train loss: 6.3157, Train R²: -123.9845\n",
      "test_loss=5.3565, test_R²=-124.5174\n",
      "Epoch 66/1200\n",
      "Train loss: 6.3443, Train R²: -123.7333\n",
      "test_loss=5.3564, test_R²=-124.5144\n",
      "Epoch 67/1200\n",
      "Train loss: 6.3569, Train R²: -124.4067\n",
      "test_loss=5.3564, test_R²=-124.5153\n",
      "Epoch 68/1200\n",
      "Train loss: 6.3020, Train R²: -123.7429\n",
      "test_loss=5.3565, test_R²=-124.5163\n",
      "Epoch 69/1200\n",
      "Train loss: 6.2952, Train R²: -124.1313\n",
      "test_loss=5.3565, test_R²=-124.5161\n",
      "Epoch 70/1200\n",
      "Train loss: 6.3141, Train R²: -123.6236\n",
      "test_loss=5.3564, test_R²=-124.5143\n",
      "Epoch 71/1200\n",
      "Train loss: 6.3144, Train R²: -124.1462\n",
      "test_loss=5.3564, test_R²=-124.5140\n",
      "Epoch 72/1200\n",
      "Train loss: 6.2972, Train R²: -125.5341\n",
      "test_loss=5.3565, test_R²=-124.5166\n",
      "Epoch 73/1200\n",
      "Train loss: 6.2699, Train R²: -125.1562\n",
      "test_loss=5.3565, test_R²=-124.5190\n",
      "Epoch 74/1200\n",
      "Train loss: 6.2935, Train R²: -125.0687\n",
      "test_loss=5.3566, test_R²=-124.5219\n",
      "Epoch 75/1200\n",
      "Train loss: 6.3302, Train R²: -124.1816\n",
      "test_loss=5.3567, test_R²=-124.5243\n",
      "Epoch 76/1200\n",
      "Train loss: 6.3019, Train R²: -123.7155\n",
      "test_loss=5.3567, test_R²=-124.5240\n",
      "Epoch 77/1200\n",
      "Train loss: 6.3218, Train R²: -124.3224\n",
      "test_loss=5.3567, test_R²=-124.5242\n",
      "Epoch 78/1200\n",
      "Train loss: 6.3178, Train R²: -124.1243\n",
      "test_loss=5.3566, test_R²=-124.5213\n",
      "Epoch 79/1200\n",
      "Train loss: 6.2815, Train R²: -124.5962\n",
      "test_loss=5.3566, test_R²=-124.5197\n",
      "Epoch 80/1200\n",
      "Train loss: 6.2962, Train R²: -123.9624\n",
      "test_loss=5.3565, test_R²=-124.5191\n",
      "Epoch 81/1200\n",
      "Train loss: 6.2862, Train R²: -124.1277\n",
      "test_loss=5.3565, test_R²=-124.5187\n",
      "Epoch 82/1200\n",
      "Train loss: 6.3317, Train R²: -124.3533\n",
      "test_loss=5.3565, test_R²=-124.5172\n",
      "Epoch 83/1200\n",
      "Train loss: 6.3300, Train R²: -124.1500\n",
      "test_loss=5.3563, test_R²=-124.5129\n",
      "Epoch 84/1200\n",
      "Train loss: 6.3609, Train R²: -124.0055\n",
      "test_loss=5.3563, test_R²=-124.5114\n",
      "Epoch 85/1200\n",
      "Train loss: 6.2858, Train R²: -123.6536\n",
      "test_loss=5.3562, test_R²=-124.5087\n",
      "Epoch 86/1200\n",
      "Train loss: 6.3107, Train R²: -124.7831\n",
      "test_loss=5.3562, test_R²=-124.5090\n",
      "Epoch 87/1200\n",
      "Train loss: 6.2983, Train R²: -125.6537\n",
      "test_loss=5.3562, test_R²=-124.5105\n",
      "Epoch 88/1200\n",
      "Train loss: 6.2841, Train R²: -125.4876\n",
      "test_loss=5.3562, test_R²=-124.5098\n",
      "Epoch 89/1200\n",
      "Train loss: 6.3169, Train R²: -123.8526\n",
      "test_loss=5.3563, test_R²=-124.5118\n",
      "Epoch 90/1200\n",
      "Train loss: 6.2969, Train R²: -125.0703\n",
      "test_loss=5.3563, test_R²=-124.5109\n",
      "Epoch 91/1200\n",
      "Train loss: 6.3113, Train R²: -125.0934\n",
      "test_loss=5.3563, test_R²=-124.5113\n",
      "Epoch 92/1200\n",
      "Train loss: 6.3006, Train R²: -124.0799\n",
      "test_loss=5.3566, test_R²=-124.5161\n",
      "Epoch 93/1200\n",
      "Train loss: 6.2953, Train R²: -124.5019\n",
      "test_loss=5.3568, test_R²=-124.5208\n",
      "Epoch 94/1200\n",
      "Train loss: 6.2929, Train R²: -124.9454\n",
      "test_loss=5.3569, test_R²=-124.5238\n",
      "Epoch 95/1200\n",
      "Train loss: 6.3036, Train R²: -125.7311\n",
      "test_loss=5.3571, test_R²=-124.5293\n",
      "Epoch 96/1200\n",
      "Train loss: 6.3097, Train R²: -124.0198\n",
      "test_loss=5.3572, test_R²=-124.5313\n",
      "Epoch 97/1200\n",
      "Train loss: 6.3189, Train R²: -123.8765\n",
      "test_loss=5.3570, test_R²=-124.5275\n",
      "Epoch 98/1200\n",
      "Train loss: 6.2752, Train R²: -124.2844\n",
      "test_loss=5.3567, test_R²=-124.5191\n",
      "Epoch 99/1200\n",
      "Train loss: 6.3313, Train R²: -124.8304\n",
      "test_loss=5.3565, test_R²=-124.5144\n",
      "Epoch 100/1200\n",
      "Train loss: 6.3232, Train R²: -123.8880\n",
      "test_loss=5.3565, test_R²=-124.5139\n",
      "Epoch 101/1200\n",
      "Train loss: 6.3221, Train R²: -123.8224\n",
      "test_loss=5.3565, test_R²=-124.5137\n",
      "Epoch 102/1200\n",
      "Train loss: 6.3085, Train R²: -124.2640\n",
      "test_loss=5.3564, test_R²=-124.5117\n",
      "Epoch 103/1200\n",
      "Train loss: 6.3461, Train R²: -125.9065\n",
      "test_loss=5.3563, test_R²=-124.5115\n",
      "Epoch 104/1200\n",
      "Train loss: 6.3206, Train R²: -124.5632\n",
      "test_loss=5.3566, test_R²=-124.5175\n",
      "Epoch 105/1200\n",
      "Train loss: 6.3085, Train R²: -123.6263\n",
      "test_loss=5.3569, test_R²=-124.5241\n",
      "Epoch 106/1200\n",
      "Train loss: 6.3544, Train R²: -125.0443\n",
      "test_loss=5.3568, test_R²=-124.5218\n",
      "Epoch 107/1200\n",
      "Train loss: 6.2746, Train R²: -123.8873\n",
      "test_loss=5.3566, test_R²=-124.5167\n",
      "Epoch 108/1200\n",
      "Train loss: 6.2627, Train R²: -125.2031\n",
      "test_loss=5.3567, test_R²=-124.5188\n",
      "Epoch 109/1200\n",
      "Train loss: 6.3540, Train R²: -124.3094\n",
      "test_loss=5.3566, test_R²=-124.5166\n",
      "Epoch 110/1200\n",
      "Train loss: 6.3240, Train R²: -124.0695\n",
      "test_loss=5.3563, test_R²=-124.5096\n",
      "Epoch 111/1200\n",
      "Train loss: 6.3127, Train R²: -124.9194\n",
      "test_loss=5.3561, test_R²=-124.5072\n",
      "Epoch 112/1200\n",
      "Train loss: 6.2992, Train R²: -124.3488\n",
      "test_loss=5.3560, test_R²=-124.5037\n",
      "Epoch 113/1200\n",
      "Train loss: 6.3114, Train R²: -124.7502\n",
      "test_loss=5.3558, test_R²=-124.4999\n",
      "Epoch 114/1200\n",
      "Train loss: 6.3160, Train R²: -124.4615\n",
      "test_loss=5.3558, test_R²=-124.4999\n",
      "Epoch 115/1200\n",
      "Train loss: 6.2929, Train R²: -124.2213\n",
      "test_loss=5.3558, test_R²=-124.4983\n",
      "Epoch 116/1200\n",
      "Train loss: 6.2995, Train R²: -124.1785\n",
      "test_loss=5.3558, test_R²=-124.4987\n",
      "Epoch 117/1200\n",
      "Train loss: 6.3020, Train R²: -124.4227\n",
      "test_loss=5.3559, test_R²=-124.5019\n",
      "Epoch 118/1200\n",
      "Train loss: 6.3064, Train R²: -123.9107\n",
      "test_loss=5.3560, test_R²=-124.5039\n",
      "Epoch 119/1200\n",
      "Train loss: 6.3104, Train R²: -123.7069\n",
      "test_loss=5.3561, test_R²=-124.5057\n",
      "Epoch 120/1200\n",
      "Train loss: 6.2867, Train R²: -124.4727\n",
      "test_loss=5.3562, test_R²=-124.5095\n",
      "Epoch 121/1200\n",
      "Train loss: 6.3534, Train R²: -124.5334\n",
      "test_loss=5.3564, test_R²=-124.5147\n",
      "Epoch 122/1200\n",
      "Train loss: 6.2948, Train R²: -124.2133\n",
      "test_loss=5.3565, test_R²=-124.5168\n",
      "Epoch 123/1200\n",
      "Train loss: 6.3012, Train R²: -124.2104\n",
      "test_loss=5.3565, test_R²=-124.5164\n",
      "Epoch 124/1200\n",
      "Train loss: 6.3132, Train R²: -124.7286\n",
      "test_loss=5.3566, test_R²=-124.5176\n",
      "Epoch 125/1200\n",
      "Train loss: 6.3275, Train R²: -123.7426\n",
      "test_loss=5.3566, test_R²=-124.5186\n",
      "Epoch 126/1200\n",
      "Train loss: 6.3201, Train R²: -124.0793\n",
      "test_loss=5.3565, test_R²=-124.5172\n",
      "Epoch 127/1200\n",
      "Train loss: 6.3143, Train R²: -124.0865\n",
      "test_loss=5.3564, test_R²=-124.5132\n",
      "Epoch 128/1200\n",
      "Train loss: 6.3076, Train R²: -124.0494\n",
      "test_loss=5.3563, test_R²=-124.5100\n",
      "Epoch 129/1200\n",
      "Train loss: 6.2788, Train R²: -124.2262\n",
      "test_loss=5.3563, test_R²=-124.5094\n",
      "Epoch 130/1200\n",
      "Train loss: 6.3020, Train R²: -125.0405\n",
      "test_loss=5.3563, test_R²=-124.5101\n",
      "Epoch 131/1200\n",
      "Train loss: 6.3127, Train R²: -124.3092\n",
      "test_loss=5.3563, test_R²=-124.5116\n",
      "Epoch 132/1200\n",
      "Train loss: 6.3277, Train R²: -124.4436\n",
      "test_loss=5.3564, test_R²=-124.5121\n",
      "Epoch 133/1200\n",
      "Train loss: 6.3088, Train R²: -124.8007\n",
      "test_loss=5.3562, test_R²=-124.5083\n",
      "Epoch 134/1200\n",
      "Train loss: 6.2746, Train R²: -124.0739\n",
      "test_loss=5.3561, test_R²=-124.5059\n",
      "Epoch 135/1200\n",
      "Train loss: 6.2997, Train R²: -124.7450\n",
      "test_loss=5.3561, test_R²=-124.5047\n",
      "Epoch 136/1200\n",
      "Train loss: 6.2860, Train R²: -124.5399\n",
      "test_loss=5.3560, test_R²=-124.5028\n",
      "Epoch 137/1200\n",
      "Train loss: 6.3330, Train R²: -123.9675\n",
      "test_loss=5.3559, test_R²=-124.5016\n",
      "Epoch 138/1200\n",
      "Train loss: 6.3111, Train R²: -124.5960\n",
      "test_loss=5.3560, test_R²=-124.5024\n",
      "Epoch 139/1200\n",
      "Train loss: 6.2665, Train R²: -124.6547\n",
      "test_loss=5.3560, test_R²=-124.5029\n",
      "Epoch 140/1200\n",
      "Train loss: 6.3101, Train R²: -124.6533\n",
      "test_loss=5.3560, test_R²=-124.5034\n",
      "Epoch 141/1200\n",
      "Train loss: 6.3119, Train R²: -125.2201\n",
      "test_loss=5.3561, test_R²=-124.5055\n",
      "Epoch 142/1200\n",
      "Train loss: 6.3100, Train R²: -124.3351\n",
      "test_loss=5.3562, test_R²=-124.5080\n",
      "Epoch 143/1200\n",
      "Train loss: 6.3307, Train R²: -124.2871\n",
      "test_loss=5.3562, test_R²=-124.5070\n",
      "Epoch 144/1200\n",
      "Train loss: 6.3397, Train R²: -124.1156\n",
      "test_loss=5.3561, test_R²=-124.5040\n",
      "Epoch 145/1200\n",
      "Train loss: 6.2858, Train R²: -124.8652\n",
      "test_loss=5.3559, test_R²=-124.4994\n",
      "Epoch 146/1200\n",
      "Train loss: 6.3467, Train R²: -124.1147\n",
      "test_loss=5.3558, test_R²=-124.4984\n",
      "Epoch 147/1200\n",
      "Train loss: 6.3261, Train R²: -125.1821\n",
      "test_loss=5.3558, test_R²=-124.4986\n",
      "Epoch 148/1200\n",
      "Train loss: 6.3029, Train R²: -125.0882\n",
      "test_loss=5.3560, test_R²=-124.5024\n",
      "Epoch 149/1200\n",
      "Train loss: 6.3040, Train R²: -124.4858\n",
      "test_loss=5.3561, test_R²=-124.5054\n",
      "Epoch 150/1200\n",
      "Train loss: 6.3416, Train R²: -123.7427\n",
      "test_loss=5.3562, test_R²=-124.5073\n",
      "Epoch 151/1200\n",
      "Train loss: 6.3185, Train R²: -124.6536\n",
      "test_loss=5.3563, test_R²=-124.5099\n",
      "Epoch 152/1200\n",
      "Train loss: 6.2823, Train R²: -124.6034\n",
      "test_loss=5.3564, test_R²=-124.5125\n",
      "Epoch 153/1200\n",
      "Train loss: 6.3583, Train R²: -124.2854\n",
      "test_loss=5.3565, test_R²=-124.5150\n",
      "Epoch 154/1200\n",
      "Train loss: 6.2916, Train R²: -124.4893\n",
      "test_loss=5.3567, test_R²=-124.5190\n",
      "Epoch 155/1200\n",
      "Train loss: 6.2869, Train R²: -123.8938\n",
      "test_loss=5.3567, test_R²=-124.5213\n",
      "Epoch 156/1200\n",
      "Train loss: 6.3527, Train R²: -125.0273\n",
      "test_loss=5.3567, test_R²=-124.5225\n",
      "Epoch 157/1200\n",
      "Train loss: 6.3566, Train R²: -124.2539\n",
      "test_loss=5.3569, test_R²=-124.5262\n",
      "Epoch 158/1200\n",
      "Train loss: 6.3245, Train R²: -125.1702\n",
      "test_loss=5.3569, test_R²=-124.5279\n",
      "Epoch 159/1200\n",
      "Train loss: 6.3041, Train R²: -124.0100\n",
      "test_loss=5.3569, test_R²=-124.5282\n",
      "Epoch 160/1200\n",
      "Train loss: 6.3627, Train R²: -125.1089\n",
      "test_loss=5.3571, test_R²=-124.5320\n",
      "Epoch 161/1200\n",
      "Train loss: 6.3297, Train R²: -124.7314\n",
      "test_loss=5.3572, test_R²=-124.5355\n",
      "Epoch 162/1200\n",
      "Train loss: 6.3086, Train R²: -123.7573\n",
      "test_loss=5.3573, test_R²=-124.5381\n",
      "Epoch 163/1200\n",
      "Train loss: 6.2948, Train R²: -124.4609\n",
      "test_loss=5.3572, test_R²=-124.5362\n",
      "Epoch 164/1200\n",
      "Train loss: 6.2922, Train R²: -123.6395\n",
      "test_loss=5.3571, test_R²=-124.5321\n",
      "Epoch 165/1200\n",
      "Train loss: 6.3410, Train R²: -124.5622\n",
      "test_loss=5.3570, test_R²=-124.5303\n",
      "Epoch 166/1200\n",
      "Train loss: 6.3385, Train R²: -124.0866\n",
      "test_loss=5.3567, test_R²=-124.5229\n",
      "Epoch 167/1200\n",
      "Train loss: 6.3029, Train R²: -124.0548\n",
      "test_loss=5.3564, test_R²=-124.5154\n",
      "Epoch 168/1200\n",
      "Train loss: 6.3125, Train R²: -125.2641\n",
      "test_loss=5.3563, test_R²=-124.5115\n",
      "Epoch 169/1200\n",
      "Train loss: 6.3151, Train R²: -124.2570\n",
      "test_loss=5.3563, test_R²=-124.5137\n",
      "Epoch 170/1200\n",
      "Train loss: 6.3480, Train R²: -124.4431\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 171/1200\n",
      "Train loss: 6.2960, Train R²: -124.2372\n",
      "test_loss=5.3565, test_R²=-124.5189\n",
      "Epoch 172/1200\n",
      "Train loss: 6.3447, Train R²: -124.1949\n",
      "test_loss=5.3564, test_R²=-124.5161\n",
      "Epoch 173/1200\n",
      "Train loss: 6.3289, Train R²: -124.1244\n",
      "test_loss=5.3563, test_R²=-124.5125\n",
      "Epoch 174/1200\n",
      "Train loss: 6.2769, Train R²: -124.2911\n",
      "test_loss=5.3562, test_R²=-124.5120\n",
      "Epoch 175/1200\n",
      "Train loss: 6.3391, Train R²: -123.8448\n",
      "test_loss=5.3562, test_R²=-124.5104\n",
      "Epoch 176/1200\n",
      "Train loss: 6.3303, Train R²: -125.4416\n",
      "test_loss=5.3562, test_R²=-124.5115\n",
      "Epoch 177/1200\n",
      "Train loss: 6.3121, Train R²: -124.3176\n",
      "test_loss=5.3563, test_R²=-124.5119\n",
      "Epoch 178/1200\n",
      "Train loss: 6.3355, Train R²: -124.0130\n",
      "test_loss=5.3562, test_R²=-124.5089\n",
      "Epoch 179/1200\n",
      "Train loss: 6.2835, Train R²: -124.9118\n",
      "test_loss=5.3560, test_R²=-124.5058\n",
      "Epoch 180/1200\n",
      "Train loss: 6.3188, Train R²: -125.1638\n",
      "test_loss=5.3561, test_R²=-124.5066\n",
      "Epoch 181/1200\n",
      "Train loss: 6.3134, Train R²: -125.3930\n",
      "test_loss=5.3560, test_R²=-124.5043\n",
      "Epoch 182/1200\n",
      "Train loss: 6.3236, Train R²: -123.8721\n",
      "test_loss=5.3560, test_R²=-124.5040\n",
      "Epoch 183/1200\n",
      "Train loss: 6.2858, Train R²: -124.3965\n",
      "test_loss=5.3560, test_R²=-124.5043\n",
      "Epoch 184/1200\n",
      "Train loss: 6.3200, Train R²: -124.6385\n",
      "test_loss=5.3560, test_R²=-124.5032\n",
      "Epoch 185/1200\n",
      "Train loss: 6.3327, Train R²: -125.3759\n",
      "test_loss=5.3562, test_R²=-124.5073\n",
      "Epoch 186/1200\n",
      "Train loss: 6.3164, Train R²: -124.6029\n",
      "test_loss=5.3564, test_R²=-124.5132\n",
      "Epoch 187/1200\n",
      "Train loss: 6.2835, Train R²: -125.3092\n",
      "test_loss=5.3569, test_R²=-124.5267\n",
      "Epoch 188/1200\n",
      "Train loss: 6.3304, Train R²: -124.5798\n",
      "test_loss=5.3571, test_R²=-124.5325\n",
      "Epoch 189/1200\n",
      "Train loss: 6.3495, Train R²: -124.9338\n",
      "test_loss=5.3573, test_R²=-124.5352\n",
      "Epoch 190/1200\n",
      "Train loss: 6.3276, Train R²: -124.0236\n",
      "test_loss=5.3572, test_R²=-124.5342\n",
      "Epoch 191/1200\n",
      "Train loss: 6.3427, Train R²: -124.0897\n",
      "test_loss=5.3570, test_R²=-124.5281\n",
      "Epoch 192/1200\n",
      "Train loss: 6.3000, Train R²: -124.0001\n",
      "test_loss=5.3568, test_R²=-124.5247\n",
      "Epoch 193/1200\n",
      "Train loss: 6.2764, Train R²: -124.0579\n",
      "test_loss=5.3567, test_R²=-124.5220\n",
      "Epoch 194/1200\n",
      "Train loss: 6.3129, Train R²: -124.0610\n",
      "test_loss=5.3567, test_R²=-124.5206\n",
      "Epoch 195/1200\n",
      "Train loss: 6.3343, Train R²: -124.0428\n",
      "test_loss=5.3566, test_R²=-124.5187\n",
      "Epoch 196/1200\n",
      "Train loss: 6.2988, Train R²: -123.8789\n",
      "test_loss=5.3565, test_R²=-124.5171\n",
      "Epoch 197/1200\n",
      "Train loss: 6.3443, Train R²: -123.7835\n",
      "test_loss=5.3565, test_R²=-124.5175\n",
      "Epoch 198/1200\n",
      "Train loss: 6.3193, Train R²: -123.8403\n",
      "test_loss=5.3565, test_R²=-124.5161\n",
      "Epoch 199/1200\n",
      "Train loss: 6.3281, Train R²: -124.2471\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 200/1200\n",
      "Train loss: 6.3056, Train R²: -124.5669\n",
      "test_loss=5.3564, test_R²=-124.5144\n",
      "Epoch 201/1200\n",
      "Train loss: 6.3039, Train R²: -123.7746\n",
      "test_loss=5.3564, test_R²=-124.5155\n",
      "Epoch 202/1200\n",
      "Train loss: 6.3672, Train R²: -124.7045\n",
      "test_loss=5.3567, test_R²=-124.5216\n",
      "Epoch 203/1200\n",
      "Train loss: 6.2946, Train R²: -124.0838\n",
      "test_loss=5.3570, test_R²=-124.5282\n",
      "Epoch 204/1200\n",
      "Train loss: 6.2890, Train R²: -124.4607\n",
      "test_loss=5.3571, test_R²=-124.5313\n",
      "Epoch 205/1200\n",
      "Train loss: 6.3180, Train R²: -124.7852\n",
      "test_loss=5.3570, test_R²=-124.5298\n",
      "Epoch 206/1200\n",
      "Train loss: 6.2800, Train R²: -124.7997\n",
      "test_loss=5.3567, test_R²=-124.5228\n",
      "Epoch 207/1200\n",
      "Train loss: 6.3387, Train R²: -123.8256\n",
      "test_loss=5.3566, test_R²=-124.5183\n",
      "Epoch 208/1200\n",
      "Train loss: 6.2874, Train R²: -124.6601\n",
      "test_loss=5.3564, test_R²=-124.5143\n",
      "Epoch 209/1200\n",
      "Train loss: 6.3107, Train R²: -124.6697\n",
      "test_loss=5.3565, test_R²=-124.5154\n",
      "Epoch 210/1200\n",
      "Train loss: 6.3615, Train R²: -123.8724\n",
      "test_loss=5.3564, test_R²=-124.5129\n",
      "Epoch 211/1200\n",
      "Train loss: 6.3009, Train R²: -125.2020\n",
      "test_loss=5.3563, test_R²=-124.5115\n",
      "Epoch 212/1200\n",
      "Train loss: 6.3102, Train R²: -124.4328\n",
      "test_loss=5.3562, test_R²=-124.5097\n",
      "Epoch 213/1200\n",
      "Train loss: 6.3105, Train R²: -123.9219\n",
      "test_loss=5.3562, test_R²=-124.5092\n",
      "Epoch 214/1200\n",
      "Train loss: 6.3342, Train R²: -123.7170\n",
      "test_loss=5.3562, test_R²=-124.5086\n",
      "Epoch 215/1200\n",
      "Train loss: 6.2848, Train R²: -126.2805\n",
      "test_loss=5.3562, test_R²=-124.5104\n",
      "Epoch 216/1200\n",
      "Train loss: 6.3300, Train R²: -124.7041\n",
      "test_loss=5.3563, test_R²=-124.5119\n",
      "Epoch 217/1200\n",
      "Train loss: 6.3123, Train R²: -125.2266\n",
      "test_loss=5.3562, test_R²=-124.5121\n",
      "Epoch 218/1200\n",
      "Train loss: 6.2775, Train R²: -123.8602\n",
      "test_loss=5.3562, test_R²=-124.5105\n",
      "Epoch 219/1200\n",
      "Train loss: 6.3433, Train R²: -124.0502\n",
      "test_loss=5.3562, test_R²=-124.5117\n",
      "Epoch 220/1200\n",
      "Train loss: 6.3508, Train R²: -124.5197\n",
      "test_loss=5.3562, test_R²=-124.5111\n",
      "Epoch 221/1200\n",
      "Train loss: 6.3159, Train R²: -124.4207\n",
      "test_loss=5.3563, test_R²=-124.5128\n",
      "Epoch 222/1200\n",
      "Train loss: 6.3315, Train R²: -124.0031\n",
      "test_loss=5.3564, test_R²=-124.5153\n",
      "Epoch 223/1200\n",
      "Train loss: 6.3199, Train R²: -124.6155\n",
      "test_loss=5.3566, test_R²=-124.5200\n",
      "Epoch 224/1200\n",
      "Train loss: 6.2769, Train R²: -123.9646\n",
      "test_loss=5.3567, test_R²=-124.5245\n",
      "Epoch 225/1200\n",
      "Train loss: 6.3326, Train R²: -124.2546\n",
      "test_loss=5.3568, test_R²=-124.5253\n",
      "Epoch 226/1200\n",
      "Train loss: 6.3209, Train R²: -124.4843\n",
      "test_loss=5.3569, test_R²=-124.5270\n",
      "Epoch 227/1200\n",
      "Train loss: 6.3005, Train R²: -124.2711\n",
      "test_loss=5.3569, test_R²=-124.5279\n",
      "Epoch 228/1200\n",
      "Train loss: 6.2816, Train R²: -123.8297\n",
      "test_loss=5.3568, test_R²=-124.5264\n",
      "Epoch 229/1200\n",
      "Train loss: 6.2787, Train R²: -124.6380\n",
      "test_loss=5.3570, test_R²=-124.5312\n",
      "Epoch 230/1200\n",
      "Train loss: 6.3267, Train R²: -124.2979\n",
      "test_loss=5.3573, test_R²=-124.5368\n",
      "Epoch 231/1200\n",
      "Train loss: 6.3062, Train R²: -124.4038\n",
      "test_loss=5.3573, test_R²=-124.5371\n",
      "Epoch 232/1200\n",
      "Train loss: 6.2751, Train R²: -123.7973\n",
      "test_loss=5.3572, test_R²=-124.5360\n",
      "Epoch 233/1200\n",
      "Train loss: 6.2878, Train R²: -124.1325\n",
      "test_loss=5.3573, test_R²=-124.5362\n",
      "Epoch 234/1200\n",
      "Train loss: 6.2850, Train R²: -124.1649\n",
      "test_loss=5.3573, test_R²=-124.5372\n",
      "Epoch 235/1200\n",
      "Train loss: 6.3085, Train R²: -123.9286\n",
      "test_loss=5.3573, test_R²=-124.5380\n",
      "Epoch 236/1200\n",
      "Train loss: 6.3143, Train R²: -124.8097\n",
      "test_loss=5.3571, test_R²=-124.5304\n",
      "Epoch 237/1200\n",
      "Train loss: 6.3054, Train R²: -123.8380\n",
      "test_loss=5.3567, test_R²=-124.5228\n",
      "Epoch 238/1200\n",
      "Train loss: 6.2720, Train R²: -123.7987\n",
      "test_loss=5.3565, test_R²=-124.5180\n",
      "Epoch 239/1200\n",
      "Train loss: 6.3291, Train R²: -124.9422\n",
      "test_loss=5.3564, test_R²=-124.5145\n",
      "Epoch 240/1200\n",
      "Train loss: 6.3209, Train R²: -124.4919\n",
      "test_loss=5.3564, test_R²=-124.5139\n",
      "Epoch 241/1200\n",
      "Train loss: 6.2953, Train R²: -124.1193\n",
      "test_loss=5.3564, test_R²=-124.5142\n",
      "Epoch 242/1200\n",
      "Train loss: 6.2994, Train R²: -124.1940\n",
      "test_loss=5.3563, test_R²=-124.5110\n",
      "Epoch 243/1200\n",
      "Train loss: 6.3361, Train R²: -124.0539\n",
      "test_loss=5.3562, test_R²=-124.5081\n",
      "Epoch 244/1200\n",
      "Train loss: 6.3153, Train R²: -124.2198\n",
      "test_loss=5.3561, test_R²=-124.5052\n",
      "Epoch 245/1200\n",
      "Train loss: 6.3099, Train R²: -125.8146\n",
      "test_loss=5.3560, test_R²=-124.5048\n",
      "Epoch 246/1200\n",
      "Train loss: 6.3248, Train R²: -125.4759\n",
      "test_loss=5.3562, test_R²=-124.5091\n",
      "Epoch 247/1200\n",
      "Train loss: 6.3228, Train R²: -123.8680\n",
      "test_loss=5.3563, test_R²=-124.5111\n",
      "Epoch 248/1200\n",
      "Train loss: 6.2943, Train R²: -124.5614\n",
      "test_loss=5.3564, test_R²=-124.5131\n",
      "Epoch 249/1200\n",
      "Train loss: 6.3400, Train R²: -123.9924\n",
      "test_loss=5.3565, test_R²=-124.5160\n",
      "Epoch 250/1200\n",
      "Train loss: 6.2972, Train R²: -124.0869\n",
      "test_loss=5.3565, test_R²=-124.5164\n",
      "Epoch 251/1200\n",
      "Train loss: 6.3044, Train R²: -124.4265\n",
      "test_loss=5.3566, test_R²=-124.5191\n",
      "Epoch 252/1200\n",
      "Train loss: 6.2809, Train R²: -124.5601\n",
      "test_loss=5.3566, test_R²=-124.5205\n",
      "Epoch 253/1200\n",
      "Train loss: 6.3665, Train R²: -124.0884\n",
      "test_loss=5.3567, test_R²=-124.5217\n",
      "Epoch 254/1200\n",
      "Train loss: 6.2979, Train R²: -124.5212\n",
      "test_loss=5.3567, test_R²=-124.5209\n",
      "Epoch 255/1200\n",
      "Train loss: 6.3125, Train R²: -124.8796\n",
      "test_loss=5.3567, test_R²=-124.5204\n",
      "Epoch 256/1200\n",
      "Train loss: 6.3382, Train R²: -123.8804\n",
      "test_loss=5.3568, test_R²=-124.5230\n",
      "Epoch 257/1200\n",
      "Train loss: 6.2987, Train R²: -124.9421\n",
      "test_loss=5.3568, test_R²=-124.5239\n",
      "Epoch 258/1200\n",
      "Train loss: 6.3333, Train R²: -124.0450\n",
      "test_loss=5.3568, test_R²=-124.5227\n",
      "Epoch 259/1200\n",
      "Train loss: 6.3003, Train R²: -124.7304\n",
      "test_loss=5.3567, test_R²=-124.5212\n",
      "Epoch 260/1200\n",
      "Train loss: 6.2973, Train R²: -124.0925\n",
      "test_loss=5.3567, test_R²=-124.5195\n",
      "Epoch 261/1200\n",
      "Train loss: 6.3052, Train R²: -123.7923\n",
      "test_loss=5.3566, test_R²=-124.5173\n",
      "Epoch 262/1200\n",
      "Train loss: 6.3103, Train R²: -123.9945\n",
      "test_loss=5.3565, test_R²=-124.5142\n",
      "Epoch 263/1200\n",
      "Train loss: 6.2748, Train R²: -124.8132\n",
      "test_loss=5.3563, test_R²=-124.5106\n",
      "Epoch 264/1200\n",
      "Train loss: 6.3246, Train R²: -123.6905\n",
      "test_loss=5.3563, test_R²=-124.5086\n",
      "Epoch 265/1200\n",
      "Train loss: 6.3433, Train R²: -125.6719\n",
      "test_loss=5.3564, test_R²=-124.5116\n",
      "Epoch 266/1200\n",
      "Train loss: 6.3112, Train R²: -124.2597\n",
      "test_loss=5.3566, test_R²=-124.5160\n",
      "Epoch 267/1200\n",
      "Train loss: 6.3154, Train R²: -124.3133\n",
      "test_loss=5.3567, test_R²=-124.5196\n",
      "Epoch 268/1200\n",
      "Train loss: 6.2905, Train R²: -123.8342\n",
      "test_loss=5.3567, test_R²=-124.5196\n",
      "Epoch 269/1200\n",
      "Train loss: 6.2913, Train R²: -124.5264\n",
      "test_loss=5.3565, test_R²=-124.5169\n",
      "Epoch 270/1200\n",
      "Train loss: 6.3307, Train R²: -124.6881\n",
      "test_loss=5.3564, test_R²=-124.5141\n",
      "Epoch 271/1200\n",
      "Train loss: 6.3164, Train R²: -124.3135\n",
      "test_loss=5.3564, test_R²=-124.5125\n",
      "Epoch 272/1200\n",
      "Train loss: 6.3356, Train R²: -124.3778\n",
      "test_loss=5.3563, test_R²=-124.5108\n",
      "Epoch 273/1200\n",
      "Train loss: 6.3120, Train R²: -123.9825\n",
      "test_loss=5.3563, test_R²=-124.5115\n",
      "Epoch 274/1200\n",
      "Train loss: 6.3193, Train R²: -123.9963\n",
      "test_loss=5.3565, test_R²=-124.5147\n",
      "Epoch 275/1200\n",
      "Train loss: 6.2887, Train R²: -124.1726\n",
      "test_loss=5.3566, test_R²=-124.5178\n",
      "Epoch 276/1200\n",
      "Train loss: 6.3463, Train R²: -124.9969\n",
      "test_loss=5.3568, test_R²=-124.5229\n",
      "Epoch 277/1200\n",
      "Train loss: 6.3034, Train R²: -124.0155\n",
      "test_loss=5.3569, test_R²=-124.5249\n",
      "Epoch 278/1200\n",
      "Train loss: 6.3501, Train R²: -123.8728\n",
      "test_loss=5.3568, test_R²=-124.5235\n",
      "Epoch 279/1200\n",
      "Train loss: 6.2955, Train R²: -124.4673\n",
      "test_loss=5.3568, test_R²=-124.5239\n",
      "Epoch 280/1200\n",
      "Train loss: 6.2932, Train R²: -124.1044\n",
      "test_loss=5.3568, test_R²=-124.5243\n",
      "Epoch 281/1200\n",
      "Train loss: 6.3141, Train R²: -124.4029\n",
      "test_loss=5.3569, test_R²=-124.5281\n",
      "Epoch 282/1200\n",
      "Train loss: 6.3625, Train R²: -124.0018\n",
      "test_loss=5.3570, test_R²=-124.5296\n",
      "Epoch 283/1200\n",
      "Train loss: 6.3561, Train R²: -124.6102\n",
      "test_loss=5.3571, test_R²=-124.5313\n",
      "Epoch 284/1200\n",
      "Train loss: 6.3516, Train R²: -123.7823\n",
      "test_loss=5.3571, test_R²=-124.5326\n",
      "Epoch 285/1200\n",
      "Train loss: 6.3365, Train R²: -124.3354\n",
      "test_loss=5.3572, test_R²=-124.5342\n",
      "Epoch 286/1200\n",
      "Train loss: 6.3135, Train R²: -124.8060\n",
      "test_loss=5.3571, test_R²=-124.5332\n",
      "Epoch 287/1200\n",
      "Train loss: 6.3149, Train R²: -125.4613\n",
      "test_loss=5.3571, test_R²=-124.5311\n",
      "Epoch 288/1200\n",
      "Train loss: 6.2833, Train R²: -124.2030\n",
      "test_loss=5.3569, test_R²=-124.5280\n",
      "Epoch 289/1200\n",
      "Train loss: 6.3494, Train R²: -124.8307\n",
      "test_loss=5.3569, test_R²=-124.5254\n",
      "Epoch 290/1200\n",
      "Train loss: 6.3401, Train R²: -123.5872\n",
      "test_loss=5.3568, test_R²=-124.5234\n",
      "Epoch 291/1200\n",
      "Train loss: 6.3157, Train R²: -124.8961\n",
      "test_loss=5.3567, test_R²=-124.5202\n",
      "Epoch 292/1200\n",
      "Train loss: 6.2981, Train R²: -123.8955\n",
      "test_loss=5.3566, test_R²=-124.5187\n",
      "Epoch 293/1200\n",
      "Train loss: 6.2927, Train R²: -124.1604\n",
      "test_loss=5.3566, test_R²=-124.5174\n",
      "Epoch 294/1200\n",
      "Train loss: 6.3082, Train R²: -124.2422\n",
      "test_loss=5.3564, test_R²=-124.5146\n",
      "Epoch 295/1200\n",
      "Train loss: 6.3335, Train R²: -124.1158\n",
      "test_loss=5.3563, test_R²=-124.5109\n",
      "Epoch 296/1200\n",
      "Train loss: 6.3161, Train R²: -123.5870\n",
      "test_loss=5.3562, test_R²=-124.5083\n",
      "Epoch 297/1200\n",
      "Train loss: 6.3342, Train R²: -123.9563\n",
      "test_loss=5.3561, test_R²=-124.5060\n",
      "Epoch 298/1200\n",
      "Train loss: 6.2733, Train R²: -123.6329\n",
      "test_loss=5.3560, test_R²=-124.5037\n",
      "Epoch 299/1200\n",
      "Train loss: 6.3227, Train R²: -123.6630\n",
      "test_loss=5.3559, test_R²=-124.5017\n",
      "Epoch 300/1200\n",
      "Train loss: 6.3426, Train R²: -124.5123\n",
      "test_loss=5.3558, test_R²=-124.4989\n",
      "Epoch 301/1200\n",
      "Train loss: 6.3052, Train R²: -124.5575\n",
      "test_loss=5.3557, test_R²=-124.4964\n",
      "Epoch 302/1200\n",
      "Train loss: 6.3394, Train R²: -124.3601\n",
      "test_loss=5.3556, test_R²=-124.4932\n",
      "Epoch 303/1200\n",
      "Train loss: 6.3332, Train R²: -124.2580\n",
      "test_loss=5.3554, test_R²=-124.4885\n",
      "Epoch 304/1200\n",
      "Train loss: 6.3421, Train R²: -125.5750\n",
      "test_loss=5.3554, test_R²=-124.4889\n",
      "Epoch 305/1200\n",
      "Train loss: 6.3224, Train R²: -125.2103\n",
      "test_loss=5.3558, test_R²=-124.4979\n",
      "Epoch 306/1200\n",
      "Train loss: 6.3163, Train R²: -124.2697\n",
      "test_loss=5.3562, test_R²=-124.5075\n",
      "Epoch 307/1200\n",
      "Train loss: 6.3204, Train R²: -123.8280\n",
      "test_loss=5.3563, test_R²=-124.5105\n",
      "Epoch 308/1200\n",
      "Train loss: 6.3019, Train R²: -124.9209\n",
      "test_loss=5.3563, test_R²=-124.5107\n",
      "Epoch 309/1200\n",
      "Train loss: 6.3044, Train R²: -124.6516\n",
      "test_loss=5.3563, test_R²=-124.5101\n",
      "Epoch 310/1200\n",
      "Train loss: 6.3478, Train R²: -124.1161\n",
      "test_loss=5.3562, test_R²=-124.5093\n",
      "Epoch 311/1200\n",
      "Train loss: 6.3503, Train R²: -125.0293\n",
      "test_loss=5.3562, test_R²=-124.5090\n",
      "Epoch 312/1200\n",
      "Train loss: 6.3332, Train R²: -123.8924\n",
      "test_loss=5.3562, test_R²=-124.5088\n",
      "Epoch 313/1200\n",
      "Train loss: 6.2814, Train R²: -124.2593\n",
      "test_loss=5.3562, test_R²=-124.5105\n",
      "Epoch 314/1200\n",
      "Train loss: 6.3001, Train R²: -124.5712\n",
      "test_loss=5.3562, test_R²=-124.5111\n",
      "Epoch 315/1200\n",
      "Train loss: 6.2985, Train R²: -124.5593\n",
      "test_loss=5.3563, test_R²=-124.5132\n",
      "Epoch 316/1200\n",
      "Train loss: 6.2787, Train R²: -124.2045\n",
      "test_loss=5.3563, test_R²=-124.5130\n",
      "Epoch 317/1200\n",
      "Train loss: 6.3195, Train R²: -125.3830\n",
      "test_loss=5.3565, test_R²=-124.5163\n",
      "Epoch 318/1200\n",
      "Train loss: 6.3099, Train R²: -123.8226\n",
      "test_loss=5.3565, test_R²=-124.5180\n",
      "Epoch 319/1200\n",
      "Train loss: 6.3205, Train R²: -124.2048\n",
      "test_loss=5.3566, test_R²=-124.5186\n",
      "Epoch 320/1200\n",
      "Train loss: 6.3313, Train R²: -124.4399\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 321/1200\n",
      "Train loss: 6.3456, Train R²: -123.7242\n",
      "test_loss=5.3566, test_R²=-124.5184\n",
      "Epoch 322/1200\n",
      "Train loss: 6.2984, Train R²: -124.0986\n",
      "test_loss=5.3565, test_R²=-124.5165\n",
      "Epoch 323/1200\n",
      "Train loss: 6.3027, Train R²: -124.0616\n",
      "test_loss=5.3564, test_R²=-124.5152\n",
      "Epoch 324/1200\n",
      "Train loss: 6.3322, Train R²: -126.6133\n",
      "test_loss=5.3565, test_R²=-124.5165\n",
      "Epoch 325/1200\n",
      "Train loss: 6.2872, Train R²: -124.4099\n",
      "test_loss=5.3567, test_R²=-124.5210\n",
      "Epoch 326/1200\n",
      "Train loss: 6.2839, Train R²: -124.3356\n",
      "test_loss=5.3568, test_R²=-124.5232\n",
      "Epoch 327/1200\n",
      "Train loss: 6.2843, Train R²: -124.6395\n",
      "test_loss=5.3568, test_R²=-124.5237\n",
      "Epoch 328/1200\n",
      "Train loss: 6.3027, Train R²: -124.6761\n",
      "test_loss=5.3569, test_R²=-124.5256\n",
      "Epoch 329/1200\n",
      "Train loss: 6.2958, Train R²: -124.1878\n",
      "test_loss=5.3569, test_R²=-124.5255\n",
      "Epoch 330/1200\n",
      "Train loss: 6.2979, Train R²: -125.0574\n",
      "test_loss=5.3569, test_R²=-124.5268\n",
      "Epoch 331/1200\n",
      "Train loss: 6.3531, Train R²: -124.2642\n",
      "test_loss=5.3569, test_R²=-124.5268\n",
      "Epoch 332/1200\n",
      "Train loss: 6.2812, Train R²: -124.4170\n",
      "test_loss=5.3569, test_R²=-124.5261\n",
      "Epoch 333/1200\n",
      "Train loss: 6.3133, Train R²: -123.9748\n",
      "test_loss=5.3567, test_R²=-124.5219\n",
      "Epoch 334/1200\n",
      "Train loss: 6.3020, Train R²: -124.7411\n",
      "test_loss=5.3567, test_R²=-124.5211\n",
      "Epoch 335/1200\n",
      "Train loss: 6.3197, Train R²: -125.3972\n",
      "test_loss=5.3569, test_R²=-124.5253\n",
      "Epoch 336/1200\n",
      "Train loss: 6.2992, Train R²: -125.0494\n",
      "test_loss=5.3568, test_R²=-124.5233\n",
      "Epoch 337/1200\n",
      "Train loss: 6.2870, Train R²: -124.5214\n",
      "test_loss=5.3568, test_R²=-124.5231\n",
      "Epoch 338/1200\n",
      "Train loss: 6.3060, Train R²: -124.0244\n",
      "test_loss=5.3568, test_R²=-124.5220\n",
      "Epoch 339/1200\n",
      "Train loss: 6.2959, Train R²: -125.5969\n",
      "test_loss=5.3567, test_R²=-124.5209\n",
      "Epoch 340/1200\n",
      "Train loss: 6.3376, Train R²: -124.4200\n",
      "test_loss=5.3567, test_R²=-124.5191\n",
      "Epoch 341/1200\n",
      "Train loss: 6.2994, Train R²: -124.1986\n",
      "test_loss=5.3566, test_R²=-124.5175\n",
      "Epoch 342/1200\n",
      "Train loss: 6.3274, Train R²: -124.1740\n",
      "test_loss=5.3566, test_R²=-124.5160\n",
      "Epoch 343/1200\n",
      "Train loss: 6.3174, Train R²: -124.4551\n",
      "test_loss=5.3566, test_R²=-124.5185\n",
      "Epoch 344/1200\n",
      "Train loss: 6.2917, Train R²: -124.5974\n",
      "test_loss=5.3566, test_R²=-124.5186\n",
      "Epoch 345/1200\n",
      "Train loss: 6.3097, Train R²: -124.3042\n",
      "test_loss=5.3566, test_R²=-124.5185\n",
      "Epoch 346/1200\n",
      "Train loss: 6.3328, Train R²: -123.6943\n",
      "test_loss=5.3566, test_R²=-124.5171\n",
      "Epoch 347/1200\n",
      "Train loss: 6.3189, Train R²: -125.1826\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 348/1200\n",
      "Train loss: 6.3251, Train R²: -124.2459\n",
      "test_loss=5.3568, test_R²=-124.5239\n",
      "Epoch 349/1200\n",
      "Train loss: 6.3235, Train R²: -123.8635\n",
      "test_loss=5.3569, test_R²=-124.5265\n",
      "Epoch 350/1200\n",
      "Train loss: 6.3416, Train R²: -124.7364\n",
      "test_loss=5.3569, test_R²=-124.5266\n",
      "Epoch 351/1200\n",
      "Train loss: 6.3065, Train R²: -124.4984\n",
      "test_loss=5.3568, test_R²=-124.5230\n",
      "Epoch 352/1200\n",
      "Train loss: 6.3372, Train R²: -123.9118\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 353/1200\n",
      "Train loss: 6.3506, Train R²: -123.7251\n",
      "test_loss=5.3565, test_R²=-124.5155\n",
      "Epoch 354/1200\n",
      "Train loss: 6.3316, Train R²: -124.0734\n",
      "test_loss=5.3563, test_R²=-124.5124\n",
      "Epoch 355/1200\n",
      "Train loss: 6.3175, Train R²: -124.3562\n",
      "test_loss=5.3562, test_R²=-124.5085\n",
      "Epoch 356/1200\n",
      "Train loss: 6.3423, Train R²: -125.0101\n",
      "test_loss=5.3561, test_R²=-124.5051\n",
      "Epoch 357/1200\n",
      "Train loss: 6.3339, Train R²: -123.8475\n",
      "test_loss=5.3560, test_R²=-124.5027\n",
      "Epoch 358/1200\n",
      "Train loss: 6.3183, Train R²: -124.9128\n",
      "test_loss=5.3560, test_R²=-124.5023\n",
      "Epoch 359/1200\n",
      "Train loss: 6.3243, Train R²: -125.1615\n",
      "test_loss=5.3561, test_R²=-124.5043\n",
      "Epoch 360/1200\n",
      "Train loss: 6.3208, Train R²: -124.8082\n",
      "test_loss=5.3561, test_R²=-124.5053\n",
      "Epoch 361/1200\n",
      "Train loss: 6.3000, Train R²: -124.7754\n",
      "test_loss=5.3563, test_R²=-124.5113\n",
      "Epoch 362/1200\n",
      "Train loss: 6.3179, Train R²: -124.6936\n",
      "test_loss=5.3565, test_R²=-124.5166\n",
      "Epoch 363/1200\n",
      "Train loss: 6.3168, Train R²: -124.3135\n",
      "test_loss=5.3566, test_R²=-124.5179\n",
      "Epoch 364/1200\n",
      "Train loss: 6.3202, Train R²: -124.3275\n",
      "test_loss=5.3565, test_R²=-124.5163\n",
      "Epoch 365/1200\n",
      "Train loss: 6.2949, Train R²: -123.9810\n",
      "test_loss=5.3565, test_R²=-124.5154\n",
      "Epoch 366/1200\n",
      "Train loss: 6.2852, Train R²: -123.7559\n",
      "test_loss=5.3565, test_R²=-124.5157\n",
      "Epoch 367/1200\n",
      "Train loss: 6.2779, Train R²: -124.1596\n",
      "test_loss=5.3565, test_R²=-124.5148\n",
      "Epoch 368/1200\n",
      "Train loss: 6.2732, Train R²: -123.9551\n",
      "test_loss=5.3564, test_R²=-124.5143\n",
      "Epoch 369/1200\n",
      "Train loss: 6.3180, Train R²: -124.4812\n",
      "test_loss=5.3564, test_R²=-124.5136\n",
      "Epoch 370/1200\n",
      "Train loss: 6.2995, Train R²: -124.8780\n",
      "test_loss=5.3565, test_R²=-124.5152\n",
      "Epoch 371/1200\n",
      "Train loss: 6.2957, Train R²: -124.3124\n",
      "test_loss=5.3564, test_R²=-124.5137\n",
      "Epoch 372/1200\n",
      "Train loss: 6.2845, Train R²: -123.8263\n",
      "test_loss=5.3563, test_R²=-124.5121\n",
      "Epoch 373/1200\n",
      "Train loss: 6.3007, Train R²: -123.7421\n",
      "test_loss=5.3564, test_R²=-124.5124\n",
      "Epoch 374/1200\n",
      "Train loss: 6.3430, Train R²: -124.5832\n",
      "test_loss=5.3564, test_R²=-124.5137\n",
      "Epoch 375/1200\n",
      "Train loss: 6.3550, Train R²: -124.5294\n",
      "test_loss=5.3564, test_R²=-124.5126\n",
      "Epoch 376/1200\n",
      "Train loss: 6.2879, Train R²: -124.2966\n",
      "test_loss=5.3563, test_R²=-124.5108\n",
      "Epoch 377/1200\n",
      "Train loss: 6.2987, Train R²: -123.8177\n",
      "test_loss=5.3560, test_R²=-124.5049\n",
      "Epoch 378/1200\n",
      "Train loss: 6.3447, Train R²: -124.8507\n",
      "test_loss=5.3561, test_R²=-124.5069\n",
      "Epoch 379/1200\n",
      "Train loss: 6.3045, Train R²: -125.5809\n",
      "test_loss=5.3565, test_R²=-124.5153\n",
      "Epoch 380/1200\n",
      "Train loss: 6.2928, Train R²: -123.7802\n",
      "test_loss=5.3567, test_R²=-124.5211\n",
      "Epoch 381/1200\n",
      "Train loss: 6.3167, Train R²: -123.9069\n",
      "test_loss=5.3568, test_R²=-124.5238\n",
      "Epoch 382/1200\n",
      "Train loss: 6.3030, Train R²: -123.8180\n",
      "test_loss=5.3568, test_R²=-124.5239\n",
      "Epoch 383/1200\n",
      "Train loss: 6.2914, Train R²: -124.7026\n",
      "test_loss=5.3567, test_R²=-124.5204\n",
      "Epoch 384/1200\n",
      "Train loss: 6.3124, Train R²: -123.9631\n",
      "test_loss=5.3565, test_R²=-124.5175\n",
      "Epoch 385/1200\n",
      "Train loss: 6.3147, Train R²: -124.0918\n",
      "test_loss=5.3564, test_R²=-124.5146\n",
      "Epoch 386/1200\n",
      "Train loss: 6.2717, Train R²: -124.1429\n",
      "test_loss=5.3564, test_R²=-124.5135\n",
      "Epoch 387/1200\n",
      "Train loss: 6.3090, Train R²: -123.8848\n",
      "test_loss=5.3564, test_R²=-124.5144\n",
      "Epoch 388/1200\n",
      "Train loss: 6.3354, Train R²: -124.3528\n",
      "test_loss=5.3564, test_R²=-124.5146\n",
      "Epoch 389/1200\n",
      "Train loss: 6.2870, Train R²: -124.4240\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 390/1200\n",
      "Train loss: 6.2973, Train R²: -125.0349\n",
      "test_loss=5.3567, test_R²=-124.5220\n",
      "Epoch 391/1200\n",
      "Train loss: 6.2854, Train R²: -124.1091\n",
      "test_loss=5.3568, test_R²=-124.5247\n",
      "Epoch 392/1200\n",
      "Train loss: 6.3482, Train R²: -123.8681\n",
      "test_loss=5.3567, test_R²=-124.5220\n",
      "Epoch 393/1200\n",
      "Train loss: 6.2887, Train R²: -124.1953\n",
      "test_loss=5.3565, test_R²=-124.5183\n",
      "Epoch 394/1200\n",
      "Train loss: 6.2945, Train R²: -123.7991\n",
      "test_loss=5.3565, test_R²=-124.5166\n",
      "Epoch 395/1200\n",
      "Train loss: 6.3074, Train R²: -124.0699\n",
      "test_loss=5.3564, test_R²=-124.5151\n",
      "Epoch 396/1200\n",
      "Train loss: 6.3127, Train R²: -123.7605\n",
      "test_loss=5.3563, test_R²=-124.5127\n",
      "Epoch 397/1200\n",
      "Train loss: 6.3026, Train R²: -126.8628\n",
      "test_loss=5.3562, test_R²=-124.5105\n",
      "Epoch 398/1200\n",
      "Train loss: 6.3294, Train R²: -125.3728\n",
      "test_loss=5.3561, test_R²=-124.5072\n",
      "Epoch 399/1200\n",
      "Train loss: 6.3192, Train R²: -124.6862\n",
      "test_loss=5.3560, test_R²=-124.5063\n",
      "Epoch 400/1200\n",
      "Train loss: 6.3220, Train R²: -125.6138\n",
      "test_loss=5.3561, test_R²=-124.5067\n",
      "Epoch 401/1200\n",
      "Train loss: 6.3188, Train R²: -124.7934\n",
      "test_loss=5.3561, test_R²=-124.5076\n",
      "Epoch 402/1200\n",
      "Train loss: 6.3243, Train R²: -124.5285\n",
      "test_loss=5.3562, test_R²=-124.5102\n",
      "Epoch 403/1200\n",
      "Train loss: 6.3515, Train R²: -124.0944\n",
      "test_loss=5.3562, test_R²=-124.5092\n",
      "Epoch 404/1200\n",
      "Train loss: 6.3282, Train R²: -125.2605\n",
      "test_loss=5.3562, test_R²=-124.5085\n",
      "Epoch 405/1200\n",
      "Train loss: 6.3158, Train R²: -125.7652\n",
      "test_loss=5.3564, test_R²=-124.5148\n",
      "Epoch 406/1200\n",
      "Train loss: 6.3172, Train R²: -124.0949\n",
      "test_loss=5.3566, test_R²=-124.5192\n",
      "Epoch 407/1200\n",
      "Train loss: 6.3099, Train R²: -124.6446\n",
      "test_loss=5.3566, test_R²=-124.5182\n",
      "Epoch 408/1200\n",
      "Train loss: 6.2872, Train R²: -123.8333\n",
      "test_loss=5.3565, test_R²=-124.5159\n",
      "Epoch 409/1200\n",
      "Train loss: 6.3394, Train R²: -125.1468\n",
      "test_loss=5.3564, test_R²=-124.5140\n",
      "Epoch 410/1200\n",
      "Train loss: 6.3276, Train R²: -124.1587\n",
      "test_loss=5.3564, test_R²=-124.5150\n",
      "Epoch 411/1200\n",
      "Train loss: 6.3195, Train R²: -123.8584\n",
      "test_loss=5.3564, test_R²=-124.5153\n",
      "Epoch 412/1200\n",
      "Train loss: 6.3185, Train R²: -124.7360\n",
      "test_loss=5.3564, test_R²=-124.5152\n",
      "Epoch 413/1200\n",
      "Train loss: 6.2810, Train R²: -124.6433\n",
      "test_loss=5.3562, test_R²=-124.5105\n",
      "Epoch 414/1200\n",
      "Train loss: 6.3326, Train R²: -124.1475\n",
      "test_loss=5.3561, test_R²=-124.5062\n",
      "Epoch 415/1200\n",
      "Train loss: 6.3280, Train R²: -124.0280\n",
      "test_loss=5.3559, test_R²=-124.5018\n",
      "Epoch 416/1200\n",
      "Train loss: 6.3058, Train R²: -124.4609\n",
      "test_loss=5.3559, test_R²=-124.5031\n",
      "Epoch 417/1200\n",
      "Train loss: 6.3462, Train R²: -123.9194\n",
      "test_loss=5.3560, test_R²=-124.5043\n",
      "Epoch 418/1200\n",
      "Train loss: 6.3154, Train R²: -125.2048\n",
      "test_loss=5.3561, test_R²=-124.5065\n",
      "Epoch 419/1200\n",
      "Train loss: 6.2921, Train R²: -124.2056\n",
      "test_loss=5.3562, test_R²=-124.5096\n",
      "Epoch 420/1200\n",
      "Train loss: 6.2926, Train R²: -123.9554\n",
      "test_loss=5.3562, test_R²=-124.5110\n",
      "Epoch 421/1200\n",
      "Train loss: 6.2989, Train R²: -124.2855\n",
      "test_loss=5.3563, test_R²=-124.5138\n",
      "Epoch 422/1200\n",
      "Train loss: 6.3210, Train R²: -125.2757\n",
      "test_loss=5.3564, test_R²=-124.5159\n",
      "Epoch 423/1200\n",
      "Train loss: 6.3180, Train R²: -125.3101\n",
      "test_loss=5.3568, test_R²=-124.5244\n",
      "Epoch 424/1200\n",
      "Train loss: 6.3002, Train R²: -124.3063\n",
      "test_loss=5.3569, test_R²=-124.5276\n",
      "Epoch 425/1200\n",
      "Train loss: 6.3044, Train R²: -124.6484\n",
      "test_loss=5.3570, test_R²=-124.5288\n",
      "Epoch 426/1200\n",
      "Train loss: 6.3445, Train R²: -124.3001\n",
      "test_loss=5.3569, test_R²=-124.5278\n",
      "Epoch 427/1200\n",
      "Train loss: 6.2996, Train R²: -124.0994\n",
      "test_loss=5.3568, test_R²=-124.5249\n",
      "Epoch 428/1200\n",
      "Train loss: 6.3112, Train R²: -124.1132\n",
      "test_loss=5.3568, test_R²=-124.5235\n",
      "Epoch 429/1200\n",
      "Train loss: 6.3198, Train R²: -123.9286\n",
      "test_loss=5.3568, test_R²=-124.5232\n",
      "Epoch 430/1200\n",
      "Train loss: 6.3092, Train R²: -125.2564\n",
      "test_loss=5.3566, test_R²=-124.5192\n",
      "Epoch 431/1200\n",
      "Train loss: 6.3103, Train R²: -123.7425\n",
      "test_loss=5.3566, test_R²=-124.5183\n",
      "Epoch 432/1200\n",
      "Train loss: 6.3098, Train R²: -124.8480\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 433/1200\n",
      "Train loss: 6.2875, Train R²: -124.2036\n",
      "test_loss=5.3565, test_R²=-124.5180\n",
      "Epoch 434/1200\n",
      "Train loss: 6.3091, Train R²: -123.9221\n",
      "test_loss=5.3565, test_R²=-124.5160\n",
      "Epoch 435/1200\n",
      "Train loss: 6.3025, Train R²: -124.3880\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 436/1200\n",
      "Train loss: 6.3109, Train R²: -124.0940\n",
      "test_loss=5.3564, test_R²=-124.5132\n",
      "Epoch 437/1200\n",
      "Train loss: 6.3600, Train R²: -124.5087\n",
      "test_loss=5.3564, test_R²=-124.5133\n",
      "Epoch 438/1200\n",
      "Train loss: 6.3105, Train R²: -125.1866\n",
      "test_loss=5.3563, test_R²=-124.5111\n",
      "Epoch 439/1200\n",
      "Train loss: 6.3046, Train R²: -124.2564\n",
      "test_loss=5.3561, test_R²=-124.5077\n",
      "Epoch 440/1200\n",
      "Train loss: 6.3430, Train R²: -124.3078\n",
      "test_loss=5.3560, test_R²=-124.5061\n",
      "Epoch 441/1200\n",
      "Train loss: 6.2930, Train R²: -123.8576\n",
      "test_loss=5.3561, test_R²=-124.5073\n",
      "Epoch 442/1200\n",
      "Train loss: 6.2838, Train R²: -124.3774\n",
      "test_loss=5.3562, test_R²=-124.5100\n",
      "Epoch 443/1200\n",
      "Train loss: 6.3023, Train R²: -124.2246\n",
      "test_loss=5.3561, test_R²=-124.5094\n",
      "Epoch 444/1200\n",
      "Train loss: 6.2876, Train R²: -125.0012\n",
      "test_loss=5.3561, test_R²=-124.5096\n",
      "Epoch 445/1200\n",
      "Train loss: 6.3200, Train R²: -124.6877\n",
      "test_loss=5.3563, test_R²=-124.5125\n",
      "Epoch 446/1200\n",
      "Train loss: 6.3220, Train R²: -123.8027\n",
      "test_loss=5.3563, test_R²=-124.5136\n",
      "Epoch 447/1200\n",
      "Train loss: 6.2897, Train R²: -124.3587\n",
      "test_loss=5.3563, test_R²=-124.5122\n",
      "Epoch 448/1200\n",
      "Train loss: 6.3025, Train R²: -124.4758\n",
      "test_loss=5.3563, test_R²=-124.5126\n",
      "Epoch 449/1200\n",
      "Train loss: 6.3070, Train R²: -124.7886\n",
      "test_loss=5.3563, test_R²=-124.5128\n",
      "Epoch 450/1200\n",
      "Train loss: 6.2915, Train R²: -123.8939\n",
      "test_loss=5.3564, test_R²=-124.5155\n",
      "Epoch 451/1200\n",
      "Train loss: 6.3348, Train R²: -125.4149\n",
      "test_loss=5.3565, test_R²=-124.5169\n",
      "Epoch 452/1200\n",
      "Train loss: 6.3174, Train R²: -124.4131\n",
      "test_loss=5.3565, test_R²=-124.5168\n",
      "Epoch 453/1200\n",
      "Train loss: 6.3075, Train R²: -125.4293\n",
      "test_loss=5.3566, test_R²=-124.5183\n",
      "Epoch 454/1200\n",
      "Train loss: 6.3382, Train R²: -124.9421\n",
      "test_loss=5.3565, test_R²=-124.5167\n",
      "Epoch 455/1200\n",
      "Train loss: 6.2802, Train R²: -125.5132\n",
      "test_loss=5.3563, test_R²=-124.5123\n",
      "Epoch 456/1200\n",
      "Train loss: 6.3383, Train R²: -124.3887\n",
      "test_loss=5.3563, test_R²=-124.5105\n",
      "Epoch 457/1200\n",
      "Train loss: 6.3333, Train R²: -124.4392\n",
      "test_loss=5.3561, test_R²=-124.5079\n",
      "Epoch 458/1200\n",
      "Train loss: 6.3540, Train R²: -123.9945\n",
      "test_loss=5.3562, test_R²=-124.5099\n",
      "Epoch 459/1200\n",
      "Train loss: 6.3392, Train R²: -124.0590\n",
      "test_loss=5.3562, test_R²=-124.5101\n",
      "Epoch 460/1200\n",
      "Train loss: 6.3018, Train R²: -124.9554\n",
      "test_loss=5.3562, test_R²=-124.5097\n",
      "Epoch 461/1200\n",
      "Train loss: 6.2669, Train R²: -125.1244\n",
      "test_loss=5.3562, test_R²=-124.5097\n",
      "Epoch 462/1200\n",
      "Train loss: 6.3359, Train R²: -124.1627\n",
      "test_loss=5.3561, test_R²=-124.5084\n",
      "Epoch 463/1200\n",
      "Train loss: 6.3723, Train R²: -124.5116\n",
      "test_loss=5.3561, test_R²=-124.5082\n",
      "Epoch 464/1200\n",
      "Train loss: 6.3024, Train R²: -123.7923\n",
      "test_loss=5.3562, test_R²=-124.5104\n",
      "Epoch 465/1200\n",
      "Train loss: 6.2852, Train R²: -124.4861\n",
      "test_loss=5.3563, test_R²=-124.5132\n",
      "Epoch 466/1200\n",
      "Train loss: 6.2864, Train R²: -124.4797\n",
      "test_loss=5.3564, test_R²=-124.5142\n",
      "Epoch 467/1200\n",
      "Train loss: 6.2932, Train R²: -124.2238\n",
      "test_loss=5.3566, test_R²=-124.5190\n",
      "Epoch 468/1200\n",
      "Train loss: 6.3179, Train R²: -125.2555\n",
      "test_loss=5.3567, test_R²=-124.5210\n",
      "Epoch 469/1200\n",
      "Train loss: 6.3037, Train R²: -125.2601\n",
      "test_loss=5.3567, test_R²=-124.5214\n",
      "Epoch 470/1200\n",
      "Train loss: 6.2924, Train R²: -124.6072\n",
      "test_loss=5.3567, test_R²=-124.5241\n",
      "Epoch 471/1200\n",
      "Train loss: 6.3379, Train R²: -123.7985\n",
      "test_loss=5.3568, test_R²=-124.5264\n",
      "Epoch 472/1200\n",
      "Train loss: 6.3029, Train R²: -123.8178\n",
      "test_loss=5.3568, test_R²=-124.5265\n",
      "Epoch 473/1200\n",
      "Train loss: 6.2837, Train R²: -124.1406\n",
      "test_loss=5.3568, test_R²=-124.5249\n",
      "Epoch 474/1200\n",
      "Train loss: 6.3106, Train R²: -123.7236\n",
      "test_loss=5.3567, test_R²=-124.5238\n",
      "Epoch 475/1200\n",
      "Train loss: 6.2789, Train R²: -124.2414\n",
      "test_loss=5.3567, test_R²=-124.5236\n",
      "Epoch 476/1200\n",
      "Train loss: 6.3244, Train R²: -124.4114\n",
      "test_loss=5.3567, test_R²=-124.5222\n",
      "Epoch 477/1200\n",
      "Train loss: 6.2905, Train R²: -124.9802\n",
      "test_loss=5.3565, test_R²=-124.5181\n",
      "Epoch 478/1200\n",
      "Train loss: 6.3142, Train R²: -124.6937\n",
      "test_loss=5.3563, test_R²=-124.5139\n",
      "Epoch 479/1200\n",
      "Train loss: 6.3100, Train R²: -124.0893\n",
      "test_loss=5.3563, test_R²=-124.5133\n",
      "Epoch 480/1200\n",
      "Train loss: 6.3213, Train R²: -124.4434\n",
      "test_loss=5.3563, test_R²=-124.5132\n",
      "Epoch 481/1200\n",
      "Train loss: 6.2699, Train R²: -123.7442\n",
      "test_loss=5.3563, test_R²=-124.5132\n",
      "Epoch 482/1200\n",
      "Train loss: 6.3101, Train R²: -124.0109\n",
      "test_loss=5.3563, test_R²=-124.5124\n",
      "Epoch 483/1200\n",
      "Train loss: 6.3112, Train R²: -123.8389\n",
      "test_loss=5.3563, test_R²=-124.5133\n",
      "Epoch 484/1200\n",
      "Train loss: 6.3010, Train R²: -124.2362\n",
      "test_loss=5.3563, test_R²=-124.5139\n",
      "Epoch 485/1200\n",
      "Train loss: 6.3268, Train R²: -125.2837\n",
      "test_loss=5.3563, test_R²=-124.5128\n",
      "Epoch 486/1200\n",
      "Train loss: 6.2935, Train R²: -124.3055\n",
      "test_loss=5.3563, test_R²=-124.5133\n",
      "Epoch 487/1200\n",
      "Train loss: 6.2871, Train R²: -125.6962\n",
      "test_loss=5.3563, test_R²=-124.5120\n",
      "Epoch 488/1200\n",
      "Train loss: 6.2948, Train R²: -124.4838\n",
      "test_loss=5.3563, test_R²=-124.5141\n",
      "Epoch 489/1200\n",
      "Train loss: 6.3552, Train R²: -124.2481\n",
      "test_loss=5.3564, test_R²=-124.5166\n",
      "Epoch 490/1200\n",
      "Train loss: 6.2611, Train R²: -124.9859\n",
      "test_loss=5.3565, test_R²=-124.5176\n",
      "Epoch 491/1200\n",
      "Train loss: 6.3235, Train R²: -124.0880\n",
      "test_loss=5.3565, test_R²=-124.5182\n",
      "Epoch 492/1200\n",
      "Train loss: 6.3183, Train R²: -124.3794\n",
      "test_loss=5.3565, test_R²=-124.5181\n",
      "Epoch 493/1200\n",
      "Train loss: 6.2650, Train R²: -124.3720\n",
      "test_loss=5.3566, test_R²=-124.5206\n",
      "Epoch 494/1200\n",
      "Train loss: 6.3488, Train R²: -124.1184\n",
      "test_loss=5.3567, test_R²=-124.5229\n",
      "Epoch 495/1200\n",
      "Train loss: 6.3790, Train R²: -124.0871\n",
      "test_loss=5.3567, test_R²=-124.5237\n",
      "Epoch 496/1200\n",
      "Train loss: 6.3313, Train R²: -125.0010\n",
      "test_loss=5.3569, test_R²=-124.5271\n",
      "Epoch 497/1200\n",
      "Train loss: 6.3008, Train R²: -123.9811\n",
      "test_loss=5.3570, test_R²=-124.5297\n",
      "Epoch 498/1200\n",
      "Train loss: 6.3385, Train R²: -123.8353\n",
      "test_loss=5.3569, test_R²=-124.5286\n",
      "Epoch 499/1200\n",
      "Train loss: 6.3106, Train R²: -123.8420\n",
      "test_loss=5.3568, test_R²=-124.5248\n",
      "Epoch 500/1200\n",
      "Train loss: 6.3136, Train R²: -124.5204\n",
      "test_loss=5.3566, test_R²=-124.5209\n",
      "Epoch 501/1200\n",
      "Train loss: 6.3027, Train R²: -124.4500\n",
      "test_loss=5.3565, test_R²=-124.5181\n",
      "Epoch 502/1200\n",
      "Train loss: 6.2930, Train R²: -124.3970\n",
      "test_loss=5.3566, test_R²=-124.5184\n",
      "Epoch 503/1200\n",
      "Train loss: 6.2858, Train R²: -123.9152\n",
      "test_loss=5.3565, test_R²=-124.5180\n",
      "Epoch 504/1200\n",
      "Train loss: 6.3197, Train R²: -123.8439\n",
      "test_loss=5.3564, test_R²=-124.5151\n",
      "Epoch 505/1200\n",
      "Train loss: 6.3245, Train R²: -123.8434\n",
      "test_loss=5.3563, test_R²=-124.5121\n",
      "Epoch 506/1200\n",
      "Train loss: 6.3565, Train R²: -124.9266\n",
      "test_loss=5.3562, test_R²=-124.5087\n",
      "Epoch 507/1200\n",
      "Train loss: 6.3204, Train R²: -124.3934\n",
      "test_loss=5.3560, test_R²=-124.5060\n",
      "Epoch 508/1200\n",
      "Train loss: 6.3231, Train R²: -124.1533\n",
      "test_loss=5.3560, test_R²=-124.5058\n",
      "Epoch 509/1200\n",
      "Train loss: 6.2798, Train R²: -124.1271\n",
      "test_loss=5.3561, test_R²=-124.5065\n",
      "Epoch 510/1200\n",
      "Train loss: 6.3297, Train R²: -124.1472\n",
      "test_loss=5.3562, test_R²=-124.5093\n",
      "Epoch 511/1200\n",
      "Train loss: 6.3091, Train R²: -125.6421\n",
      "test_loss=5.3562, test_R²=-124.5098\n",
      "Epoch 512/1200\n",
      "Train loss: 6.3158, Train R²: -124.5440\n",
      "test_loss=5.3560, test_R²=-124.5069\n",
      "Epoch 513/1200\n",
      "Train loss: 6.2932, Train R²: -124.4176\n",
      "test_loss=5.3559, test_R²=-124.5035\n",
      "Epoch 514/1200\n",
      "Train loss: 6.3110, Train R²: -124.6652\n",
      "test_loss=5.3559, test_R²=-124.5041\n",
      "Epoch 515/1200\n",
      "Train loss: 6.3095, Train R²: -123.8305\n",
      "test_loss=5.3561, test_R²=-124.5085\n",
      "Epoch 516/1200\n",
      "Train loss: 6.3140, Train R²: -124.2636\n",
      "test_loss=5.3563, test_R²=-124.5136\n",
      "Epoch 517/1200\n",
      "Train loss: 6.3282, Train R²: -124.1255\n",
      "test_loss=5.3564, test_R²=-124.5153\n",
      "Epoch 518/1200\n",
      "Train loss: 6.2761, Train R²: -123.7811\n",
      "test_loss=5.3564, test_R²=-124.5163\n",
      "Epoch 519/1200\n",
      "Train loss: 6.2746, Train R²: -123.8634\n",
      "test_loss=5.3565, test_R²=-124.5180\n",
      "Epoch 520/1200\n",
      "Train loss: 6.3000, Train R²: -124.4649\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 521/1200\n",
      "Train loss: 6.3099, Train R²: -125.3344\n",
      "test_loss=5.3566, test_R²=-124.5214\n",
      "Epoch 522/1200\n",
      "Train loss: 6.3098, Train R²: -123.9499\n",
      "test_loss=5.3567, test_R²=-124.5229\n",
      "Epoch 523/1200\n",
      "Train loss: 6.3374, Train R²: -124.7263\n",
      "test_loss=5.3567, test_R²=-124.5246\n",
      "Epoch 524/1200\n",
      "Train loss: 6.3264, Train R²: -124.2126\n",
      "test_loss=5.3567, test_R²=-124.5236\n",
      "Epoch 525/1200\n",
      "Train loss: 6.3192, Train R²: -125.2867\n",
      "test_loss=5.3568, test_R²=-124.5268\n",
      "Epoch 526/1200\n",
      "Train loss: 6.2952, Train R²: -124.0835\n",
      "test_loss=5.3571, test_R²=-124.5330\n",
      "Epoch 527/1200\n",
      "Train loss: 6.2839, Train R²: -124.9951\n",
      "test_loss=5.3571, test_R²=-124.5347\n",
      "Epoch 528/1200\n",
      "Train loss: 6.3228, Train R²: -124.7654\n",
      "test_loss=5.3571, test_R²=-124.5337\n",
      "Epoch 529/1200\n",
      "Train loss: 6.3051, Train R²: -124.0685\n",
      "test_loss=5.3571, test_R²=-124.5340\n",
      "Epoch 530/1200\n",
      "Train loss: 6.3384, Train R²: -124.1375\n",
      "test_loss=5.3568, test_R²=-124.5268\n",
      "Epoch 531/1200\n",
      "Train loss: 6.3127, Train R²: -123.8159\n",
      "test_loss=5.3567, test_R²=-124.5221\n",
      "Epoch 532/1200\n",
      "Train loss: 6.3215, Train R²: -123.9309\n",
      "test_loss=5.3565, test_R²=-124.5191\n",
      "Epoch 533/1200\n",
      "Train loss: 6.2933, Train R²: -124.0321\n",
      "test_loss=5.3565, test_R²=-124.5183\n",
      "Epoch 534/1200\n",
      "Train loss: 6.3498, Train R²: -123.9820\n",
      "test_loss=5.3565, test_R²=-124.5195\n",
      "Epoch 535/1200\n",
      "Train loss: 6.3105, Train R²: -124.1732\n",
      "test_loss=5.3566, test_R²=-124.5210\n",
      "Epoch 536/1200\n",
      "Train loss: 6.3056, Train R²: -123.9481\n",
      "test_loss=5.3567, test_R²=-124.5221\n",
      "Epoch 537/1200\n",
      "Train loss: 6.3452, Train R²: -123.6542\n",
      "test_loss=5.3568, test_R²=-124.5239\n",
      "Epoch 538/1200\n",
      "Train loss: 6.3265, Train R²: -123.8207\n",
      "test_loss=5.3568, test_R²=-124.5247\n",
      "Epoch 539/1200\n",
      "Train loss: 6.2757, Train R²: -123.9918\n",
      "test_loss=5.3569, test_R²=-124.5261\n",
      "Epoch 540/1200\n",
      "Train loss: 6.2889, Train R²: -124.4817\n",
      "test_loss=5.3568, test_R²=-124.5255\n",
      "Epoch 541/1200\n",
      "Train loss: 6.2733, Train R²: -124.4539\n",
      "test_loss=5.3567, test_R²=-124.5226\n",
      "Epoch 542/1200\n",
      "Train loss: 6.2856, Train R²: -124.3582\n",
      "test_loss=5.3564, test_R²=-124.5146\n",
      "Epoch 543/1200\n",
      "Train loss: 6.3244, Train R²: -123.8884\n",
      "test_loss=5.3562, test_R²=-124.5094\n",
      "Epoch 544/1200\n",
      "Train loss: 6.2908, Train R²: -125.0871\n",
      "test_loss=5.3561, test_R²=-124.5062\n",
      "Epoch 545/1200\n",
      "Train loss: 6.3009, Train R²: -123.9976\n",
      "test_loss=5.3560, test_R²=-124.5048\n",
      "Epoch 546/1200\n",
      "Train loss: 6.3395, Train R²: -124.2595\n",
      "test_loss=5.3560, test_R²=-124.5044\n",
      "Epoch 547/1200\n",
      "Train loss: 6.3020, Train R²: -123.5876\n",
      "test_loss=5.3560, test_R²=-124.5056\n",
      "Epoch 548/1200\n",
      "Train loss: 6.2849, Train R²: -124.1475\n",
      "test_loss=5.3561, test_R²=-124.5067\n",
      "Epoch 549/1200\n",
      "Train loss: 6.2683, Train R²: -124.2425\n",
      "test_loss=5.3561, test_R²=-124.5070\n",
      "Epoch 550/1200\n",
      "Train loss: 6.3275, Train R²: -123.9511\n",
      "test_loss=5.3561, test_R²=-124.5060\n",
      "Epoch 551/1200\n",
      "Train loss: 6.3125, Train R²: -124.0208\n",
      "test_loss=5.3561, test_R²=-124.5076\n",
      "Epoch 552/1200\n",
      "Train loss: 6.3427, Train R²: -124.0076\n",
      "test_loss=5.3562, test_R²=-124.5088\n",
      "Epoch 553/1200\n",
      "Train loss: 6.3441, Train R²: -124.3578\n",
      "test_loss=5.3563, test_R²=-124.5110\n",
      "Epoch 554/1200\n",
      "Train loss: 6.3256, Train R²: -124.5524\n",
      "test_loss=5.3564, test_R²=-124.5134\n",
      "Epoch 555/1200\n",
      "Train loss: 6.3257, Train R²: -123.8441\n",
      "test_loss=5.3565, test_R²=-124.5156\n",
      "Epoch 556/1200\n",
      "Train loss: 6.3010, Train R²: -125.6524\n",
      "test_loss=5.3565, test_R²=-124.5171\n",
      "Epoch 557/1200\n",
      "Train loss: 6.2809, Train R²: -124.2461\n",
      "test_loss=5.3566, test_R²=-124.5203\n",
      "Epoch 558/1200\n",
      "Train loss: 6.3474, Train R²: -124.6755\n",
      "test_loss=5.3567, test_R²=-124.5224\n",
      "Epoch 559/1200\n",
      "Train loss: 6.2950, Train R²: -124.4175\n",
      "test_loss=5.3567, test_R²=-124.5214\n",
      "Epoch 560/1200\n",
      "Train loss: 6.3383, Train R²: -124.1029\n",
      "test_loss=5.3566, test_R²=-124.5196\n",
      "Epoch 561/1200\n",
      "Train loss: 6.3151, Train R²: -124.8829\n",
      "test_loss=5.3567, test_R²=-124.5215\n",
      "Epoch 562/1200\n",
      "Train loss: 6.3175, Train R²: -123.9333\n",
      "test_loss=5.3566, test_R²=-124.5207\n",
      "Epoch 563/1200\n",
      "Train loss: 6.3240, Train R²: -125.5559\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 564/1200\n",
      "Train loss: 6.3515, Train R²: -123.6995\n",
      "test_loss=5.3564, test_R²=-124.5157\n",
      "Epoch 565/1200\n",
      "Train loss: 6.3398, Train R²: -124.3254\n",
      "test_loss=5.3564, test_R²=-124.5143\n",
      "Epoch 566/1200\n",
      "Train loss: 6.3130, Train R²: -123.9766\n",
      "test_loss=5.3564, test_R²=-124.5144\n",
      "Epoch 567/1200\n",
      "Train loss: 6.3027, Train R²: -124.4159\n",
      "test_loss=5.3564, test_R²=-124.5145\n",
      "Epoch 568/1200\n",
      "Train loss: 6.3484, Train R²: -123.9784\n",
      "test_loss=5.3565, test_R²=-124.5152\n",
      "Epoch 569/1200\n",
      "Train loss: 6.3290, Train R²: -125.0939\n",
      "test_loss=5.3564, test_R²=-124.5141\n",
      "Epoch 570/1200\n",
      "Train loss: 6.3076, Train R²: -124.2253\n",
      "test_loss=5.3564, test_R²=-124.5133\n",
      "Epoch 571/1200\n",
      "Train loss: 6.3214, Train R²: -123.7912\n",
      "test_loss=5.3564, test_R²=-124.5134\n",
      "Epoch 572/1200\n",
      "Train loss: 6.2936, Train R²: -124.6349\n",
      "test_loss=5.3565, test_R²=-124.5152\n",
      "Epoch 573/1200\n",
      "Train loss: 6.2967, Train R²: -124.6644\n",
      "test_loss=5.3566, test_R²=-124.5191\n",
      "Epoch 574/1200\n",
      "Train loss: 6.3223, Train R²: -123.9034\n",
      "test_loss=5.3566, test_R²=-124.5186\n",
      "Epoch 575/1200\n",
      "Train loss: 6.3182, Train R²: -125.5089\n",
      "test_loss=5.3566, test_R²=-124.5187\n",
      "Epoch 576/1200\n",
      "Train loss: 6.3257, Train R²: -123.9340\n",
      "test_loss=5.3565, test_R²=-124.5165\n",
      "Epoch 577/1200\n",
      "Train loss: 6.2888, Train R²: -124.0357\n",
      "test_loss=5.3563, test_R²=-124.5130\n",
      "Epoch 578/1200\n",
      "Train loss: 6.2963, Train R²: -125.2719\n",
      "test_loss=5.3562, test_R²=-124.5086\n",
      "Epoch 579/1200\n",
      "Train loss: 6.3354, Train R²: -124.1092\n",
      "test_loss=5.3561, test_R²=-124.5077\n",
      "Epoch 580/1200\n",
      "Train loss: 6.3047, Train R²: -124.8809\n",
      "test_loss=5.3560, test_R²=-124.5046\n",
      "Epoch 581/1200\n",
      "Train loss: 6.2840, Train R²: -124.2021\n",
      "test_loss=5.3558, test_R²=-124.4995\n",
      "Epoch 582/1200\n",
      "Train loss: 6.3355, Train R²: -125.3420\n",
      "test_loss=5.3557, test_R²=-124.4979\n",
      "Epoch 583/1200\n",
      "Train loss: 6.2859, Train R²: -124.7809\n",
      "test_loss=5.3559, test_R²=-124.5032\n",
      "Epoch 584/1200\n",
      "Train loss: 6.2870, Train R²: -124.4818\n",
      "test_loss=5.3563, test_R²=-124.5114\n",
      "Epoch 585/1200\n",
      "Train loss: 6.3197, Train R²: -124.0584\n",
      "test_loss=5.3565, test_R²=-124.5163\n",
      "Epoch 586/1200\n",
      "Train loss: 6.3038, Train R²: -123.7480\n",
      "test_loss=5.3566, test_R²=-124.5191\n",
      "Epoch 587/1200\n",
      "Train loss: 6.2932, Train R²: -124.3890\n",
      "test_loss=5.3568, test_R²=-124.5255\n",
      "Epoch 588/1200\n",
      "Train loss: 6.3109, Train R²: -124.1688\n",
      "test_loss=5.3569, test_R²=-124.5282\n",
      "Epoch 589/1200\n",
      "Train loss: 6.3020, Train R²: -124.9928\n",
      "test_loss=5.3570, test_R²=-124.5294\n",
      "Epoch 590/1200\n",
      "Train loss: 6.3105, Train R²: -123.8748\n",
      "test_loss=5.3569, test_R²=-124.5277\n",
      "Epoch 591/1200\n",
      "Train loss: 6.3256, Train R²: -124.1341\n",
      "test_loss=5.3568, test_R²=-124.5249\n",
      "Epoch 592/1200\n",
      "Train loss: 6.3201, Train R²: -124.3898\n",
      "test_loss=5.3567, test_R²=-124.5218\n",
      "Epoch 593/1200\n",
      "Train loss: 6.3210, Train R²: -124.4380\n",
      "test_loss=5.3565, test_R²=-124.5181\n",
      "Epoch 594/1200\n",
      "Train loss: 6.3555, Train R²: -124.3913\n",
      "test_loss=5.3564, test_R²=-124.5158\n",
      "Epoch 595/1200\n",
      "Train loss: 6.2881, Train R²: -124.4310\n",
      "test_loss=5.3564, test_R²=-124.5150\n",
      "Epoch 596/1200\n",
      "Train loss: 6.3128, Train R²: -123.6609\n",
      "test_loss=5.3566, test_R²=-124.5192\n",
      "Epoch 597/1200\n",
      "Train loss: 6.3320, Train R²: -123.8336\n",
      "test_loss=5.3567, test_R²=-124.5208\n",
      "Epoch 598/1200\n",
      "Train loss: 6.3108, Train R²: -125.3708\n",
      "test_loss=5.3566, test_R²=-124.5200\n",
      "Epoch 599/1200\n",
      "Train loss: 6.3202, Train R²: -124.5327\n",
      "test_loss=5.3564, test_R²=-124.5137\n",
      "Epoch 600/1200\n",
      "Train loss: 6.3281, Train R²: -124.2341\n",
      "test_loss=5.3562, test_R²=-124.5094\n",
      "Epoch 601/1200\n",
      "Train loss: 6.3193, Train R²: -124.1154\n",
      "test_loss=5.3562, test_R²=-124.5085\n",
      "Epoch 602/1200\n",
      "Train loss: 6.3669, Train R²: -123.8637\n",
      "test_loss=5.3561, test_R²=-124.5064\n",
      "Epoch 603/1200\n",
      "Train loss: 6.3154, Train R²: -124.4021\n",
      "test_loss=5.3560, test_R²=-124.5042\n",
      "Epoch 604/1200\n",
      "Train loss: 6.2714, Train R²: -124.2641\n",
      "test_loss=5.3562, test_R²=-124.5092\n",
      "Epoch 605/1200\n",
      "Train loss: 6.3177, Train R²: -123.8037\n",
      "test_loss=5.3563, test_R²=-124.5108\n",
      "Epoch 606/1200\n",
      "Train loss: 6.3333, Train R²: -124.6939\n",
      "test_loss=5.3562, test_R²=-124.5095\n",
      "Epoch 607/1200\n",
      "Train loss: 6.3239, Train R²: -124.8843\n",
      "test_loss=5.3562, test_R²=-124.5082\n",
      "Epoch 608/1200\n",
      "Train loss: 6.3094, Train R²: -123.7192\n",
      "test_loss=5.3561, test_R²=-124.5065\n",
      "Epoch 609/1200\n",
      "Train loss: 6.3881, Train R²: -123.7861\n",
      "test_loss=5.3560, test_R²=-124.5040\n",
      "Epoch 610/1200\n",
      "Train loss: 6.3172, Train R²: -123.9517\n",
      "test_loss=5.3559, test_R²=-124.5003\n",
      "Epoch 611/1200\n",
      "Train loss: 6.3580, Train R²: -124.5349\n",
      "test_loss=5.3558, test_R²=-124.4983\n",
      "Epoch 612/1200\n",
      "Train loss: 6.2669, Train R²: -124.8148\n",
      "test_loss=5.3558, test_R²=-124.4984\n",
      "Epoch 613/1200\n",
      "Train loss: 6.3369, Train R²: -124.0850\n",
      "test_loss=5.3559, test_R²=-124.5017\n",
      "Epoch 614/1200\n",
      "Train loss: 6.3394, Train R²: -124.5275\n",
      "test_loss=5.3559, test_R²=-124.5022\n",
      "Epoch 615/1200\n",
      "Train loss: 6.2967, Train R²: -124.8664\n",
      "test_loss=5.3560, test_R²=-124.5038\n",
      "Epoch 616/1200\n",
      "Train loss: 6.3119, Train R²: -124.2728\n",
      "test_loss=5.3560, test_R²=-124.5036\n",
      "Epoch 617/1200\n",
      "Train loss: 6.3757, Train R²: -124.2502\n",
      "test_loss=5.3560, test_R²=-124.5035\n",
      "Epoch 618/1200\n",
      "Train loss: 6.3252, Train R²: -124.1018\n",
      "test_loss=5.3560, test_R²=-124.5037\n",
      "Epoch 619/1200\n",
      "Train loss: 6.3528, Train R²: -124.1698\n",
      "test_loss=5.3560, test_R²=-124.5045\n",
      "Epoch 620/1200\n",
      "Train loss: 6.3074, Train R²: -124.3855\n",
      "test_loss=5.3560, test_R²=-124.5048\n",
      "Epoch 621/1200\n",
      "Train loss: 6.3613, Train R²: -124.4219\n",
      "test_loss=5.3560, test_R²=-124.5047\n",
      "Epoch 622/1200\n",
      "Train loss: 6.3003, Train R²: -124.3236\n",
      "test_loss=5.3561, test_R²=-124.5069\n",
      "Epoch 623/1200\n",
      "Train loss: 6.2840, Train R²: -124.1023\n",
      "test_loss=5.3562, test_R²=-124.5086\n",
      "Epoch 624/1200\n",
      "Train loss: 6.3010, Train R²: -123.8754\n",
      "test_loss=5.3561, test_R²=-124.5077\n",
      "Epoch 625/1200\n",
      "Train loss: 6.2950, Train R²: -124.1752\n",
      "test_loss=5.3562, test_R²=-124.5081\n",
      "Epoch 626/1200\n",
      "Train loss: 6.2806, Train R²: -123.9658\n",
      "test_loss=5.3561, test_R²=-124.5076\n",
      "Epoch 627/1200\n",
      "Train loss: 6.2913, Train R²: -124.2995\n",
      "test_loss=5.3561, test_R²=-124.5075\n",
      "Epoch 628/1200\n",
      "Train loss: 6.3016, Train R²: -124.3745\n",
      "test_loss=5.3561, test_R²=-124.5056\n",
      "Epoch 629/1200\n",
      "Train loss: 6.3177, Train R²: -123.7311\n",
      "test_loss=5.3560, test_R²=-124.5037\n",
      "Epoch 630/1200\n",
      "Train loss: 6.3372, Train R²: -124.4167\n",
      "test_loss=5.3560, test_R²=-124.5033\n",
      "Epoch 631/1200\n",
      "Train loss: 6.2771, Train R²: -125.1192\n",
      "test_loss=5.3560, test_R²=-124.5045\n",
      "Epoch 632/1200\n",
      "Train loss: 6.2702, Train R²: -125.3289\n",
      "test_loss=5.3562, test_R²=-124.5098\n",
      "Epoch 633/1200\n",
      "Train loss: 6.3265, Train R²: -123.9524\n",
      "test_loss=5.3563, test_R²=-124.5118\n",
      "Epoch 634/1200\n",
      "Train loss: 6.3058, Train R²: -124.0197\n",
      "test_loss=5.3563, test_R²=-124.5128\n",
      "Epoch 635/1200\n",
      "Train loss: 6.3219, Train R²: -123.9908\n",
      "test_loss=5.3562, test_R²=-124.5099\n",
      "Epoch 636/1200\n",
      "Train loss: 6.3164, Train R²: -124.2120\n",
      "test_loss=5.3561, test_R²=-124.5080\n",
      "Epoch 637/1200\n",
      "Train loss: 6.2935, Train R²: -124.0804\n",
      "test_loss=5.3562, test_R²=-124.5077\n",
      "Epoch 638/1200\n",
      "Train loss: 6.2893, Train R²: -124.9111\n",
      "test_loss=5.3563, test_R²=-124.5122\n",
      "Epoch 639/1200\n",
      "Train loss: 6.3213, Train R²: -123.8702\n",
      "test_loss=5.3565, test_R²=-124.5172\n",
      "Epoch 640/1200\n",
      "Train loss: 6.3267, Train R²: -124.2094\n",
      "test_loss=5.3567, test_R²=-124.5206\n",
      "Epoch 641/1200\n",
      "Train loss: 6.3003, Train R²: -124.1116\n",
      "test_loss=5.3566, test_R²=-124.5180\n",
      "Epoch 642/1200\n",
      "Train loss: 6.3185, Train R²: -124.1223\n",
      "test_loss=5.3566, test_R²=-124.5180\n",
      "Epoch 643/1200\n",
      "Train loss: 6.3091, Train R²: -124.6246\n",
      "test_loss=5.3567, test_R²=-124.5201\n",
      "Epoch 644/1200\n",
      "Train loss: 6.3529, Train R²: -123.9042\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 645/1200\n",
      "Train loss: 6.3283, Train R²: -124.1971\n",
      "test_loss=5.3566, test_R²=-124.5190\n",
      "Epoch 646/1200\n",
      "Train loss: 6.3185, Train R²: -124.0008\n",
      "test_loss=5.3566, test_R²=-124.5187\n",
      "Epoch 647/1200\n",
      "Train loss: 6.3259, Train R²: -124.0010\n",
      "test_loss=5.3566, test_R²=-124.5187\n",
      "Epoch 648/1200\n",
      "Train loss: 6.3184, Train R²: -123.8982\n",
      "test_loss=5.3566, test_R²=-124.5189\n",
      "Epoch 649/1200\n",
      "Train loss: 6.3176, Train R²: -125.4009\n",
      "test_loss=5.3565, test_R²=-124.5157\n",
      "Epoch 650/1200\n",
      "Train loss: 6.2860, Train R²: -124.6948\n",
      "test_loss=5.3566, test_R²=-124.5185\n",
      "Epoch 651/1200\n",
      "Train loss: 6.3188, Train R²: -124.3706\n",
      "test_loss=5.3565, test_R²=-124.5157\n",
      "Epoch 652/1200\n",
      "Train loss: 6.3353, Train R²: -124.5899\n",
      "test_loss=5.3564, test_R²=-124.5122\n",
      "Epoch 653/1200\n",
      "Train loss: 6.3507, Train R²: -123.9506\n",
      "test_loss=5.3564, test_R²=-124.5135\n",
      "Epoch 654/1200\n",
      "Train loss: 6.3375, Train R²: -124.5888\n",
      "test_loss=5.3564, test_R²=-124.5137\n",
      "Epoch 655/1200\n",
      "Train loss: 6.3396, Train R²: -124.1846\n",
      "test_loss=5.3564, test_R²=-124.5130\n",
      "Epoch 656/1200\n",
      "Train loss: 6.3112, Train R²: -124.8921\n",
      "test_loss=5.3563, test_R²=-124.5105\n",
      "Epoch 657/1200\n",
      "Train loss: 6.3044, Train R²: -124.6154\n",
      "test_loss=5.3563, test_R²=-124.5108\n",
      "Epoch 658/1200\n",
      "Train loss: 6.2822, Train R²: -124.2289\n",
      "test_loss=5.3563, test_R²=-124.5118\n",
      "Epoch 659/1200\n",
      "Train loss: 6.3430, Train R²: -124.6687\n",
      "test_loss=5.3563, test_R²=-124.5123\n",
      "Epoch 660/1200\n",
      "Train loss: 6.2957, Train R²: -123.9752\n",
      "test_loss=5.3564, test_R²=-124.5136\n",
      "Epoch 661/1200\n",
      "Train loss: 6.2994, Train R²: -124.4463\n",
      "test_loss=5.3564, test_R²=-124.5140\n",
      "Epoch 662/1200\n",
      "Train loss: 6.3246, Train R²: -124.6389\n",
      "test_loss=5.3566, test_R²=-124.5194\n",
      "Epoch 663/1200\n",
      "Train loss: 6.3480, Train R²: -124.0280\n",
      "test_loss=5.3567, test_R²=-124.5212\n",
      "Epoch 664/1200\n",
      "Train loss: 6.3140, Train R²: -124.0455\n",
      "test_loss=5.3566, test_R²=-124.5184\n",
      "Epoch 665/1200\n",
      "Train loss: 6.3231, Train R²: -124.3094\n",
      "test_loss=5.3564, test_R²=-124.5123\n",
      "Epoch 666/1200\n",
      "Train loss: 6.2923, Train R²: -124.2154\n",
      "test_loss=5.3562, test_R²=-124.5075\n",
      "Epoch 667/1200\n",
      "Train loss: 6.3184, Train R²: -123.8367\n",
      "test_loss=5.3561, test_R²=-124.5055\n",
      "Epoch 668/1200\n",
      "Train loss: 6.2873, Train R²: -123.9697\n",
      "test_loss=5.3560, test_R²=-124.5047\n",
      "Epoch 669/1200\n",
      "Train loss: 6.2974, Train R²: -124.2214\n",
      "test_loss=5.3560, test_R²=-124.5025\n",
      "Epoch 670/1200\n",
      "Train loss: 6.3047, Train R²: -124.8410\n",
      "test_loss=5.3559, test_R²=-124.5020\n",
      "Epoch 671/1200\n",
      "Train loss: 6.3158, Train R²: -124.4647\n",
      "test_loss=5.3559, test_R²=-124.5011\n",
      "Epoch 672/1200\n",
      "Train loss: 6.3117, Train R²: -124.6316\n",
      "test_loss=5.3559, test_R²=-124.5008\n",
      "Epoch 673/1200\n",
      "Train loss: 6.2980, Train R²: -124.4672\n",
      "test_loss=5.3560, test_R²=-124.5027\n",
      "Epoch 674/1200\n",
      "Train loss: 6.3301, Train R²: -124.5030\n",
      "test_loss=5.3559, test_R²=-124.5024\n",
      "Epoch 675/1200\n",
      "Train loss: 6.3298, Train R²: -124.8003\n",
      "test_loss=5.3562, test_R²=-124.5081\n",
      "Epoch 676/1200\n",
      "Train loss: 6.3147, Train R²: -124.3562\n",
      "test_loss=5.3563, test_R²=-124.5130\n",
      "Epoch 677/1200\n",
      "Train loss: 6.3310, Train R²: -124.8122\n",
      "test_loss=5.3565, test_R²=-124.5185\n",
      "Epoch 678/1200\n",
      "Train loss: 6.3127, Train R²: -124.1361\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 679/1200\n",
      "Train loss: 6.3132, Train R²: -124.1805\n",
      "test_loss=5.3564, test_R²=-124.5154\n",
      "Epoch 680/1200\n",
      "Train loss: 6.3215, Train R²: -124.0344\n",
      "test_loss=5.3564, test_R²=-124.5142\n",
      "Epoch 681/1200\n",
      "Train loss: 6.3173, Train R²: -124.0807\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 682/1200\n",
      "Train loss: 6.2894, Train R²: -124.1017\n",
      "test_loss=5.3564, test_R²=-124.5156\n",
      "Epoch 683/1200\n",
      "Train loss: 6.3336, Train R²: -124.5277\n",
      "test_loss=5.3565, test_R²=-124.5166\n",
      "Epoch 684/1200\n",
      "Train loss: 6.2908, Train R²: -124.4770\n",
      "test_loss=5.3564, test_R²=-124.5132\n",
      "Epoch 685/1200\n",
      "Train loss: 6.2788, Train R²: -125.8781\n",
      "test_loss=5.3562, test_R²=-124.5100\n",
      "Epoch 686/1200\n",
      "Train loss: 6.3193, Train R²: -124.7681\n",
      "test_loss=5.3562, test_R²=-124.5094\n",
      "Epoch 687/1200\n",
      "Train loss: 6.2970, Train R²: -124.6117\n",
      "test_loss=5.3562, test_R²=-124.5086\n",
      "Epoch 688/1200\n",
      "Train loss: 6.3154, Train R²: -124.6181\n",
      "test_loss=5.3563, test_R²=-124.5104\n",
      "Epoch 689/1200\n",
      "Train loss: 6.3043, Train R²: -124.6611\n",
      "test_loss=5.3562, test_R²=-124.5093\n",
      "Epoch 690/1200\n",
      "Train loss: 6.2784, Train R²: -124.4624\n",
      "test_loss=5.3562, test_R²=-124.5089\n",
      "Epoch 691/1200\n",
      "Train loss: 6.3745, Train R²: -124.1232\n",
      "test_loss=5.3562, test_R²=-124.5089\n",
      "Epoch 692/1200\n",
      "Train loss: 6.2990, Train R²: -124.1116\n",
      "test_loss=5.3562, test_R²=-124.5090\n",
      "Epoch 693/1200\n",
      "Train loss: 6.2937, Train R²: -125.1339\n",
      "test_loss=5.3563, test_R²=-124.5124\n",
      "Epoch 694/1200\n",
      "Train loss: 6.3118, Train R²: -124.9154\n",
      "test_loss=5.3564, test_R²=-124.5142\n",
      "Epoch 695/1200\n",
      "Train loss: 6.2649, Train R²: -124.5384\n",
      "test_loss=5.3565, test_R²=-124.5155\n",
      "Epoch 696/1200\n",
      "Train loss: 6.3112, Train R²: -124.5097\n",
      "test_loss=5.3565, test_R²=-124.5167\n",
      "Epoch 697/1200\n",
      "Train loss: 6.3289, Train R²: -123.8116\n",
      "test_loss=5.3566, test_R²=-124.5179\n",
      "Epoch 698/1200\n",
      "Train loss: 6.2933, Train R²: -123.7300\n",
      "test_loss=5.3565, test_R²=-124.5164\n",
      "Epoch 699/1200\n",
      "Train loss: 6.2846, Train R²: -124.2756\n",
      "test_loss=5.3565, test_R²=-124.5166\n",
      "Epoch 700/1200\n",
      "Train loss: 6.2712, Train R²: -125.0993\n",
      "test_loss=5.3565, test_R²=-124.5175\n",
      "Epoch 701/1200\n",
      "Train loss: 6.2987, Train R²: -123.6152\n",
      "test_loss=5.3565, test_R²=-124.5188\n",
      "Epoch 702/1200\n",
      "Train loss: 6.3094, Train R²: -123.7942\n",
      "test_loss=5.3565, test_R²=-124.5178\n",
      "Epoch 703/1200\n",
      "Train loss: 6.3081, Train R²: -125.1509\n",
      "test_loss=5.3565, test_R²=-124.5168\n",
      "Epoch 704/1200\n",
      "Train loss: 6.3692, Train R²: -124.0339\n",
      "test_loss=5.3564, test_R²=-124.5152\n",
      "Epoch 705/1200\n",
      "Train loss: 6.3107, Train R²: -124.3690\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 706/1200\n",
      "Train loss: 6.3298, Train R²: -124.0672\n",
      "test_loss=5.3564, test_R²=-124.5153\n",
      "Epoch 707/1200\n",
      "Train loss: 6.3603, Train R²: -125.2341\n",
      "test_loss=5.3565, test_R²=-124.5182\n",
      "Epoch 708/1200\n",
      "Train loss: 6.3301, Train R²: -124.5812\n",
      "test_loss=5.3566, test_R²=-124.5214\n",
      "Epoch 709/1200\n",
      "Train loss: 6.3091, Train R²: -124.4374\n",
      "test_loss=5.3569, test_R²=-124.5268\n",
      "Epoch 710/1200\n",
      "Train loss: 6.2680, Train R²: -124.1773\n",
      "test_loss=5.3570, test_R²=-124.5309\n",
      "Epoch 711/1200\n",
      "Train loss: 6.3024, Train R²: -124.6607\n",
      "test_loss=5.3571, test_R²=-124.5329\n",
      "Epoch 712/1200\n",
      "Train loss: 6.3424, Train R²: -124.5626\n",
      "test_loss=5.3571, test_R²=-124.5334\n",
      "Epoch 713/1200\n",
      "Train loss: 6.3113, Train R²: -124.1549\n",
      "test_loss=5.3570, test_R²=-124.5309\n",
      "Epoch 714/1200\n",
      "Train loss: 6.2986, Train R²: -123.9610\n",
      "test_loss=5.3569, test_R²=-124.5269\n",
      "Epoch 715/1200\n",
      "Train loss: 6.3195, Train R²: -124.3980\n",
      "test_loss=5.3568, test_R²=-124.5244\n",
      "Epoch 716/1200\n",
      "Train loss: 6.3413, Train R²: -124.3260\n",
      "test_loss=5.3568, test_R²=-124.5235\n",
      "Epoch 717/1200\n",
      "Train loss: 6.3686, Train R²: -124.5324\n",
      "test_loss=5.3568, test_R²=-124.5236\n",
      "Epoch 718/1200\n",
      "Train loss: 6.2931, Train R²: -123.9391\n",
      "test_loss=5.3567, test_R²=-124.5223\n",
      "Epoch 719/1200\n",
      "Train loss: 6.2969, Train R²: -125.1358\n",
      "test_loss=5.3566, test_R²=-124.5184\n",
      "Epoch 720/1200\n",
      "Train loss: 6.3313, Train R²: -123.8297\n",
      "test_loss=5.3565, test_R²=-124.5170\n",
      "Epoch 721/1200\n",
      "Train loss: 6.3046, Train R²: -124.5594\n",
      "test_loss=5.3565, test_R²=-124.5162\n",
      "Epoch 722/1200\n",
      "Train loss: 6.3201, Train R²: -124.2079\n",
      "test_loss=5.3564, test_R²=-124.5124\n",
      "Epoch 723/1200\n",
      "Train loss: 6.3013, Train R²: -124.7140\n",
      "test_loss=5.3563, test_R²=-124.5102\n",
      "Epoch 724/1200\n",
      "Train loss: 6.3039, Train R²: -125.3863\n",
      "test_loss=5.3563, test_R²=-124.5114\n",
      "Epoch 725/1200\n",
      "Train loss: 6.3064, Train R²: -124.6053\n",
      "test_loss=5.3565, test_R²=-124.5168\n",
      "Epoch 726/1200\n",
      "Train loss: 6.2983, Train R²: -123.8684\n",
      "test_loss=5.3567, test_R²=-124.5207\n",
      "Epoch 727/1200\n",
      "Train loss: 6.3277, Train R²: -124.1123\n",
      "test_loss=5.3567, test_R²=-124.5198\n",
      "Epoch 728/1200\n",
      "Train loss: 6.3007, Train R²: -124.5911\n",
      "test_loss=5.3565, test_R²=-124.5165\n",
      "Epoch 729/1200\n",
      "Train loss: 6.2830, Train R²: -125.6351\n",
      "test_loss=5.3566, test_R²=-124.5179\n",
      "Epoch 730/1200\n",
      "Train loss: 6.3053, Train R²: -124.5303\n",
      "test_loss=5.3568, test_R²=-124.5219\n",
      "Epoch 731/1200\n",
      "Train loss: 6.3078, Train R²: -123.8934\n",
      "test_loss=5.3568, test_R²=-124.5230\n",
      "Epoch 732/1200\n",
      "Train loss: 6.2751, Train R²: -123.9260\n",
      "test_loss=5.3568, test_R²=-124.5233\n",
      "Epoch 733/1200\n",
      "Train loss: 6.2966, Train R²: -124.2290\n",
      "test_loss=5.3569, test_R²=-124.5252\n",
      "Epoch 734/1200\n",
      "Train loss: 6.2802, Train R²: -124.6649\n",
      "test_loss=5.3572, test_R²=-124.5307\n",
      "Epoch 735/1200\n",
      "Train loss: 6.3311, Train R²: -125.0985\n",
      "test_loss=5.3573, test_R²=-124.5337\n",
      "Epoch 736/1200\n",
      "Train loss: 6.3200, Train R²: -124.3664\n",
      "test_loss=5.3571, test_R²=-124.5295\n",
      "Epoch 737/1200\n",
      "Train loss: 6.2854, Train R²: -124.2599\n",
      "test_loss=5.3569, test_R²=-124.5254\n",
      "Epoch 738/1200\n",
      "Train loss: 6.3021, Train R²: -124.3591\n",
      "test_loss=5.3567, test_R²=-124.5207\n",
      "Epoch 739/1200\n",
      "Train loss: 6.2903, Train R²: -123.8718\n",
      "test_loss=5.3567, test_R²=-124.5208\n",
      "Epoch 740/1200\n",
      "Train loss: 6.3030, Train R²: -124.8727\n",
      "test_loss=5.3566, test_R²=-124.5188\n",
      "Epoch 741/1200\n",
      "Train loss: 6.3079, Train R²: -125.1006\n",
      "test_loss=5.3564, test_R²=-124.5145\n",
      "Epoch 742/1200\n",
      "Train loss: 6.3377, Train R²: -124.3229\n",
      "test_loss=5.3564, test_R²=-124.5137\n",
      "Epoch 743/1200\n",
      "Train loss: 6.3193, Train R²: -124.7251\n",
      "test_loss=5.3564, test_R²=-124.5136\n",
      "Epoch 744/1200\n",
      "Train loss: 6.3325, Train R²: -125.5525\n",
      "test_loss=5.3565, test_R²=-124.5158\n",
      "Epoch 745/1200\n",
      "Train loss: 6.3487, Train R²: -125.0626\n",
      "test_loss=5.3566, test_R²=-124.5183\n",
      "Epoch 746/1200\n",
      "Train loss: 6.3018, Train R²: -124.0148\n",
      "test_loss=5.3568, test_R²=-124.5231\n",
      "Epoch 747/1200\n",
      "Train loss: 6.3024, Train R²: -124.2819\n",
      "test_loss=5.3569, test_R²=-124.5238\n",
      "Epoch 748/1200\n",
      "Train loss: 6.3145, Train R²: -124.6385\n",
      "test_loss=5.3568, test_R²=-124.5229\n",
      "Epoch 749/1200\n",
      "Train loss: 6.3210, Train R²: -124.6163\n",
      "test_loss=5.3568, test_R²=-124.5233\n",
      "Epoch 750/1200\n",
      "Train loss: 6.3547, Train R²: -125.1027\n",
      "test_loss=5.3568, test_R²=-124.5221\n",
      "Epoch 751/1200\n",
      "Train loss: 6.2878, Train R²: -123.9328\n",
      "test_loss=5.3567, test_R²=-124.5200\n",
      "Epoch 752/1200\n",
      "Train loss: 6.3432, Train R²: -124.8169\n",
      "test_loss=5.3566, test_R²=-124.5178\n",
      "Epoch 753/1200\n",
      "Train loss: 6.3224, Train R²: -124.5256\n",
      "test_loss=5.3565, test_R²=-124.5153\n",
      "Epoch 754/1200\n",
      "Train loss: 6.3448, Train R²: -124.4530\n",
      "test_loss=5.3565, test_R²=-124.5144\n",
      "Epoch 755/1200\n",
      "Train loss: 6.2784, Train R²: -124.4788\n",
      "test_loss=5.3566, test_R²=-124.5178\n",
      "Epoch 756/1200\n",
      "Train loss: 6.3057, Train R²: -123.9984\n",
      "test_loss=5.3566, test_R²=-124.5185\n",
      "Epoch 757/1200\n",
      "Train loss: 6.3127, Train R²: -124.7818\n",
      "test_loss=5.3567, test_R²=-124.5196\n",
      "Epoch 758/1200\n",
      "Train loss: 6.2849, Train R²: -124.7023\n",
      "test_loss=5.3568, test_R²=-124.5226\n",
      "Epoch 759/1200\n",
      "Train loss: 6.3351, Train R²: -124.1601\n",
      "test_loss=5.3569, test_R²=-124.5240\n",
      "Epoch 760/1200\n",
      "Train loss: 6.3768, Train R²: -124.3943\n",
      "test_loss=5.3570, test_R²=-124.5264\n",
      "Epoch 761/1200\n",
      "Train loss: 6.3557, Train R²: -124.3889\n",
      "test_loss=5.3570, test_R²=-124.5282\n",
      "Epoch 762/1200\n",
      "Train loss: 6.3284, Train R²: -124.2946\n",
      "test_loss=5.3571, test_R²=-124.5309\n",
      "Epoch 763/1200\n",
      "Train loss: 6.2722, Train R²: -124.1856\n",
      "test_loss=5.3571, test_R²=-124.5320\n",
      "Epoch 764/1200\n",
      "Train loss: 6.2850, Train R²: -123.8664\n",
      "test_loss=5.3571, test_R²=-124.5320\n",
      "Epoch 765/1200\n",
      "Train loss: 6.2955, Train R²: -124.5757\n",
      "test_loss=5.3571, test_R²=-124.5309\n",
      "Epoch 766/1200\n",
      "Train loss: 6.3313, Train R²: -124.2125\n",
      "test_loss=5.3570, test_R²=-124.5295\n",
      "Epoch 767/1200\n",
      "Train loss: 6.3432, Train R²: -124.1732\n",
      "test_loss=5.3569, test_R²=-124.5272\n",
      "Epoch 768/1200\n",
      "Train loss: 6.2964, Train R²: -124.3005\n",
      "test_loss=5.3568, test_R²=-124.5236\n",
      "Epoch 769/1200\n",
      "Train loss: 6.3776, Train R²: -123.8180\n",
      "test_loss=5.3567, test_R²=-124.5207\n",
      "Epoch 770/1200\n",
      "Train loss: 6.3592, Train R²: -124.3741\n",
      "test_loss=5.3565, test_R²=-124.5179\n",
      "Epoch 771/1200\n",
      "Train loss: 6.3131, Train R²: -124.3894\n",
      "test_loss=5.3566, test_R²=-124.5191\n",
      "Epoch 772/1200\n",
      "Train loss: 6.3187, Train R²: -123.9520\n",
      "test_loss=5.3567, test_R²=-124.5204\n",
      "Epoch 773/1200\n",
      "Train loss: 6.3175, Train R²: -123.8325\n",
      "test_loss=5.3566, test_R²=-124.5198\n",
      "Epoch 774/1200\n",
      "Train loss: 6.3473, Train R²: -124.2618\n",
      "test_loss=5.3566, test_R²=-124.5198\n",
      "Epoch 775/1200\n",
      "Train loss: 6.3398, Train R²: -123.8768\n",
      "test_loss=5.3567, test_R²=-124.5214\n",
      "Epoch 776/1200\n",
      "Train loss: 6.3032, Train R²: -123.9860\n",
      "test_loss=5.3567, test_R²=-124.5213\n",
      "Epoch 777/1200\n",
      "Train loss: 6.3235, Train R²: -124.8745\n",
      "test_loss=5.3565, test_R²=-124.5174\n",
      "Epoch 778/1200\n",
      "Train loss: 6.3563, Train R²: -124.8180\n",
      "test_loss=5.3565, test_R²=-124.5158\n",
      "Epoch 779/1200\n",
      "Train loss: 6.3314, Train R²: -124.2373\n",
      "test_loss=5.3564, test_R²=-124.5145\n",
      "Epoch 780/1200\n",
      "Train loss: 6.3344, Train R²: -125.1189\n",
      "test_loss=5.3564, test_R²=-124.5151\n",
      "Epoch 781/1200\n",
      "Train loss: 6.3404, Train R²: -124.2229\n",
      "test_loss=5.3566, test_R²=-124.5185\n",
      "Epoch 782/1200\n",
      "Train loss: 6.3452, Train R²: -123.8671\n",
      "test_loss=5.3566, test_R²=-124.5192\n",
      "Epoch 783/1200\n",
      "Train loss: 6.3252, Train R²: -124.3502\n",
      "test_loss=5.3567, test_R²=-124.5214\n",
      "Epoch 784/1200\n",
      "Train loss: 6.3101, Train R²: -124.9725\n",
      "test_loss=5.3566, test_R²=-124.5177\n",
      "Epoch 785/1200\n",
      "Train loss: 6.3125, Train R²: -124.3766\n",
      "test_loss=5.3565, test_R²=-124.5150\n",
      "Epoch 786/1200\n",
      "Train loss: 6.2983, Train R²: -124.3725\n",
      "test_loss=5.3565, test_R²=-124.5151\n",
      "Epoch 787/1200\n",
      "Train loss: 6.3202, Train R²: -123.6627\n",
      "test_loss=5.3566, test_R²=-124.5163\n",
      "Epoch 788/1200\n",
      "Train loss: 6.2989, Train R²: -124.4645\n",
      "test_loss=5.3566, test_R²=-124.5164\n",
      "Epoch 789/1200\n",
      "Train loss: 6.3531, Train R²: -124.5315\n",
      "test_loss=5.3565, test_R²=-124.5147\n",
      "Epoch 790/1200\n",
      "Train loss: 6.2960, Train R²: -124.0679\n",
      "test_loss=5.3564, test_R²=-124.5129\n",
      "Epoch 791/1200\n",
      "Train loss: 6.3440, Train R²: -124.5768\n",
      "test_loss=5.3564, test_R²=-124.5116\n",
      "Epoch 792/1200\n",
      "Train loss: 6.3450, Train R²: -124.6133\n",
      "test_loss=5.3564, test_R²=-124.5131\n",
      "Epoch 793/1200\n",
      "Train loss: 6.3044, Train R²: -124.3610\n",
      "test_loss=5.3565, test_R²=-124.5148\n",
      "Epoch 794/1200\n",
      "Train loss: 6.2977, Train R²: -123.6905\n",
      "test_loss=5.3566, test_R²=-124.5157\n",
      "Epoch 795/1200\n",
      "Train loss: 6.3102, Train R²: -124.6118\n",
      "test_loss=5.3565, test_R²=-124.5142\n",
      "Epoch 796/1200\n",
      "Train loss: 6.3423, Train R²: -124.0687\n",
      "test_loss=5.3564, test_R²=-124.5130\n",
      "Epoch 797/1200\n",
      "Train loss: 6.2888, Train R²: -124.7734\n",
      "test_loss=5.3565, test_R²=-124.5153\n",
      "Epoch 798/1200\n",
      "Train loss: 6.3084, Train R²: -123.8083\n",
      "test_loss=5.3564, test_R²=-124.5132\n",
      "Epoch 799/1200\n",
      "Train loss: 6.3634, Train R²: -124.2274\n",
      "test_loss=5.3564, test_R²=-124.5125\n",
      "Epoch 800/1200\n",
      "Train loss: 6.2927, Train R²: -124.7963\n",
      "test_loss=5.3564, test_R²=-124.5114\n",
      "Epoch 801/1200\n",
      "Train loss: 6.3213, Train R²: -124.1923\n",
      "test_loss=5.3564, test_R²=-124.5114\n",
      "Epoch 802/1200\n",
      "Train loss: 6.3197, Train R²: -124.3366\n",
      "test_loss=5.3564, test_R²=-124.5127\n",
      "Epoch 803/1200\n",
      "Train loss: 6.3462, Train R²: -123.7278\n",
      "test_loss=5.3565, test_R²=-124.5141\n",
      "Epoch 804/1200\n",
      "Train loss: 6.3170, Train R²: -124.1172\n",
      "test_loss=5.3566, test_R²=-124.5170\n",
      "Epoch 805/1200\n",
      "Train loss: 6.2964, Train R²: -123.9315\n",
      "test_loss=5.3565, test_R²=-124.5164\n",
      "Epoch 806/1200\n",
      "Train loss: 6.2833, Train R²: -124.0637\n",
      "test_loss=5.3565, test_R²=-124.5144\n",
      "Epoch 807/1200\n",
      "Train loss: 6.2912, Train R²: -124.3833\n",
      "test_loss=5.3566, test_R²=-124.5170\n",
      "Epoch 808/1200\n",
      "Train loss: 6.3682, Train R²: -124.5102\n",
      "test_loss=5.3568, test_R²=-124.5225\n",
      "Epoch 809/1200\n",
      "Train loss: 6.3462, Train R²: -125.1016\n",
      "test_loss=5.3569, test_R²=-124.5247\n",
      "Epoch 810/1200\n",
      "Train loss: 6.3215, Train R²: -124.1500\n",
      "test_loss=5.3570, test_R²=-124.5271\n",
      "Epoch 811/1200\n",
      "Train loss: 6.3341, Train R²: -124.5627\n",
      "test_loss=5.3569, test_R²=-124.5267\n",
      "Epoch 812/1200\n",
      "Train loss: 6.2860, Train R²: -124.3716\n",
      "test_loss=5.3570, test_R²=-124.5269\n",
      "Epoch 813/1200\n",
      "Train loss: 6.2790, Train R²: -124.5894\n",
      "test_loss=5.3569, test_R²=-124.5252\n",
      "Epoch 814/1200\n",
      "Train loss: 6.2937, Train R²: -124.0529\n",
      "test_loss=5.3568, test_R²=-124.5236\n",
      "Epoch 815/1200\n",
      "Train loss: 6.3425, Train R²: -124.3672\n",
      "test_loss=5.3567, test_R²=-124.5226\n",
      "Epoch 816/1200\n",
      "Train loss: 6.2932, Train R²: -123.7823\n",
      "test_loss=5.3567, test_R²=-124.5207\n",
      "Epoch 817/1200\n",
      "Train loss: 6.2882, Train R²: -123.9167\n",
      "test_loss=5.3566, test_R²=-124.5202\n",
      "Epoch 818/1200\n",
      "Train loss: 6.3396, Train R²: -124.5429\n",
      "test_loss=5.3568, test_R²=-124.5229\n",
      "Epoch 819/1200\n",
      "Train loss: 6.3468, Train R²: -126.2875\n",
      "test_loss=5.3570, test_R²=-124.5287\n",
      "Epoch 820/1200\n",
      "Train loss: 6.3114, Train R²: -124.5840\n",
      "test_loss=5.3573, test_R²=-124.5357\n",
      "Epoch 821/1200\n",
      "Train loss: 6.2738, Train R²: -123.7946\n",
      "test_loss=5.3573, test_R²=-124.5364\n",
      "Epoch 822/1200\n",
      "Train loss: 6.2890, Train R²: -124.3452\n",
      "test_loss=5.3571, test_R²=-124.5304\n",
      "Epoch 823/1200\n",
      "Train loss: 6.3083, Train R²: -124.8867\n",
      "test_loss=5.3570, test_R²=-124.5271\n",
      "Epoch 824/1200\n",
      "Train loss: 6.3076, Train R²: -124.9104\n",
      "test_loss=5.3568, test_R²=-124.5234\n",
      "Epoch 825/1200\n",
      "Train loss: 6.3461, Train R²: -124.0954\n",
      "test_loss=5.3566, test_R²=-124.5178\n",
      "Epoch 826/1200\n",
      "Train loss: 6.3091, Train R²: -124.5090\n",
      "test_loss=5.3564, test_R²=-124.5132\n",
      "Epoch 827/1200\n",
      "Train loss: 6.3007, Train R²: -124.0385\n",
      "test_loss=5.3563, test_R²=-124.5110\n",
      "Epoch 828/1200\n",
      "Train loss: 6.3739, Train R²: -124.2214\n",
      "test_loss=5.3562, test_R²=-124.5076\n",
      "Epoch 829/1200\n",
      "Train loss: 6.3234, Train R²: -124.2922\n",
      "test_loss=5.3561, test_R²=-124.5061\n",
      "Epoch 830/1200\n",
      "Train loss: 6.3529, Train R²: -124.3600\n",
      "test_loss=5.3561, test_R²=-124.5070\n",
      "Epoch 831/1200\n",
      "Train loss: 6.3150, Train R²: -124.5434\n",
      "test_loss=5.3563, test_R²=-124.5110\n",
      "Epoch 832/1200\n",
      "Train loss: 6.3006, Train R²: -124.5214\n",
      "test_loss=5.3566, test_R²=-124.5182\n",
      "Epoch 833/1200\n",
      "Train loss: 6.3476, Train R²: -124.4497\n",
      "test_loss=5.3567, test_R²=-124.5204\n",
      "Epoch 834/1200\n",
      "Train loss: 6.2947, Train R²: -123.8246\n",
      "test_loss=5.3566, test_R²=-124.5184\n",
      "Epoch 835/1200\n",
      "Train loss: 6.2990, Train R²: -124.0207\n",
      "test_loss=5.3565, test_R²=-124.5170\n",
      "Epoch 836/1200\n",
      "Train loss: 6.3247, Train R²: -124.4817\n",
      "test_loss=5.3565, test_R²=-124.5158\n",
      "Epoch 837/1200\n",
      "Train loss: 6.2977, Train R²: -124.0098\n",
      "test_loss=5.3563, test_R²=-124.5116\n",
      "Epoch 838/1200\n",
      "Train loss: 6.3171, Train R²: -124.4828\n",
      "test_loss=5.3563, test_R²=-124.5109\n",
      "Epoch 839/1200\n",
      "Train loss: 6.3497, Train R²: -124.4091\n",
      "test_loss=5.3563, test_R²=-124.5105\n",
      "Epoch 840/1200\n",
      "Train loss: 6.3148, Train R²: -124.3551\n",
      "test_loss=5.3563, test_R²=-124.5109\n",
      "Epoch 841/1200\n",
      "Train loss: 6.2944, Train R²: -123.9136\n",
      "test_loss=5.3563, test_R²=-124.5104\n",
      "Epoch 842/1200\n",
      "Train loss: 6.3094, Train R²: -123.9867\n",
      "test_loss=5.3562, test_R²=-124.5075\n",
      "Epoch 843/1200\n",
      "Train loss: 6.3289, Train R²: -125.1502\n",
      "test_loss=5.3563, test_R²=-124.5107\n",
      "Epoch 844/1200\n",
      "Train loss: 6.2765, Train R²: -124.0879\n",
      "test_loss=5.3563, test_R²=-124.5119\n",
      "Epoch 845/1200\n",
      "Train loss: 6.3563, Train R²: -124.7347\n",
      "test_loss=5.3563, test_R²=-124.5118\n",
      "Epoch 846/1200\n",
      "Train loss: 6.3421, Train R²: -124.0915\n",
      "test_loss=5.3563, test_R²=-124.5116\n",
      "Epoch 847/1200\n",
      "Train loss: 6.2886, Train R²: -124.6182\n",
      "test_loss=5.3563, test_R²=-124.5102\n",
      "Epoch 848/1200\n",
      "Train loss: 6.2783, Train R²: -125.4100\n",
      "test_loss=5.3562, test_R²=-124.5077\n",
      "Epoch 849/1200\n",
      "Train loss: 6.3160, Train R²: -123.8322\n",
      "test_loss=5.3561, test_R²=-124.5066\n",
      "Epoch 850/1200\n",
      "Train loss: 6.3265, Train R²: -124.0626\n",
      "test_loss=5.3561, test_R²=-124.5068\n",
      "Epoch 851/1200\n",
      "Train loss: 6.3096, Train R²: -124.7798\n",
      "test_loss=5.3559, test_R²=-124.5022\n",
      "Epoch 852/1200\n",
      "Train loss: 6.3577, Train R²: -124.5435\n",
      "test_loss=5.3559, test_R²=-124.5016\n",
      "Epoch 853/1200\n",
      "Train loss: 6.2964, Train R²: -124.2793\n",
      "test_loss=5.3560, test_R²=-124.5045\n",
      "Epoch 854/1200\n",
      "Train loss: 6.2717, Train R²: -124.1875\n",
      "test_loss=5.3561, test_R²=-124.5060\n",
      "Epoch 855/1200\n",
      "Train loss: 6.3282, Train R²: -124.2612\n",
      "test_loss=5.3561, test_R²=-124.5060\n",
      "Epoch 856/1200\n",
      "Train loss: 6.3391, Train R²: -124.2848\n",
      "test_loss=5.3559, test_R²=-124.4999\n",
      "Epoch 857/1200\n",
      "Train loss: 6.3261, Train R²: -124.6106\n",
      "test_loss=5.3556, test_R²=-124.4949\n",
      "Epoch 858/1200\n",
      "Train loss: 6.3164, Train R²: -123.9166\n",
      "test_loss=5.3555, test_R²=-124.4913\n",
      "Epoch 859/1200\n",
      "Train loss: 6.2855, Train R²: -123.7243\n",
      "test_loss=5.3555, test_R²=-124.4920\n",
      "Epoch 860/1200\n",
      "Train loss: 6.2938, Train R²: -124.1445\n",
      "test_loss=5.3557, test_R²=-124.4954\n",
      "Epoch 861/1200\n",
      "Train loss: 6.3047, Train R²: -123.7385\n",
      "test_loss=5.3557, test_R²=-124.4973\n",
      "Epoch 862/1200\n",
      "Train loss: 6.3002, Train R²: -124.6729\n",
      "test_loss=5.3557, test_R²=-124.4971\n",
      "Epoch 863/1200\n",
      "Train loss: 6.3120, Train R²: -123.7165\n",
      "test_loss=5.3556, test_R²=-124.4941\n",
      "Epoch 864/1200\n",
      "Train loss: 6.3208, Train R²: -123.8860\n",
      "test_loss=5.3555, test_R²=-124.4925\n",
      "Epoch 865/1200\n",
      "Train loss: 6.3366, Train R²: -123.9063\n",
      "test_loss=5.3555, test_R²=-124.4930\n",
      "Epoch 866/1200\n",
      "Train loss: 6.3181, Train R²: -124.3843\n",
      "test_loss=5.3557, test_R²=-124.4980\n",
      "Epoch 867/1200\n",
      "Train loss: 6.2870, Train R²: -124.5111\n",
      "test_loss=5.3559, test_R²=-124.5015\n",
      "Epoch 868/1200\n",
      "Train loss: 6.3110, Train R²: -124.6421\n",
      "test_loss=5.3560, test_R²=-124.5034\n",
      "Epoch 869/1200\n",
      "Train loss: 6.3095, Train R²: -124.4105\n",
      "test_loss=5.3561, test_R²=-124.5053\n",
      "Epoch 870/1200\n",
      "Train loss: 6.2918, Train R²: -123.9082\n",
      "test_loss=5.3560, test_R²=-124.5034\n",
      "Epoch 871/1200\n",
      "Train loss: 6.3506, Train R²: -124.2209\n",
      "test_loss=5.3558, test_R²=-124.4996\n",
      "Epoch 872/1200\n",
      "Train loss: 6.3016, Train R²: -123.6330\n",
      "test_loss=5.3557, test_R²=-124.4973\n",
      "Epoch 873/1200\n",
      "Train loss: 6.3174, Train R²: -124.1536\n",
      "test_loss=5.3557, test_R²=-124.4967\n",
      "Epoch 874/1200\n",
      "Train loss: 6.3414, Train R²: -124.2444\n",
      "test_loss=5.3557, test_R²=-124.4974\n",
      "Epoch 875/1200\n",
      "Train loss: 6.2858, Train R²: -125.6416\n",
      "test_loss=5.3559, test_R²=-124.5025\n",
      "Epoch 876/1200\n",
      "Train loss: 6.3319, Train R²: -124.6296\n",
      "test_loss=5.3561, test_R²=-124.5075\n",
      "Epoch 877/1200\n",
      "Train loss: 6.3328, Train R²: -124.2907\n",
      "test_loss=5.3562, test_R²=-124.5106\n",
      "Epoch 878/1200\n",
      "Train loss: 6.3455, Train R²: -123.6336\n",
      "test_loss=5.3563, test_R²=-124.5128\n",
      "Epoch 879/1200\n",
      "Train loss: 6.3270, Train R²: -123.6362\n",
      "test_loss=5.3564, test_R²=-124.5134\n",
      "Epoch 880/1200\n",
      "Train loss: 6.2922, Train R²: -124.9849\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 881/1200\n",
      "Train loss: 6.2855, Train R²: -123.9833\n",
      "test_loss=5.3566, test_R²=-124.5191\n",
      "Epoch 882/1200\n",
      "Train loss: 6.3125, Train R²: -124.2581\n",
      "test_loss=5.3564, test_R²=-124.5156\n",
      "Epoch 883/1200\n",
      "Train loss: 6.3271, Train R²: -124.8408\n",
      "test_loss=5.3563, test_R²=-124.5139\n",
      "Epoch 884/1200\n",
      "Train loss: 6.2996, Train R²: -123.7230\n",
      "test_loss=5.3563, test_R²=-124.5138\n",
      "Epoch 885/1200\n",
      "Train loss: 6.3045, Train R²: -123.9779\n",
      "test_loss=5.3563, test_R²=-124.5142\n",
      "Epoch 886/1200\n",
      "Train loss: 6.3299, Train R²: -124.5491\n",
      "test_loss=5.3564, test_R²=-124.5139\n",
      "Epoch 887/1200\n",
      "Train loss: 6.2710, Train R²: -124.3561\n",
      "test_loss=5.3564, test_R²=-124.5155\n",
      "Epoch 888/1200\n",
      "Train loss: 6.3018, Train R²: -124.5246\n",
      "test_loss=5.3564, test_R²=-124.5156\n",
      "Epoch 889/1200\n",
      "Train loss: 6.3090, Train R²: -124.5379\n",
      "test_loss=5.3563, test_R²=-124.5130\n",
      "Epoch 890/1200\n",
      "Train loss: 6.3215, Train R²: -123.8567\n",
      "test_loss=5.3563, test_R²=-124.5108\n",
      "Epoch 891/1200\n",
      "Train loss: 6.3112, Train R²: -124.1521\n",
      "test_loss=5.3562, test_R²=-124.5099\n",
      "Epoch 892/1200\n",
      "Train loss: 6.3314, Train R²: -123.9961\n",
      "test_loss=5.3563, test_R²=-124.5112\n",
      "Epoch 893/1200\n",
      "Train loss: 6.3368, Train R²: -124.8565\n",
      "test_loss=5.3565, test_R²=-124.5172\n",
      "Epoch 894/1200\n",
      "Train loss: 6.3457, Train R²: -124.0877\n",
      "test_loss=5.3566, test_R²=-124.5202\n",
      "Epoch 895/1200\n",
      "Train loss: 6.3501, Train R²: -123.6273\n",
      "test_loss=5.3566, test_R²=-124.5196\n",
      "Epoch 896/1200\n",
      "Train loss: 6.3112, Train R²: -124.3671\n",
      "test_loss=5.3566, test_R²=-124.5194\n",
      "Epoch 897/1200\n",
      "Train loss: 6.3000, Train R²: -124.8493\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 898/1200\n",
      "Train loss: 6.3271, Train R²: -124.2749\n",
      "test_loss=5.3567, test_R²=-124.5223\n",
      "Epoch 899/1200\n",
      "Train loss: 6.3096, Train R²: -124.3976\n",
      "test_loss=5.3568, test_R²=-124.5228\n",
      "Epoch 900/1200\n",
      "Train loss: 6.3310, Train R²: -123.8471\n",
      "test_loss=5.3567, test_R²=-124.5210\n",
      "Epoch 901/1200\n",
      "Train loss: 6.3580, Train R²: -123.8742\n",
      "test_loss=5.3565, test_R²=-124.5176\n",
      "Epoch 902/1200\n",
      "Train loss: 6.3267, Train R²: -124.1418\n",
      "test_loss=5.3564, test_R²=-124.5155\n",
      "Epoch 903/1200\n",
      "Train loss: 6.3135, Train R²: -125.2759\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 904/1200\n",
      "Train loss: 6.3640, Train R²: -124.4593\n",
      "test_loss=5.3567, test_R²=-124.5221\n",
      "Epoch 905/1200\n",
      "Train loss: 6.3007, Train R²: -123.8767\n",
      "test_loss=5.3569, test_R²=-124.5268\n",
      "Epoch 906/1200\n",
      "Train loss: 6.3501, Train R²: -124.5834\n",
      "test_loss=5.3571, test_R²=-124.5301\n",
      "Epoch 907/1200\n",
      "Train loss: 6.3636, Train R²: -124.7921\n",
      "test_loss=5.3571, test_R²=-124.5315\n",
      "Epoch 908/1200\n",
      "Train loss: 6.3087, Train R²: -124.6771\n",
      "test_loss=5.3570, test_R²=-124.5291\n",
      "Epoch 909/1200\n",
      "Train loss: 6.3434, Train R²: -124.1598\n",
      "test_loss=5.3569, test_R²=-124.5254\n",
      "Epoch 910/1200\n",
      "Train loss: 6.3299, Train R²: -124.4545\n",
      "test_loss=5.3568, test_R²=-124.5232\n",
      "Epoch 911/1200\n",
      "Train loss: 6.3075, Train R²: -124.0126\n",
      "test_loss=5.3568, test_R²=-124.5228\n",
      "Epoch 912/1200\n",
      "Train loss: 6.3577, Train R²: -124.5526\n",
      "test_loss=5.3568, test_R²=-124.5239\n",
      "Epoch 913/1200\n",
      "Train loss: 6.3426, Train R²: -124.5387\n",
      "test_loss=5.3569, test_R²=-124.5257\n",
      "Epoch 914/1200\n",
      "Train loss: 6.3153, Train R²: -124.8448\n",
      "test_loss=5.3570, test_R²=-124.5285\n",
      "Epoch 915/1200\n",
      "Train loss: 6.3226, Train R²: -124.4136\n",
      "test_loss=5.3571, test_R²=-124.5326\n",
      "Epoch 916/1200\n",
      "Train loss: 6.3130, Train R²: -125.1957\n",
      "test_loss=5.3572, test_R²=-124.5354\n",
      "Epoch 917/1200\n",
      "Train loss: 6.3172, Train R²: -123.9068\n",
      "test_loss=5.3571, test_R²=-124.5328\n",
      "Epoch 918/1200\n",
      "Train loss: 6.2907, Train R²: -124.1138\n",
      "test_loss=5.3569, test_R²=-124.5290\n",
      "Epoch 919/1200\n",
      "Train loss: 6.3458, Train R²: -124.7300\n",
      "test_loss=5.3569, test_R²=-124.5280\n",
      "Epoch 920/1200\n",
      "Train loss: 6.3393, Train R²: -124.0366\n",
      "test_loss=5.3568, test_R²=-124.5244\n",
      "Epoch 921/1200\n",
      "Train loss: 6.3289, Train R²: -124.9470\n",
      "test_loss=5.3567, test_R²=-124.5220\n",
      "Epoch 922/1200\n",
      "Train loss: 6.3034, Train R²: -124.4465\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 923/1200\n",
      "Train loss: 6.3634, Train R²: -124.8253\n",
      "test_loss=5.3566, test_R²=-124.5178\n",
      "Epoch 924/1200\n",
      "Train loss: 6.3685, Train R²: -125.2427\n",
      "test_loss=5.3566, test_R²=-124.5188\n",
      "Epoch 925/1200\n",
      "Train loss: 6.2768, Train R²: -124.0953\n",
      "test_loss=5.3567, test_R²=-124.5209\n",
      "Epoch 926/1200\n",
      "Train loss: 6.2952, Train R²: -123.6962\n",
      "test_loss=5.3566, test_R²=-124.5201\n",
      "Epoch 927/1200\n",
      "Train loss: 6.3580, Train R²: -126.0857\n",
      "test_loss=5.3565, test_R²=-124.5173\n",
      "Epoch 928/1200\n",
      "Train loss: 6.2955, Train R²: -125.7244\n",
      "test_loss=5.3565, test_R²=-124.5172\n",
      "Epoch 929/1200\n",
      "Train loss: 6.2965, Train R²: -124.5998\n",
      "test_loss=5.3566, test_R²=-124.5194\n",
      "Epoch 930/1200\n",
      "Train loss: 6.3500, Train R²: -124.0046\n",
      "test_loss=5.3566, test_R²=-124.5204\n",
      "Epoch 931/1200\n",
      "Train loss: 6.3428, Train R²: -124.0767\n",
      "test_loss=5.3566, test_R²=-124.5209\n",
      "Epoch 932/1200\n",
      "Train loss: 6.2910, Train R²: -123.9646\n",
      "test_loss=5.3566, test_R²=-124.5213\n",
      "Epoch 933/1200\n",
      "Train loss: 6.3240, Train R²: -124.0014\n",
      "test_loss=5.3566, test_R²=-124.5209\n",
      "Epoch 934/1200\n",
      "Train loss: 6.2832, Train R²: -124.1171\n",
      "test_loss=5.3565, test_R²=-124.5192\n",
      "Epoch 935/1200\n",
      "Train loss: 6.3184, Train R²: -124.1527\n",
      "test_loss=5.3564, test_R²=-124.5149\n",
      "Epoch 936/1200\n",
      "Train loss: 6.3046, Train R²: -124.6401\n",
      "test_loss=5.3564, test_R²=-124.5158\n",
      "Epoch 937/1200\n",
      "Train loss: 6.3258, Train R²: -124.6860\n",
      "test_loss=5.3565, test_R²=-124.5190\n",
      "Epoch 938/1200\n",
      "Train loss: 6.2891, Train R²: -124.5836\n",
      "test_loss=5.3567, test_R²=-124.5232\n",
      "Epoch 939/1200\n",
      "Train loss: 6.2748, Train R²: -124.2227\n",
      "test_loss=5.3568, test_R²=-124.5257\n",
      "Epoch 940/1200\n",
      "Train loss: 6.2942, Train R²: -124.3574\n",
      "test_loss=5.3569, test_R²=-124.5260\n",
      "Epoch 941/1200\n",
      "Train loss: 6.3372, Train R²: -124.1438\n",
      "test_loss=5.3568, test_R²=-124.5241\n",
      "Epoch 942/1200\n",
      "Train loss: 6.3201, Train R²: -124.0669\n",
      "test_loss=5.3568, test_R²=-124.5242\n",
      "Epoch 943/1200\n",
      "Train loss: 6.3371, Train R²: -124.4931\n",
      "test_loss=5.3567, test_R²=-124.5228\n",
      "Epoch 944/1200\n",
      "Train loss: 6.3074, Train R²: -125.2221\n",
      "test_loss=5.3566, test_R²=-124.5195\n",
      "Epoch 945/1200\n",
      "Train loss: 6.3313, Train R²: -123.7427\n",
      "test_loss=5.3564, test_R²=-124.5151\n",
      "Epoch 946/1200\n",
      "Train loss: 6.3337, Train R²: -123.8767\n",
      "test_loss=5.3563, test_R²=-124.5131\n",
      "Epoch 947/1200\n",
      "Train loss: 6.3430, Train R²: -123.7353\n",
      "test_loss=5.3563, test_R²=-124.5116\n",
      "Epoch 948/1200\n",
      "Train loss: 6.3150, Train R²: -123.8587\n",
      "test_loss=5.3562, test_R²=-124.5096\n",
      "Epoch 949/1200\n",
      "Train loss: 6.2800, Train R²: -124.9884\n",
      "test_loss=5.3563, test_R²=-124.5115\n",
      "Epoch 950/1200\n",
      "Train loss: 6.3097, Train R²: -125.6133\n",
      "test_loss=5.3565, test_R²=-124.5149\n",
      "Epoch 951/1200\n",
      "Train loss: 6.2913, Train R²: -124.6397\n",
      "test_loss=5.3565, test_R²=-124.5155\n",
      "Epoch 952/1200\n",
      "Train loss: 6.3155, Train R²: -124.0838\n",
      "test_loss=5.3565, test_R²=-124.5153\n",
      "Epoch 953/1200\n",
      "Train loss: 6.3298, Train R²: -125.6516\n",
      "test_loss=5.3564, test_R²=-124.5137\n",
      "Epoch 954/1200\n",
      "Train loss: 6.3248, Train R²: -124.5799\n",
      "test_loss=5.3563, test_R²=-124.5112\n",
      "Epoch 955/1200\n",
      "Train loss: 6.3030, Train R²: -123.9250\n",
      "test_loss=5.3562, test_R²=-124.5080\n",
      "Epoch 956/1200\n",
      "Train loss: 6.3108, Train R²: -124.7905\n",
      "test_loss=5.3561, test_R²=-124.5037\n",
      "Epoch 957/1200\n",
      "Train loss: 6.3308, Train R²: -124.1047\n",
      "test_loss=5.3560, test_R²=-124.5024\n",
      "Epoch 958/1200\n",
      "Train loss: 6.2977, Train R²: -123.8240\n",
      "test_loss=5.3560, test_R²=-124.5026\n",
      "Epoch 959/1200\n",
      "Train loss: 6.2842, Train R²: -124.4216\n",
      "test_loss=5.3561, test_R²=-124.5043\n",
      "Epoch 960/1200\n",
      "Train loss: 6.2938, Train R²: -124.1008\n",
      "test_loss=5.3562, test_R²=-124.5076\n",
      "Epoch 961/1200\n",
      "Train loss: 6.2893, Train R²: -124.0769\n",
      "test_loss=5.3564, test_R²=-124.5128\n",
      "Epoch 962/1200\n",
      "Train loss: 6.2930, Train R²: -124.5244\n",
      "test_loss=5.3564, test_R²=-124.5124\n",
      "Epoch 963/1200\n",
      "Train loss: 6.3448, Train R²: -123.9938\n",
      "test_loss=5.3564, test_R²=-124.5125\n",
      "Epoch 964/1200\n",
      "Train loss: 6.2999, Train R²: -124.0054\n",
      "test_loss=5.3564, test_R²=-124.5136\n",
      "Epoch 965/1200\n",
      "Train loss: 6.3511, Train R²: -124.5258\n",
      "test_loss=5.3564, test_R²=-124.5123\n",
      "Epoch 966/1200\n",
      "Train loss: 6.3130, Train R²: -124.4010\n",
      "test_loss=5.3562, test_R²=-124.5094\n",
      "Epoch 967/1200\n",
      "Train loss: 6.2885, Train R²: -124.7696\n",
      "test_loss=5.3564, test_R²=-124.5137\n",
      "Epoch 968/1200\n",
      "Train loss: 6.2990, Train R²: -123.9137\n",
      "test_loss=5.3567, test_R²=-124.5197\n",
      "Epoch 969/1200\n",
      "Train loss: 6.3404, Train R²: -124.3912\n",
      "test_loss=5.3568, test_R²=-124.5232\n",
      "Epoch 970/1200\n",
      "Train loss: 6.3541, Train R²: -124.1830\n",
      "test_loss=5.3566, test_R²=-124.5188\n",
      "Epoch 971/1200\n",
      "Train loss: 6.3182, Train R²: -124.6348\n",
      "test_loss=5.3564, test_R²=-124.5142\n",
      "Epoch 972/1200\n",
      "Train loss: 6.3086, Train R²: -124.0430\n",
      "test_loss=5.3563, test_R²=-124.5106\n",
      "Epoch 973/1200\n",
      "Train loss: 6.2973, Train R²: -124.8327\n",
      "test_loss=5.3563, test_R²=-124.5106\n",
      "Epoch 974/1200\n",
      "Train loss: 6.2912, Train R²: -124.3367\n",
      "test_loss=5.3564, test_R²=-124.5117\n",
      "Epoch 975/1200\n",
      "Train loss: 6.2968, Train R²: -124.4944\n",
      "test_loss=5.3565, test_R²=-124.5126\n",
      "Epoch 976/1200\n",
      "Train loss: 6.2957, Train R²: -123.8278\n",
      "test_loss=5.3564, test_R²=-124.5114\n",
      "Epoch 977/1200\n",
      "Train loss: 6.3087, Train R²: -124.6615\n",
      "test_loss=5.3564, test_R²=-124.5107\n",
      "Epoch 978/1200\n",
      "Train loss: 6.3308, Train R²: -124.0181\n",
      "test_loss=5.3564, test_R²=-124.5122\n",
      "Epoch 979/1200\n",
      "Train loss: 6.2986, Train R²: -123.8999\n",
      "test_loss=5.3565, test_R²=-124.5140\n",
      "Epoch 980/1200\n",
      "Train loss: 6.2899, Train R²: -124.2673\n",
      "test_loss=5.3565, test_R²=-124.5148\n",
      "Epoch 981/1200\n",
      "Train loss: 6.3457, Train R²: -125.8344\n",
      "test_loss=5.3563, test_R²=-124.5100\n",
      "Epoch 982/1200\n",
      "Train loss: 6.3428, Train R²: -125.5168\n",
      "test_loss=5.3561, test_R²=-124.5066\n",
      "Epoch 983/1200\n",
      "Train loss: 6.3469, Train R²: -123.7994\n",
      "test_loss=5.3561, test_R²=-124.5053\n",
      "Epoch 984/1200\n",
      "Train loss: 6.3124, Train R²: -124.3755\n",
      "test_loss=5.3559, test_R²=-124.5014\n",
      "Epoch 985/1200\n",
      "Train loss: 6.3130, Train R²: -124.3179\n",
      "test_loss=5.3559, test_R²=-124.4989\n",
      "Epoch 986/1200\n",
      "Train loss: 6.3353, Train R²: -125.3672\n",
      "test_loss=5.3559, test_R²=-124.5001\n",
      "Epoch 987/1200\n",
      "Train loss: 6.3210, Train R²: -124.9162\n",
      "test_loss=5.3561, test_R²=-124.5059\n",
      "Epoch 988/1200\n",
      "Train loss: 6.2962, Train R²: -124.5224\n",
      "test_loss=5.3563, test_R²=-124.5100\n",
      "Epoch 989/1200\n",
      "Train loss: 6.3031, Train R²: -125.1076\n",
      "test_loss=5.3565, test_R²=-124.5148\n",
      "Epoch 990/1200\n",
      "Train loss: 6.2977, Train R²: -124.4506\n",
      "test_loss=5.3565, test_R²=-124.5164\n",
      "Epoch 991/1200\n",
      "Train loss: 6.2986, Train R²: -125.0549\n",
      "test_loss=5.3565, test_R²=-124.5163\n",
      "Epoch 992/1200\n",
      "Train loss: 6.3012, Train R²: -123.9663\n",
      "test_loss=5.3565, test_R²=-124.5149\n",
      "Epoch 993/1200\n",
      "Train loss: 6.3215, Train R²: -125.9219\n",
      "test_loss=5.3563, test_R²=-124.5119\n",
      "Epoch 994/1200\n",
      "Train loss: 6.3143, Train R²: -123.8651\n",
      "test_loss=5.3563, test_R²=-124.5124\n",
      "Epoch 995/1200\n",
      "Train loss: 6.2690, Train R²: -123.9043\n",
      "test_loss=5.3563, test_R²=-124.5115\n",
      "Epoch 996/1200\n",
      "Train loss: 6.3179, Train R²: -124.3985\n",
      "test_loss=5.3562, test_R²=-124.5092\n",
      "Epoch 997/1200\n",
      "Train loss: 6.3405, Train R²: -124.4575\n",
      "test_loss=5.3562, test_R²=-124.5094\n",
      "Epoch 998/1200\n",
      "Train loss: 6.3212, Train R²: -124.0513\n",
      "test_loss=5.3562, test_R²=-124.5092\n",
      "Epoch 999/1200\n",
      "Train loss: 6.3179, Train R²: -124.1639\n",
      "test_loss=5.3560, test_R²=-124.5049\n",
      "Epoch 1000/1200\n",
      "Train loss: 6.3109, Train R²: -123.8195\n",
      "test_loss=5.3559, test_R²=-124.5024\n",
      "Epoch 1001/1200\n",
      "Train loss: 6.3272, Train R²: -125.0528\n",
      "test_loss=5.3559, test_R²=-124.5036\n",
      "Epoch 1002/1200\n",
      "Train loss: 6.3081, Train R²: -124.3503\n",
      "test_loss=5.3560, test_R²=-124.5055\n",
      "Epoch 1003/1200\n",
      "Train loss: 6.3552, Train R²: -123.9322\n",
      "test_loss=5.3561, test_R²=-124.5064\n",
      "Epoch 1004/1200\n",
      "Train loss: 6.3397, Train R²: -124.0028\n",
      "test_loss=5.3561, test_R²=-124.5058\n",
      "Epoch 1005/1200\n",
      "Train loss: 6.3237, Train R²: -124.2565\n",
      "test_loss=5.3561, test_R²=-124.5052\n",
      "Epoch 1006/1200\n",
      "Train loss: 6.2748, Train R²: -123.7520\n",
      "test_loss=5.3562, test_R²=-124.5073\n",
      "Epoch 1007/1200\n",
      "Train loss: 6.3424, Train R²: -124.2742\n",
      "test_loss=5.3561, test_R²=-124.5068\n",
      "Epoch 1008/1200\n",
      "Train loss: 6.3355, Train R²: -124.6352\n",
      "test_loss=5.3561, test_R²=-124.5070\n",
      "Epoch 1009/1200\n",
      "Train loss: 6.2967, Train R²: -123.7941\n",
      "test_loss=5.3561, test_R²=-124.5069\n",
      "Epoch 1010/1200\n",
      "Train loss: 6.3045, Train R²: -124.0390\n",
      "test_loss=5.3562, test_R²=-124.5083\n",
      "Epoch 1011/1200\n",
      "Train loss: 6.3495, Train R²: -123.9981\n",
      "test_loss=5.3562, test_R²=-124.5098\n",
      "Epoch 1012/1200\n",
      "Train loss: 6.2887, Train R²: -124.6127\n",
      "test_loss=5.3563, test_R²=-124.5126\n",
      "Epoch 1013/1200\n",
      "Train loss: 6.3344, Train R²: -124.6959\n",
      "test_loss=5.3566, test_R²=-124.5202\n",
      "Epoch 1014/1200\n",
      "Train loss: 6.2714, Train R²: -124.3405\n",
      "test_loss=5.3569, test_R²=-124.5256\n",
      "Epoch 1015/1200\n",
      "Train loss: 6.2784, Train R²: -125.7735\n",
      "test_loss=5.3568, test_R²=-124.5247\n",
      "Epoch 1016/1200\n",
      "Train loss: 6.3014, Train R²: -124.3743\n",
      "test_loss=5.3567, test_R²=-124.5209\n",
      "Epoch 1017/1200\n",
      "Train loss: 6.2698, Train R²: -124.8902\n",
      "test_loss=5.3565, test_R²=-124.5168\n",
      "Epoch 1018/1200\n",
      "Train loss: 6.2898, Train R²: -124.5652\n",
      "test_loss=5.3564, test_R²=-124.5151\n",
      "Epoch 1019/1200\n",
      "Train loss: 6.3078, Train R²: -124.4919\n",
      "test_loss=5.3564, test_R²=-124.5145\n",
      "Epoch 1020/1200\n",
      "Train loss: 6.2948, Train R²: -124.4434\n",
      "test_loss=5.3565, test_R²=-124.5158\n",
      "Epoch 1021/1200\n",
      "Train loss: 6.3055, Train R²: -124.8334\n",
      "test_loss=5.3566, test_R²=-124.5193\n",
      "Epoch 1022/1200\n",
      "Train loss: 6.2905, Train R²: -124.0494\n",
      "test_loss=5.3566, test_R²=-124.5188\n",
      "Epoch 1023/1200\n",
      "Train loss: 6.2930, Train R²: -123.9271\n",
      "test_loss=5.3565, test_R²=-124.5146\n",
      "Epoch 1024/1200\n",
      "Train loss: 6.3308, Train R²: -124.1012\n",
      "test_loss=5.3564, test_R²=-124.5128\n",
      "Epoch 1025/1200\n",
      "Train loss: 6.3612, Train R²: -124.4685\n",
      "test_loss=5.3563, test_R²=-124.5115\n",
      "Epoch 1026/1200\n",
      "Train loss: 6.2890, Train R²: -124.2472\n",
      "test_loss=5.3562, test_R²=-124.5101\n",
      "Epoch 1027/1200\n",
      "Train loss: 6.3199, Train R²: -124.0110\n",
      "test_loss=5.3562, test_R²=-124.5102\n",
      "Epoch 1028/1200\n",
      "Train loss: 6.3031, Train R²: -124.0038\n",
      "test_loss=5.3562, test_R²=-124.5099\n",
      "Epoch 1029/1200\n",
      "Train loss: 6.3539, Train R²: -124.3997\n",
      "test_loss=5.3563, test_R²=-124.5108\n",
      "Epoch 1030/1200\n",
      "Train loss: 6.3245, Train R²: -125.2217\n",
      "test_loss=5.3564, test_R²=-124.5150\n",
      "Epoch 1031/1200\n",
      "Train loss: 6.3213, Train R²: -124.3884\n",
      "test_loss=5.3564, test_R²=-124.5133\n",
      "Epoch 1032/1200\n",
      "Train loss: 6.2924, Train R²: -124.3312\n",
      "test_loss=5.3563, test_R²=-124.5123\n",
      "Epoch 1033/1200\n",
      "Train loss: 6.2898, Train R²: -123.8676\n",
      "test_loss=5.3563, test_R²=-124.5112\n",
      "Epoch 1034/1200\n",
      "Train loss: 6.3511, Train R²: -124.0846\n",
      "test_loss=5.3563, test_R²=-124.5098\n",
      "Epoch 1035/1200\n",
      "Train loss: 6.3201, Train R²: -124.2111\n",
      "test_loss=5.3562, test_R²=-124.5084\n",
      "Epoch 1036/1200\n",
      "Train loss: 6.3110, Train R²: -124.2274\n",
      "test_loss=5.3563, test_R²=-124.5101\n",
      "Epoch 1037/1200\n",
      "Train loss: 6.3517, Train R²: -124.2064\n",
      "test_loss=5.3565, test_R²=-124.5149\n",
      "Epoch 1038/1200\n",
      "Train loss: 6.3547, Train R²: -124.0010\n",
      "test_loss=5.3566, test_R²=-124.5196\n",
      "Epoch 1039/1200\n",
      "Train loss: 6.2803, Train R²: -124.1386\n",
      "test_loss=5.3568, test_R²=-124.5235\n",
      "Epoch 1040/1200\n",
      "Train loss: 6.3061, Train R²: -123.9229\n",
      "test_loss=5.3569, test_R²=-124.5274\n",
      "Epoch 1041/1200\n",
      "Train loss: 6.3111, Train R²: -124.4145\n",
      "test_loss=5.3570, test_R²=-124.5293\n",
      "Epoch 1042/1200\n",
      "Train loss: 6.3135, Train R²: -124.8251\n",
      "test_loss=5.3571, test_R²=-124.5308\n",
      "Epoch 1043/1200\n",
      "Train loss: 6.3034, Train R²: -123.7742\n",
      "test_loss=5.3569, test_R²=-124.5278\n",
      "Epoch 1044/1200\n",
      "Train loss: 6.3311, Train R²: -124.4457\n",
      "test_loss=5.3569, test_R²=-124.5272\n",
      "Epoch 1045/1200\n",
      "Train loss: 6.3140, Train R²: -123.8285\n",
      "test_loss=5.3569, test_R²=-124.5266\n",
      "Epoch 1046/1200\n",
      "Train loss: 6.3352, Train R²: -124.4128\n",
      "test_loss=5.3569, test_R²=-124.5256\n",
      "Epoch 1047/1200\n",
      "Train loss: 6.3275, Train R²: -124.0426\n",
      "test_loss=5.3568, test_R²=-124.5248\n",
      "Epoch 1048/1200\n",
      "Train loss: 6.3403, Train R²: -123.9910\n",
      "test_loss=5.3569, test_R²=-124.5251\n",
      "Epoch 1049/1200\n",
      "Train loss: 6.3169, Train R²: -123.8655\n",
      "test_loss=5.3568, test_R²=-124.5244\n",
      "Epoch 1050/1200\n",
      "Train loss: 6.3215, Train R²: -125.0803\n",
      "test_loss=5.3568, test_R²=-124.5245\n",
      "Epoch 1051/1200\n",
      "Train loss: 6.3208, Train R²: -123.7458\n",
      "test_loss=5.3568, test_R²=-124.5239\n",
      "Epoch 1052/1200\n",
      "Train loss: 6.2806, Train R²: -123.9522\n",
      "test_loss=5.3567, test_R²=-124.5214\n",
      "Epoch 1053/1200\n",
      "Train loss: 6.3122, Train R²: -124.0181\n",
      "test_loss=5.3567, test_R²=-124.5201\n",
      "Epoch 1054/1200\n",
      "Train loss: 6.3180, Train R²: -124.8209\n",
      "test_loss=5.3567, test_R²=-124.5203\n",
      "Epoch 1055/1200\n",
      "Train loss: 6.3114, Train R²: -123.8090\n",
      "test_loss=5.3566, test_R²=-124.5198\n",
      "Epoch 1056/1200\n",
      "Train loss: 6.3058, Train R²: -124.5943\n",
      "test_loss=5.3566, test_R²=-124.5199\n",
      "Epoch 1057/1200\n",
      "Train loss: 6.3184, Train R²: -124.4082\n",
      "test_loss=5.3567, test_R²=-124.5205\n",
      "Epoch 1058/1200\n",
      "Train loss: 6.3083, Train R²: -124.3219\n",
      "test_loss=5.3566, test_R²=-124.5180\n",
      "Epoch 1059/1200\n",
      "Train loss: 6.3209, Train R²: -123.6514\n",
      "test_loss=5.3565, test_R²=-124.5165\n",
      "Epoch 1060/1200\n",
      "Train loss: 6.2829, Train R²: -124.2521\n",
      "test_loss=5.3565, test_R²=-124.5153\n",
      "Epoch 1061/1200\n",
      "Train loss: 6.3102, Train R²: -124.4226\n",
      "test_loss=5.3564, test_R²=-124.5129\n",
      "Epoch 1062/1200\n",
      "Train loss: 6.3192, Train R²: -124.6242\n",
      "test_loss=5.3564, test_R²=-124.5147\n",
      "Epoch 1063/1200\n",
      "Train loss: 6.3238, Train R²: -125.0483\n",
      "test_loss=5.3566, test_R²=-124.5187\n",
      "Epoch 1064/1200\n",
      "Train loss: 6.2686, Train R²: -124.3490\n",
      "test_loss=5.3567, test_R²=-124.5226\n",
      "Epoch 1065/1200\n",
      "Train loss: 6.3217, Train R²: -123.8126\n",
      "test_loss=5.3568, test_R²=-124.5236\n",
      "Epoch 1066/1200\n",
      "Train loss: 6.3252, Train R²: -123.8846\n",
      "test_loss=5.3567, test_R²=-124.5225\n",
      "Epoch 1067/1200\n",
      "Train loss: 6.2906, Train R²: -125.0712\n",
      "test_loss=5.3565, test_R²=-124.5180\n",
      "Epoch 1068/1200\n",
      "Train loss: 6.3207, Train R²: -123.7984\n",
      "test_loss=5.3564, test_R²=-124.5152\n",
      "Epoch 1069/1200\n",
      "Train loss: 6.3065, Train R²: -123.9606\n",
      "test_loss=5.3563, test_R²=-124.5131\n",
      "Epoch 1070/1200\n",
      "Train loss: 6.2698, Train R²: -125.0663\n",
      "test_loss=5.3563, test_R²=-124.5130\n",
      "Epoch 1071/1200\n",
      "Train loss: 6.3136, Train R²: -124.9196\n",
      "test_loss=5.3564, test_R²=-124.5148\n",
      "Epoch 1072/1200\n",
      "Train loss: 6.3189, Train R²: -124.0581\n",
      "test_loss=5.3565, test_R²=-124.5167\n",
      "Epoch 1073/1200\n",
      "Train loss: 6.3525, Train R²: -124.0803\n",
      "test_loss=5.3565, test_R²=-124.5154\n",
      "Epoch 1074/1200\n",
      "Train loss: 6.2862, Train R²: -124.2893\n",
      "test_loss=5.3564, test_R²=-124.5131\n",
      "Epoch 1075/1200\n",
      "Train loss: 6.3420, Train R²: -123.8077\n",
      "test_loss=5.3563, test_R²=-124.5113\n",
      "Epoch 1076/1200\n",
      "Train loss: 6.3262, Train R²: -124.3351\n",
      "test_loss=5.3563, test_R²=-124.5111\n",
      "Epoch 1077/1200\n",
      "Train loss: 6.3008, Train R²: -125.4631\n",
      "test_loss=5.3565, test_R²=-124.5157\n",
      "Epoch 1078/1200\n",
      "Train loss: 6.3289, Train R²: -124.2731\n",
      "test_loss=5.3566, test_R²=-124.5197\n",
      "Epoch 1079/1200\n",
      "Train loss: 6.3130, Train R²: -124.4950\n",
      "test_loss=5.3567, test_R²=-124.5226\n",
      "Epoch 1080/1200\n",
      "Train loss: 6.3593, Train R²: -125.4764\n",
      "test_loss=5.3569, test_R²=-124.5272\n",
      "Epoch 1081/1200\n",
      "Train loss: 6.2952, Train R²: -124.1760\n",
      "test_loss=5.3569, test_R²=-124.5276\n",
      "Epoch 1082/1200\n",
      "Train loss: 6.3434, Train R²: -123.9220\n",
      "test_loss=5.3569, test_R²=-124.5261\n",
      "Epoch 1083/1200\n",
      "Train loss: 6.3290, Train R²: -124.6189\n",
      "test_loss=5.3570, test_R²=-124.5299\n",
      "Epoch 1084/1200\n",
      "Train loss: 6.3537, Train R²: -124.5691\n",
      "test_loss=5.3571, test_R²=-124.5321\n",
      "Epoch 1085/1200\n",
      "Train loss: 6.3008, Train R²: -124.2298\n",
      "test_loss=5.3571, test_R²=-124.5327\n",
      "Epoch 1086/1200\n",
      "Train loss: 6.3337, Train R²: -124.4762\n",
      "test_loss=5.3572, test_R²=-124.5353\n",
      "Epoch 1087/1200\n",
      "Train loss: 6.3156, Train R²: -124.2313\n",
      "test_loss=5.3570, test_R²=-124.5303\n",
      "Epoch 1088/1200\n",
      "Train loss: 6.2681, Train R²: -123.7645\n",
      "test_loss=5.3569, test_R²=-124.5268\n",
      "Epoch 1089/1200\n",
      "Train loss: 6.3612, Train R²: -125.4026\n",
      "test_loss=5.3568, test_R²=-124.5258\n",
      "Epoch 1090/1200\n",
      "Train loss: 6.3106, Train R²: -123.9390\n",
      "test_loss=5.3568, test_R²=-124.5242\n",
      "Epoch 1091/1200\n",
      "Train loss: 6.3308, Train R²: -125.4166\n",
      "test_loss=5.3566, test_R²=-124.5193\n",
      "Epoch 1092/1200\n",
      "Train loss: 6.3250, Train R²: -124.3297\n",
      "test_loss=5.3564, test_R²=-124.5145\n",
      "Epoch 1093/1200\n",
      "Train loss: 6.3534, Train R²: -123.8870\n",
      "test_loss=5.3563, test_R²=-124.5123\n",
      "Epoch 1094/1200\n",
      "Train loss: 6.3159, Train R²: -123.9683\n",
      "test_loss=5.3563, test_R²=-124.5113\n",
      "Epoch 1095/1200\n",
      "Train loss: 6.2910, Train R²: -125.5404\n",
      "test_loss=5.3563, test_R²=-124.5121\n",
      "Epoch 1096/1200\n",
      "Train loss: 6.3183, Train R²: -124.3112\n",
      "test_loss=5.3563, test_R²=-124.5126\n",
      "Epoch 1097/1200\n",
      "Train loss: 6.3026, Train R²: -124.6031\n",
      "test_loss=5.3564, test_R²=-124.5141\n",
      "Epoch 1098/1200\n",
      "Train loss: 6.3584, Train R²: -124.8852\n",
      "test_loss=5.3564, test_R²=-124.5135\n",
      "Epoch 1099/1200\n",
      "Train loss: 6.3029, Train R²: -123.9112\n",
      "test_loss=5.3562, test_R²=-124.5087\n",
      "Epoch 1100/1200\n",
      "Train loss: 6.2892, Train R²: -123.9394\n",
      "test_loss=5.3560, test_R²=-124.5038\n",
      "Epoch 1101/1200\n",
      "Train loss: 6.3483, Train R²: -123.9155\n",
      "test_loss=5.3559, test_R²=-124.5011\n",
      "Epoch 1102/1200\n",
      "Train loss: 6.2924, Train R²: -123.9738\n",
      "test_loss=5.3559, test_R²=-124.5005\n",
      "Epoch 1103/1200\n",
      "Train loss: 6.3397, Train R²: -123.7339\n",
      "test_loss=5.3558, test_R²=-124.5000\n",
      "Epoch 1104/1200\n",
      "Train loss: 6.2991, Train R²: -124.1411\n",
      "test_loss=5.3558, test_R²=-124.4979\n",
      "Epoch 1105/1200\n",
      "Train loss: 6.2914, Train R²: -124.1540\n",
      "test_loss=5.3559, test_R²=-124.5007\n",
      "Epoch 1106/1200\n",
      "Train loss: 6.2962, Train R²: -124.3560\n",
      "test_loss=5.3559, test_R²=-124.5019\n",
      "Epoch 1107/1200\n",
      "Train loss: 6.2929, Train R²: -123.9948\n",
      "test_loss=5.3560, test_R²=-124.5040\n",
      "Epoch 1108/1200\n",
      "Train loss: 6.3108, Train R²: -124.8896\n",
      "test_loss=5.3562, test_R²=-124.5099\n",
      "Epoch 1109/1200\n",
      "Train loss: 6.3293, Train R²: -124.5125\n",
      "test_loss=5.3563, test_R²=-124.5124\n",
      "Epoch 1110/1200\n",
      "Train loss: 6.3069, Train R²: -125.4962\n",
      "test_loss=5.3565, test_R²=-124.5168\n",
      "Epoch 1111/1200\n",
      "Train loss: 6.3874, Train R²: -124.0350\n",
      "test_loss=5.3566, test_R²=-124.5182\n",
      "Epoch 1112/1200\n",
      "Train loss: 6.3623, Train R²: -124.5229\n",
      "test_loss=5.3564, test_R²=-124.5145\n",
      "Epoch 1113/1200\n",
      "Train loss: 6.3305, Train R²: -124.2718\n",
      "test_loss=5.3563, test_R²=-124.5127\n",
      "Epoch 1114/1200\n",
      "Train loss: 6.2742, Train R²: -124.3304\n",
      "test_loss=5.3562, test_R²=-124.5100\n",
      "Epoch 1115/1200\n",
      "Train loss: 6.3288, Train R²: -125.4856\n",
      "test_loss=5.3562, test_R²=-124.5086\n",
      "Epoch 1116/1200\n",
      "Train loss: 6.3604, Train R²: -123.9811\n",
      "test_loss=5.3562, test_R²=-124.5082\n",
      "Epoch 1117/1200\n",
      "Train loss: 6.2750, Train R²: -124.6244\n",
      "test_loss=5.3562, test_R²=-124.5099\n",
      "Epoch 1118/1200\n",
      "Train loss: 6.3273, Train R²: -124.1067\n",
      "test_loss=5.3563, test_R²=-124.5114\n",
      "Epoch 1119/1200\n",
      "Train loss: 6.2819, Train R²: -124.3829\n",
      "test_loss=5.3564, test_R²=-124.5136\n",
      "Epoch 1120/1200\n",
      "Train loss: 6.3116, Train R²: -124.2737\n",
      "test_loss=5.3565, test_R²=-124.5157\n",
      "Epoch 1121/1200\n",
      "Train loss: 6.3130, Train R²: -124.1768\n",
      "test_loss=5.3564, test_R²=-124.5136\n",
      "Epoch 1122/1200\n",
      "Train loss: 6.3064, Train R²: -124.2668\n",
      "test_loss=5.3563, test_R²=-124.5115\n",
      "Epoch 1123/1200\n",
      "Train loss: 6.3655, Train R²: -124.3799\n",
      "test_loss=5.3562, test_R²=-124.5094\n",
      "Epoch 1124/1200\n",
      "Train loss: 6.2816, Train R²: -124.0670\n",
      "test_loss=5.3562, test_R²=-124.5086\n",
      "Epoch 1125/1200\n",
      "Train loss: 6.2986, Train R²: -124.2262\n",
      "test_loss=5.3562, test_R²=-124.5097\n",
      "Epoch 1126/1200\n",
      "Train loss: 6.2828, Train R²: -124.3307\n",
      "test_loss=5.3562, test_R²=-124.5096\n",
      "Epoch 1127/1200\n",
      "Train loss: 6.2864, Train R²: -123.9830\n",
      "test_loss=5.3562, test_R²=-124.5110\n",
      "Epoch 1128/1200\n",
      "Train loss: 6.3038, Train R²: -124.3632\n",
      "test_loss=5.3563, test_R²=-124.5129\n",
      "Epoch 1129/1200\n",
      "Train loss: 6.2938, Train R²: -126.2671\n",
      "test_loss=5.3566, test_R²=-124.5192\n",
      "Epoch 1130/1200\n",
      "Train loss: 6.3442, Train R²: -124.3083\n",
      "test_loss=5.3568, test_R²=-124.5241\n",
      "Epoch 1131/1200\n",
      "Train loss: 6.3190, Train R²: -124.1186\n",
      "test_loss=5.3570, test_R²=-124.5283\n",
      "Epoch 1132/1200\n",
      "Train loss: 6.3035, Train R²: -124.2314\n",
      "test_loss=5.3569, test_R²=-124.5262\n",
      "Epoch 1133/1200\n",
      "Train loss: 6.3121, Train R²: -124.5017\n",
      "test_loss=5.3569, test_R²=-124.5256\n",
      "Epoch 1134/1200\n",
      "Train loss: 6.2977, Train R²: -124.4825\n",
      "test_loss=5.3568, test_R²=-124.5228\n",
      "Epoch 1135/1200\n",
      "Train loss: 6.3293, Train R²: -124.7934\n",
      "test_loss=5.3567, test_R²=-124.5210\n",
      "Epoch 1136/1200\n",
      "Train loss: 6.3180, Train R²: -124.0011\n",
      "test_loss=5.3569, test_R²=-124.5259\n",
      "Epoch 1137/1200\n",
      "Train loss: 6.3246, Train R²: -125.5050\n",
      "test_loss=5.3571, test_R²=-124.5319\n",
      "Epoch 1138/1200\n",
      "Train loss: 6.3261, Train R²: -124.2833\n",
      "test_loss=5.3572, test_R²=-124.5343\n",
      "Epoch 1139/1200\n",
      "Train loss: 6.3059, Train R²: -124.5970\n",
      "test_loss=5.3571, test_R²=-124.5313\n",
      "Epoch 1140/1200\n",
      "Train loss: 6.3324, Train R²: -124.9915\n",
      "test_loss=5.3570, test_R²=-124.5298\n",
      "Epoch 1141/1200\n",
      "Train loss: 6.2976, Train R²: -123.9495\n",
      "test_loss=5.3570, test_R²=-124.5293\n",
      "Epoch 1142/1200\n",
      "Train loss: 6.2823, Train R²: -124.9874\n",
      "test_loss=5.3570, test_R²=-124.5284\n",
      "Epoch 1143/1200\n",
      "Train loss: 6.3135, Train R²: -123.8039\n",
      "test_loss=5.3570, test_R²=-124.5307\n",
      "Epoch 1144/1200\n",
      "Train loss: 6.3021, Train R²: -124.3473\n",
      "test_loss=5.3570, test_R²=-124.5302\n",
      "Epoch 1145/1200\n",
      "Train loss: 6.3266, Train R²: -124.8697\n",
      "test_loss=5.3571, test_R²=-124.5318\n",
      "Epoch 1146/1200\n",
      "Train loss: 6.3335, Train R²: -123.8846\n",
      "test_loss=5.3570, test_R²=-124.5306\n",
      "Epoch 1147/1200\n",
      "Train loss: 6.3277, Train R²: -123.7747\n",
      "test_loss=5.3569, test_R²=-124.5273\n",
      "Epoch 1148/1200\n",
      "Train loss: 6.2943, Train R²: -124.2327\n",
      "test_loss=5.3568, test_R²=-124.5245\n",
      "Epoch 1149/1200\n",
      "Train loss: 6.3378, Train R²: -125.2346\n",
      "test_loss=5.3567, test_R²=-124.5225\n",
      "Epoch 1150/1200\n",
      "Train loss: 6.3710, Train R²: -124.1055\n",
      "test_loss=5.3566, test_R²=-124.5208\n",
      "Epoch 1151/1200\n",
      "Train loss: 6.3273, Train R²: -124.5839\n",
      "test_loss=5.3566, test_R²=-124.5194\n",
      "Epoch 1152/1200\n",
      "Train loss: 6.3084, Train R²: -124.9452\n",
      "test_loss=5.3569, test_R²=-124.5264\n",
      "Epoch 1153/1200\n",
      "Train loss: 6.3064, Train R²: -123.7910\n",
      "test_loss=5.3570, test_R²=-124.5306\n",
      "Epoch 1154/1200\n",
      "Train loss: 6.3027, Train R²: -124.7869\n",
      "test_loss=5.3569, test_R²=-124.5289\n",
      "Epoch 1155/1200\n",
      "Train loss: 6.2874, Train R²: -124.1794\n",
      "test_loss=5.3568, test_R²=-124.5258\n",
      "Epoch 1156/1200\n",
      "Train loss: 6.3129, Train R²: -124.4713\n",
      "test_loss=5.3567, test_R²=-124.5219\n",
      "Epoch 1157/1200\n",
      "Train loss: 6.2894, Train R²: -123.8225\n",
      "test_loss=5.3565, test_R²=-124.5181\n",
      "Epoch 1158/1200\n",
      "Train loss: 6.3065, Train R²: -124.5495\n",
      "test_loss=5.3563, test_R²=-124.5126\n",
      "Epoch 1159/1200\n",
      "Train loss: 6.3106, Train R²: -124.0328\n",
      "test_loss=5.3561, test_R²=-124.5081\n",
      "Epoch 1160/1200\n",
      "Train loss: 6.3035, Train R²: -123.9594\n",
      "test_loss=5.3560, test_R²=-124.5053\n",
      "Epoch 1161/1200\n",
      "Train loss: 6.3081, Train R²: -124.1279\n",
      "test_loss=5.3559, test_R²=-124.5031\n",
      "Epoch 1162/1200\n",
      "Train loss: 6.3310, Train R²: -125.0122\n",
      "test_loss=5.3559, test_R²=-124.5019\n",
      "Epoch 1163/1200\n",
      "Train loss: 6.3171, Train R²: -125.0355\n",
      "test_loss=5.3559, test_R²=-124.5029\n",
      "Epoch 1164/1200\n",
      "Train loss: 6.2711, Train R²: -124.7075\n",
      "test_loss=5.3559, test_R²=-124.5020\n",
      "Epoch 1165/1200\n",
      "Train loss: 6.3595, Train R²: -123.8374\n",
      "test_loss=5.3558, test_R²=-124.5008\n",
      "Epoch 1166/1200\n",
      "Train loss: 6.3272, Train R²: -125.5206\n",
      "test_loss=5.3558, test_R²=-124.4998\n",
      "Epoch 1167/1200\n",
      "Train loss: 6.2954, Train R²: -124.1693\n",
      "test_loss=5.3559, test_R²=-124.5004\n",
      "Epoch 1168/1200\n",
      "Train loss: 6.3426, Train R²: -124.0149\n",
      "test_loss=5.3559, test_R²=-124.5018\n",
      "Epoch 1169/1200\n",
      "Train loss: 6.2847, Train R²: -125.5712\n",
      "test_loss=5.3559, test_R²=-124.5036\n",
      "Epoch 1170/1200\n",
      "Train loss: 6.3532, Train R²: -124.0017\n",
      "test_loss=5.3559, test_R²=-124.5034\n",
      "Epoch 1171/1200\n",
      "Train loss: 6.3273, Train R²: -123.7928\n",
      "test_loss=5.3560, test_R²=-124.5044\n",
      "Epoch 1172/1200\n",
      "Train loss: 6.3146, Train R²: -124.8802\n",
      "test_loss=5.3561, test_R²=-124.5072\n",
      "Epoch 1173/1200\n",
      "Train loss: 6.3258, Train R²: -125.3487\n",
      "test_loss=5.3563, test_R²=-124.5122\n",
      "Epoch 1174/1200\n",
      "Train loss: 6.2966, Train R²: -125.0181\n",
      "test_loss=5.3564, test_R²=-124.5145\n",
      "Epoch 1175/1200\n",
      "Train loss: 6.2935, Train R²: -124.1033\n",
      "test_loss=5.3565, test_R²=-124.5171\n",
      "Epoch 1176/1200\n",
      "Train loss: 6.3090, Train R²: -123.9538\n",
      "test_loss=5.3566, test_R²=-124.5200\n",
      "Epoch 1177/1200\n",
      "Train loss: 6.3034, Train R²: -124.9892\n",
      "test_loss=5.3567, test_R²=-124.5225\n",
      "Epoch 1178/1200\n",
      "Train loss: 6.3545, Train R²: -124.1986\n",
      "test_loss=5.3566, test_R²=-124.5218\n",
      "Epoch 1179/1200\n",
      "Train loss: 6.2900, Train R²: -124.1105\n",
      "test_loss=5.3566, test_R²=-124.5197\n",
      "Epoch 1180/1200\n",
      "Train loss: 6.3116, Train R²: -125.0399\n",
      "test_loss=5.3565, test_R²=-124.5187\n",
      "Epoch 1181/1200\n",
      "Train loss: 6.3050, Train R²: -123.7696\n",
      "test_loss=5.3567, test_R²=-124.5230\n",
      "Epoch 1182/1200\n",
      "Train loss: 6.3089, Train R²: -124.5470\n",
      "test_loss=5.3568, test_R²=-124.5244\n",
      "Epoch 1183/1200\n",
      "Train loss: 6.3037, Train R²: -124.0613\n",
      "test_loss=5.3568, test_R²=-124.5259\n",
      "Epoch 1184/1200\n",
      "Train loss: 6.3375, Train R²: -123.8060\n",
      "test_loss=5.3568, test_R²=-124.5250\n",
      "Epoch 1185/1200\n",
      "Train loss: 6.2759, Train R²: -124.1552\n",
      "test_loss=5.3568, test_R²=-124.5243\n",
      "Epoch 1186/1200\n",
      "Train loss: 6.3369, Train R²: -124.6478\n",
      "test_loss=5.3567, test_R²=-124.5237\n",
      "Epoch 1187/1200\n",
      "Train loss: 6.3290, Train R²: -124.8551\n",
      "test_loss=5.3566, test_R²=-124.5208\n",
      "Epoch 1188/1200\n",
      "Train loss: 6.3253, Train R²: -123.7565\n",
      "test_loss=5.3565, test_R²=-124.5184\n",
      "Epoch 1189/1200\n",
      "Train loss: 6.2954, Train R²: -124.2451\n",
      "test_loss=5.3566, test_R²=-124.5210\n",
      "Epoch 1190/1200\n",
      "Train loss: 6.2942, Train R²: -125.4890\n",
      "test_loss=5.3567, test_R²=-124.5221\n",
      "Epoch 1191/1200\n",
      "Train loss: 6.3257, Train R²: -124.7761\n",
      "test_loss=5.3569, test_R²=-124.5268\n",
      "Epoch 1192/1200\n",
      "Train loss: 6.2790, Train R²: -125.5646\n",
      "test_loss=5.3571, test_R²=-124.5328\n",
      "Epoch 1193/1200\n",
      "Train loss: 6.3360, Train R²: -123.8845\n",
      "test_loss=5.3572, test_R²=-124.5333\n",
      "Epoch 1194/1200\n",
      "Train loss: 6.3331, Train R²: -123.8364\n",
      "test_loss=5.3572, test_R²=-124.5325\n",
      "Epoch 1195/1200\n",
      "Train loss: 6.3360, Train R²: -124.2389\n",
      "test_loss=5.3571, test_R²=-124.5304\n",
      "Epoch 1196/1200\n",
      "Train loss: 6.3012, Train R²: -124.5902\n",
      "test_loss=5.3569, test_R²=-124.5259\n",
      "Epoch 1197/1200\n",
      "Train loss: 6.2734, Train R²: -124.8172\n",
      "test_loss=5.3567, test_R²=-124.5201\n",
      "Epoch 1198/1200\n",
      "Train loss: 6.3167, Train R²: -123.7594\n",
      "test_loss=5.3565, test_R²=-124.5168\n",
      "Epoch 1199/1200\n",
      "Train loss: 6.3545, Train R²: -124.5631\n",
      "test_loss=5.3565, test_R²=-124.5164\n",
      "Epoch 1200/1200\n",
      "Train loss: 6.3425, Train R²: -123.8912\n",
      "test_loss=5.3566, test_R²=-124.5190\n"
     ]
    }
   ],
   "source": [
    "metrics = model.fit(dl_train, dl_test, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 metrics 字典已经有 'train_loss' 和 'test_loss'\n",
    "# 这些是从 trainer.fit() 返回的\n",
    "train_loss = metrics[\"train_loss\"]\n",
    "test_loss = metrics[\"test_loss\"]\n",
    "\n",
    "# 画出训练损失和测试损失的曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, label=\"Train Loss\")\n",
    "plt.plot(test_loss, label=\"Test Loss\")\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title(\"Training and Testing Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01163047, 0.00232218, 0.6961326 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01163047, 0.00232218, 0.6961326 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.10086137, 0.09390121, 0.03867403, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.01455339, 0.00300526, 0.26519337, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00476716, 0.00174412, 0.27071823, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00424864, 0.00119572, 0.48066298, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_table.drop(columns=\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIjCAYAAAAJLyrXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB06UlEQVR4nO3dd3gU5d7G8Xt3k2x6QnoCSei9CghRFASUZkGx0/RgQ7CABVGPCB5FURE7Ho8CIogHXyuCilQFpPcmhBIgnUB633n/COwxUjck2ZTv57r2SuaZZ5/5zTrE3JmZZ0yGYRgCAAAAAFw0s7MLAAAAAIDqhiAFAAAAAA4iSAEAAACAgwhSAAAAAOAgghQAAAAAOIggBQAAAAAOIkgBAAAAgIMIUgAAAADgIIIUAAAAADiIIAUAleDFF1+UyWSqlG316NFDPXr0sC8vX75cJpNJX331VaVs/5577lH9+vUrZVtllZWVpfvuu09hYWEymUx6/PHHK2W71eGzAQBcHIIUADho5syZMplM9pe7u7siIiLUp08fvfPOO8rMzCyX7cTHx+vFF1/Uli1bymW88lSVa7sYr7zyimbOnKmRI0dq9uzZGjp0qLNLAgBUMy7OLgAAqqtJkyapQYMGKiwsVGJiopYvX67HH39cU6dO1ffff6+2bdva+z7//PN65plnHBo/Pj5eEydOVP369dW+ffuLft8vv/zi0HbK4ny1ffzxx7LZbBVew6VYunSpunbtqgkTJji7FABANUWQAoAy6tevnzp16mRfHj9+vJYuXarrr79eN954o3bv3i0PDw9JkouLi1xcKvZHbk5Ojjw9PeXm5lah27kQV1dXp27/YiQnJ6tly5bOLgMAUI1xaR8AlKOePXvqn//8pw4fPqzPP//c3n62e6QWL16sbt26yd/fX97e3mrWrJmeffZZSSX3NXXu3FmSdO+999ovI5w5c6akkvugWrdurY0bN+rqq6+Wp6en/b1/v0fqtOLiYj377LMKCwuTl5eXbrzxRh05cqRUn/r16+uee+45471/HfNCtZ3tPqDs7Gw98cQTioyMlNVqVbNmzfTGG2/IMIxS/Uwmk0aPHq1vv/1WrVu3ltVqVatWrfTTTz+d/QP/m+TkZI0YMUKhoaFyd3dXu3btNGvWLPv60/eLHTx4UD/++KO99kOHDp133M8//1yXX365PD09VadOHV199dVnnPn74IMP1KpVK1mtVkVERGjUqFE6efLkecc9Xc/y5ctLtR86dKjUZyqVfK7e3t6Ki4vT9ddfL29vb9WtW1fvv/++JGn79u3q2bOnvLy8FB0drblz55Ya8/QlqatWrdLYsWMVHBwsLy8v3XzzzUpJSTlvnX93+njev3+/7rnnHvn7+8vPz0/33nuvcnJySvWdMWOGevbsqZCQEFmtVrVs2VIffvjhGWPWr19f119/vZYvX65OnTrJw8NDbdq0sX82X3/9tdq0aSN3d3d17NhRmzdvPmOMPXv26NZbb1VAQIDc3d3VqVMnff/99w7tGwBcLIIUAJSz0/fbnO8Su507d+r6669Xfn6+Jk2apDfffFM33nijVq1aJUlq0aKFJk2aJEl64IEHNHv2bM2ePVtXX321fYzjx4+rX79+at++vaZNm6ZrrrnmvHW9/PLL+vHHHzVu3Dg9+uijWrx4sXr37q3c3FyH9u9iavsrwzB044036q233lLfvn01depUNWvWTE899ZTGjh17Rv/ff/9dDz/8sO68805NmTJFeXl5GjRokI4fP37eunJzc9WjRw/Nnj1bgwcP1uuvvy4/Pz/dc889evvtt+21z549W0FBQWrfvr299uDg4HOOO3HiRA0dOlSurq6aNGmSJk6cqMjISC1dutTe58UXX9SoUaMUERGhN998U4MGDdJHH32k6667ToWFhRf8TC9WcXGx+vXrp8jISE2ZMkX169fX6NGjNXPmTPXt21edOnXSa6+9Jh8fHw0bNkwHDx48Y4xHHnlEW7du1YQJEzRy5Ej98MMPGj16dJnquf3225WZmanJkyfr9ttv18yZMzVx4sRSfT788ENFR0fr2Wef1ZtvvqnIyEg9/PDD9gD4V/v379fdd9+tG264QZMnT9aJEyd0ww03aM6cORozZoyGDBmiiRMnKjY2VrfffnupS0h37typrl27avfu3XrmmWf05ptvysvLSwMHDtQ333xTpv0DgPMyAAAOmTFjhiHJWL9+/Tn7+Pn5GR06dLAvT5gwwfjrj9y33nrLkGSkpKScc4z169cbkowZM2acsa579+6GJGP69OlnXde9e3f78rJlywxJRt26dY2MjAx7+3//+19DkvH222/b26Kjo43hw4dfcMzz1TZ8+HAjOjravvztt98akox//etfpfrdeuuthslkMvbv329vk2S4ubmVatu6dashyXj33XfP2NZfTZs2zZBkfP755/a2goICIyYmxvD29i6179HR0caAAQPOO55hGMa+ffsMs9ls3HzzzUZxcXGpdTabzTAMw0hOTjbc3NyM6667rlSf9957z5BkfPrpp/a2v382p//bLFu2rNTYBw8ePOPzHT58uCHJeOWVV+xtJ06cMDw8PAyTyWTMmzfP3r5nzx5DkjFhwgR72+njtnfv3vbaDcMwxowZY1gsFuPkyZMX/DxOO308/+Mf/yjVfvPNNxuBgYGl2nJycs54f58+fYyGDRuWaouOjjYkGatXr7a3/fzzz4Ykw8PDwzh8+LC9/aOPPjrjc+vVq5fRpk0bIy8vz95ms9mMK664wmjSpMlF7xsAXCzOSAFABfD29j7v7H3+/v6SpO+++67MEzNYrVbde++9F91/2LBh8vHxsS/feuutCg8P18KFC8u0/Yu1cOFCWSwWPfroo6Xan3jiCRmGoUWLFpVq7927txo1amRfbtu2rXx9fXXgwIELbicsLEx33XWXvc3V1VWPPvqosrKytGLFCodr//bbb2Wz2fTCCy/IbC79v8zTl2r++uuvKigo0OOPP16qz/333y9fX1/9+OOPDm/3fO677z779/7+/mrWrJm8vLx0++2329ubNWsmf3//s35mDzzwQKnLTK+66ioVFxfr8OHDDtfy0EMPlVq+6qqrdPz4cWVkZNjbTt8nKEnp6elKTU1V9+7ddeDAAaWnp5d6f8uWLRUTE2Nf7tKli6SSS2ajoqLOaD+9f2lpaVq6dKn9DFlqaqpSU1N1/Phx9enTR/v27dOxY8cc3j8AOB+CFABUgKysrFKh5e/uuOMOXXnllbrvvvsUGhqqO++8U//9738dClV169Z1aGKJJk2alFo2mUxq3LjxBe8PulSHDx9WRETEGZ9HixYt7Ov/6q+/MJ9Wp04dnThx4oLbadKkyRmB51zbuRixsbEym83nnZji9LjNmjUr1e7m5qaGDRuWabvn4u7ufsZliH5+fqpXr94Z9+D5+fmd9TP7++dbp04dSbrg53s2FzPWqlWr1Lt3b3l5ecnf31/BwcH2+/n+HqT+Pp6fn58kKTIy8qztp7ezf/9+GYahf/7znwoODi71Oj0zY3JyssP7BwDnw6x9AFDOjh49qvT0dDVu3PicfTw8PLRy5UotW7ZMP/74o3766Sd9+eWX6tmzp3755RdZLJYLbuevf+kvL+d6aHBxcfFF1VQezrUd428TU9QE5/u8z+Zcn40jn1l5fr4XGis2Nla9evVS8+bNNXXqVEVGRsrNzU0LFy7UW2+9dcYfDsq6f6fHefLJJ9WnT5+z9j3fv0cAKAuCFACUs9mzZ0vSOX+hO81sNqtXr17q1auXpk6dqldeeUXPPfecli1bpt69e5/zl+yy2rdvX6llwzC0f//+Us+7qlOnzllnmjt8+LAaNmxoX3aktujoaP3666/KzMwsdVZqz5499vXlITo6Wtu2bZPNZit1VupSttOoUSPZbDbt2rXrnM/yOj3u3r17S31GBQUFOnjwoHr37n3O8U+fwfn7Z16eZ7Gc6YcfflB+fr6+//77Umebli1bVq7bOf25u7q6nvfzBoDyxKV9AFCOli5dqpdeekkNGjTQ4MGDz9kvLS3tjLbTv6jn5+dLkry8vCSd+Ut2WX322Wel7tv66quvlJCQoH79+tnbGjVqpD/++EMFBQX2tgULFpwxTbojtfXv31/FxcV67733SrW/9dZbMplMpbZ/Kfr376/ExER9+eWX9raioiK9++678vb2Vvfu3R0ec+DAgTKbzZo0adIZZ09Onw3p3bu33Nzc9M4775Q6q/PJJ58oPT1dAwYMOOf40dHRslgsWrlyZan2Dz74wOFaq6LTZ5L++rmkp6drxowZ5bqdkJAQ9ejRQx999JESEhLOWO/o9O4AcDE4IwUAZbRo0SLt2bNHRUVFSkpK0tKlS7V48WJFR0fr+++/l7u7+znfO2nSJK1cuVIDBgxQdHS0kpOT9cEHH6hevXrq1q2bpJJQ4+/vr+nTp8vHx0deXl7q0qWLGjRoUKZ6AwIC1K1bN917771KSkrStGnT1LhxY91///32Pvfdd5+++uor9e3bV7fffrtiY2P1+eefl5r8wdHabrjhBl1zzTV67rnndOjQIbVr106//PKLvvvuOz3++ONnjF1WDzzwgD766CPdc8892rhxo+rXr6+vvvpKq1at0rRp0857z9q5NG7cWM8995xeeuklXXXVVbrllltktVq1fv16RUREaPLkyQoODtb48eM1ceJE9e3bVzfeeKP27t2rDz74QJ07d9aQIUPOOb6fn59uu+02vfvuuzKZTGrUqJEWLFhQY+7nue666+Tm5qYbbrhBDz74oLKysvTxxx8rJCTkrIHnUrz//vvq1q2b2rRpo/vvv18NGzZUUlKS1qxZo6NHj2rr1q3luj0AIEgBQBm98MILkkomFQgICFCbNm00bdo03XvvvRf8pf3GG2/UoUOH9Omnnyo1NVVBQUHq3r27Jk6caL+R3tXVVbNmzdL48eP10EMPqaioSDNmzChzkHr22We1bds2TZ48WZmZmerVq5c++OADeXp62vv06dNHb775pqZOnarHH39cnTp10oIFC/TEE0+UGsuR2sxms77//nu98MIL+vLLLzVjxgzVr19fr7/++hnjXgoPDw8tX75czzzzjGbNmqWMjAw1a9ZMM2bMOOtDhi/WpEmT1KBBA7377rt67rnn5OnpqbZt29qfFyaVPEcqODhY7733nsaMGaOAgAA98MADeuWVV+Tq6nre8d99910VFhZq+vTpslqtuv322/X666+rdevWZa65qmjWrJm++uorPf/883ryyScVFhamkSNHKjg4WP/4xz/KdVstW7bUhg0bNHHiRM2cOVPHjx9XSEiIOnToYP+3CgDlyWTUxLt3AQAAAKACcY8UAAAAADiIS/sAAICkkuefZWVlnbdPcHBwpU2FDwBVGUEKAABIkt544w1NnDjxvH0OHjyo+vXrV05BAFCFVZlL+1599VWZTCY9/vjj9ra8vDyNGjVKgYGB8vb21qBBg5SUlFTqfXFxcRowYIA8PT0VEhKip556SkVFRZVcPQAA1d+wYcO0ePHi877CwsKcXSYAVAlV4ozU+vXr9dFHH5V6KKQkjRkzRj/++KPmz58vPz8/jR49WrfccotWrVolqeTJ7wMGDFBYWJhWr16thIQEDRs2TK6urnrllVecsSsAAFRbDRs2LPVQYQDAuTl91r6srCxddtll+uCDD/Svf/1L7du317Rp05Senq7g4GDNnTtXt956q6SSp9O3aNFCa9asUdeuXbVo0SJdf/31io+PV2hoqCRp+vTpGjdunFJSUuTm5ubMXQMAAABQQzn9jNSoUaM0YMAA9e7dW//617/s7Rs3blRhYaF69+5tb2vevLmioqLsQWrNmjVq06aNPURJJc9AGTlypHbu3KkOHTqcdZv5+fnKz8+3L9tsNqWlpSkwMFAmk6kC9hIAAABAdWAYhjIzMxURESGz+dx3Qjk1SM2bN0+bNm3S+vXrz1iXmJgoNzc3+fv7l2oPDQ1VYmKivc9fQ9Tp9afXncvkyZMveDMtAAAAgNrryJEjqlev3jnXOy1IHTlyRI899pgWL14sd3f3St32+PHjNXbsWPtyenq6oqKidOTIEfn6+lZqLQAAAACqjoyMDEVGRsrHx+e8/ZwWpDZu3Kjk5GRddtll9rbi4mKtXLlS7733nn7++WcVFBTo5MmTpc5KJSUl2WcMCgsL07p160qNe3pWv/PNKmS1WmW1Ws9o9/X1JUgBAAAAuOAtP06b/rxXr17avn27tmzZYn916tRJgwcPtn/v6uqqJUuW2N+zd+9excXFKSYmRpIUExOj7du3Kzk52d5n8eLF8vX1VcuWLSt9nwAAAADUDk47I+Xj46PWrVuXavPy8lJgYKC9fcSIERo7dqwCAgLk6+urRx55RDExMeratask6brrrlPLli01dOhQTZkyRYmJiXr++ec1atSos55xAgAAAIDy4PRZ+87nrbfektls1qBBg5Sfn68+ffrogw8+sK+3WCxasGCBRo4cqZiYGHl5eWn48OGaNGmSE6sGAAAAUNM5/TlSVUFGRob8/PyUnp7OPVIAAABALXax2cBp90gBAAAAQHVFkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEFODVIffvih2rZtK19fX/n6+iomJkaLFi2yr+/Ro4dMJlOp10MPPVRqjLi4OA0YMECenp4KCQnRU089paKiosreFQAAAAC1iIszN16vXj29+uqratKkiQzD0KxZs3TTTTdp8+bNatWqlSTp/vvv16RJk+zv8fT0tH9fXFysAQMGKCwsTKtXr1ZCQoKGDRsmV1dXvfLKK5W+PwAAAABqB5NhGIazi/irgIAAvf766xoxYoR69Oih9u3ba9q0aWftu2jRIl1//fWKj49XaGioJGn69OkaN26cUlJS5Obmdtb35efnKz8/376ckZGhyMhIpaeny9fXt9z3CQAAAED1kJGRIT8/vwtmgypzj1RxcbHmzZun7OxsxcTE2NvnzJmjoKAgtW7dWuPHj1dOTo593Zo1a9SmTRt7iJKkPn36KCMjQzt37jzntiZPniw/Pz/7KzIysmJ2CgAAAECN5NRL+yRp+/btiomJUV5enry9vfXNN9+oZcuWkqS7775b0dHRioiI0LZt2zRu3Djt3btXX3/9tSQpMTGxVIiSZF9OTEw85zbHjx+vsWPH2pdPn5ECAAAAgIvh9CDVrFkzbdmyRenp6frqq680fPhwrVixQi1bttQDDzxg79emTRuFh4erV69eio2NVaNGjcq8TavVKqvVWh7lAwAAAKiFnH5pn5ubmxo3bqyOHTtq8uTJateund5+++2z9u3SpYskaf/+/ZKksLAwJSUllepzejksLKwCqwYAAABQmzk9SP2dzWYrNRHEX23ZskWSFB4eLkmKiYnR9u3blZycbO+zePFi+fr62i8PBAAAAIDy5tRL+8aPH69+/fopKipKmZmZmjt3rpYvX66ff/5ZsbGxmjt3rvr376/AwEBt27ZNY8aM0dVXX622bdtKkq677jq1bNlSQ4cO1ZQpU5SYmKjnn39eo0aN4tI9AAAAABXGqUEqOTlZw4YNU0JCgvz8/NS2bVv9/PPPuvbaa3XkyBH9+uuvmjZtmrKzsxUZGalBgwbp+eeft7/fYrFowYIFGjlypGJiYuTl5aXhw4eXeu4UAAAAAJS3KvccKWe42LniAQAAANRs1e45UgAAAABQXRCkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEFODVIffvih2rZtK19fX/n6+iomJkaLFi2yr8/Ly9OoUaMUGBgob29vDRo0SElJSaXGiIuL04ABA+Tp6amQkBA99dRTKioqquxdAQAAAFCLODVI1atXT6+++qo2btyoDRs2qGfPnrrpppu0c+dOSdKYMWP0ww8/aP78+VqxYoXi4+N1yy232N9fXFysAQMGqKCgQKtXr9asWbM0c+ZMvfDCC87aJQAAAAC1gMkwDMPZRfxVQECAXn/9dd16660KDg7W3Llzdeutt0qS9uzZoxYtWmjNmjXq2rWrFi1apOuvv17x8fEKDQ2VJE2fPl3jxo1TSkqK3NzcLmqbGRkZ8vPzU3p6unx9fSts3wAAAABUbRebDarMPVLFxcWaN2+esrOzFRMTo40bN6qwsFC9e/e292nevLmioqK0Zs0aSdKaNWvUpk0be4iSpD59+igjI8N+Vuts8vPzlZGRUeoFAAAAABfL6UFq+/bt8vb2ltVq1UMPPaRvvvlGLVu2VGJiotzc3OTv71+qf2hoqBITEyVJiYmJpULU6fWn153L5MmT5efnZ39FRkaW704BAAAAqNGcHqSaNWumLVu2aO3atRo5cqSGDx+uXbt2Veg2x48fr/T0dPvryJEjFbo9AAAAADWLi7MLcHNzU+PGjSVJHTt21Pr16/X222/rjjvuUEFBgU6ePFnqrFRSUpLCwsIkSWFhYVq3bl2p8U7P6ne6z9lYrVZZrdZy3hMAAAAAtYXTz0j9nc1mU35+vjp27ChXV1ctWbLEvm7v3r2Ki4tTTEyMJCkmJkbbt29XcnKyvc/ixYvl6+urli1bVnrtAAAAAGoHp56RGj9+vPr166eoqChlZmZq7ty5Wr58uX7++Wf5+flpxIgRGjt2rAICAuTr66tHHnlEMTEx6tq1qyTpuuuuU8uWLTV06FBNmTJFiYmJev755zVq1CjOOAEAAACoME4NUsnJyRo2bJgSEhLk5+entm3b6ueff9a1114rSXrrrbdkNps1aNAg5efnq0+fPvrggw/s77dYLFqwYIFGjhypmJgYeXl5afjw4Zo0aZKzdgkAAABALVDlniPlDDxHCgAAAIBUDZ8jBQAAAADVBUEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHOTVITZ48WZ07d5aPj49CQkI0cOBA7d27t1SfHj16yGQylXo99NBDpfrExcVpwIAB8vT0VEhIiJ566ikVFRVV5q4AAAAAqEVcnLnxFStWaNSoUercubOKior07LPP6rrrrtOuXbvk5eVl73f//fdr0qRJ9mVPT0/798XFxRowYIDCwsK0evVqJSQkaNiwYXJ1ddUrr7xSqfsDAAAAoHYwGYZhOLuI01JSUhQSEqIVK1bo6quvllRyRqp9+/aaNm3aWd+zaNEiXX/99YqPj1doaKgkafr06Ro3bpxSUlLk5uZ2we1mZGTIz89P6enp8vX1Lbf9AQAAAFC9XGw2qFL3SKWnp0uSAgICSrXPmTNHQUFBat26tcaPH6+cnBz7ujVr1qhNmzb2ECVJffr0UUZGhnbu3HnW7eTn5ysjI6PUCwAAAAAullMv7fsrm82mxx9/XFdeeaVat25tb7/77rsVHR2tiIgIbdu2TePGjdPevXv19ddfS5ISExNLhShJ9uXExMSzbmvy5MmaOHFiBe0JAAAAgJquygSpUaNGaceOHfr9999LtT/wwAP279u0aaPw8HD16tVLsbGxatSoUZm2NX78eI0dO9a+nJGRocjIyLIVDgAAAKDWqRKX9o0ePVoLFizQsmXLVK9evfP27dKliyRp//79kqSwsDAlJSWV6nN6OSws7KxjWK1W+fr6lnoBAAAAwMVyapAyDEOjR4/WN998o6VLl6pBgwYXfM+WLVskSeHh4ZKkmJgYbd++XcnJyfY+ixcvlq+vr1q2bFkhdQMAAACo3Zx6ad+oUaM0d+5cfffdd/Lx8bHf0+Tn5ycPDw/FxsZq7ty56t+/vwIDA7Vt2zaNGTNGV199tdq2bStJuu6669SyZUsNHTpUU6ZMUWJiop5//nmNGjVKVqvVmbsHAAAAoIZy6vTnJpPprO0zZszQPffcoyNHjmjIkCHasWOHsrOzFRkZqZtvvlnPP/98qcvxDh8+rJEjR2r58uXy8vLS8OHD9eqrr8rF5eJyItOfAwAAAJAuPhtUqedIOQtBCgAAAIBUTZ8jBQAAAADVAUEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwEEEKAAAAABxEkAIAAAAABxGkAAAAAMBBBCkAAAAAcBBBCgAAAAAcVKYgdeDAgfKuAwAAAACqjTIFqcaNG+uaa67R559/rry8vPKuCQAAAACqtDIFqU2bNqlt27YaO3aswsLC9OCDD2rdunXlXRsAAAAAVEllClLt27fX22+/rfj4eH366adKSEhQt27d1Lp1a02dOlUpKSnlXScAAAAAVBmXNNmEi4uLbrnlFs2fP1+vvfaa9u/fryeffFKRkZEaNmyYEhISyqtOAAAAAKgyLilIbdiwQQ8//LDCw8M1depUPfnkk4qNjdXixYsVHx+vm266qbzqBAAAAIAqw6Usb5o6dapmzJihvXv3qn///vrss8/Uv39/mc0luaxBgwaaOXOm6tevX561AgAAAECVUKYg9eGHH+of//iH7rnnHoWHh5+1T0hIiD755JNLKg4AAAAAqiKTYRiGs4twtoyMDPn5+Sk9PV2+vr7OLgcAAACAk1xsNijTPVIzZszQ/Pnzz2ifP3++Zs2aVZYhAQAAAKDaKFOQmjx5soKCgs5oDwkJ0SuvvHLJRQEAAABAVVamIBUXF6cGDRqc0R4dHa24uLhLLgoAAAAAqrIyBamQkBBt27btjPatW7cqMDDwkosCAAAAgKqsTEHqrrvu0qOPPqply5apuLhYxcXFWrp0qR577DHdeeed5V0jAAAAAFQpZZr+/KWXXtKhQ4fUq1cvubiUDGGz2TRs2DDukQIAAABQ413S9Od//vmntm7dKg8PD7Vp00bR0dHlWVulYfpzAAAAANLFZ4MynZE6rWnTpmratOmlDAEAAAAA1U6ZglRxcbFmzpypJUuWKDk5WTabrdT6pUuXlktxAAAAAFAVlSlIPfbYY5o5c6YGDBig1q1by2QylXddAAAAAFBllSlIzZs3T//973/Vv3//8q4HAAAAAKq8Mk1/7ubmpsaNG5d3LQAAAABQLZQpSD3xxBN6++23dQkT/gEAAABAtVWmS/t+//13LVu2TIsWLVKrVq3k6upaav3XX39dLsUBAAAAQFVUpiDl7++vm2++ubxrAQAAAIBqoUxBasaMGeVdBwAAAABUG2W6R0qSioqK9Ouvv+qjjz5SZmamJCk+Pl5ZWVnlVhwAAAAAVEVlOiN1+PBh9e3bV3FxccrPz9e1114rHx8fvfbaa8rPz9f06dPLu04AAAAAqDLKdEbqscceU6dOnXTixAl5eHjY22+++WYtWbKk3IoDAAAAgKqoTGekfvvtN61evVpubm6l2uvXr69jx46VS2EAAAAAUFWV6YyUzWZTcXHxGe1Hjx6Vj4/PRY8zefJkde7cWT4+PgoJCdHAgQO1d+/eUn3y8vI0atQoBQYGytvbW4MGDVJSUlKpPnFxcRowYIA8PT0VEhKip556SkVFRWXZNQAAAAC4oDIFqeuuu07Tpk2zL5tMJmVlZWnChAnq37//RY+zYsUKjRo1Sn/88YcWL16swsJCXXfddcrOzrb3GTNmjH744QfNnz9fK1asUHx8vG655Rb7+uLiYg0YMEAFBQVavXq1Zs2apZkzZ+qFF14oy64BAAAAwAWZDMMwHH3T0aNH1adPHxmGoX379qlTp07at2+fgoKCtHLlSoWEhJSpmJSUFIWEhGjFihW6+uqrlZ6eruDgYM2dO1e33nqrJGnPnj1q0aKF1qxZo65du2rRokW6/vrrFR8fr9DQUEnS9OnTNW7cOKWkpJxx+eHZZGRkyM/PT+np6fL19S1T7QAAAACqv4vNBmU6I1WvXj1t3bpVzz77rMaMGaMOHTro1Vdf1ebNm8scoiQpPT1dkhQQECBJ2rhxowoLC9W7d297n+bNmysqKkpr1qyRJK1Zs0Zt2rSxhyhJ6tOnjzIyMrRz586zbic/P18ZGRmlXgAAAABwsco02YQkubi4aMiQIeVWiM1m0+OPP64rr7xSrVu3liQlJibKzc1N/v7+pfqGhoYqMTHR3uevIer0+tPrzmby5MmaOHFiudUOAAAAoHYpU5D67LPPzrt+2LBhDo85atQo7dixQ7///ntZSnLI+PHjNXbsWPtyRkaGIiMjK3y7AAAAAGqGMgWpxx57rNRyYWGhcnJy5ObmJk9PT4eD1OjRo7VgwQKtXLlS9erVs7eHhYWpoKBAJ0+eLHVWKikpSWFhYfY+69atKzXe6Vn9Tvf5O6vVKqvV6lCNAAAAAHBame6ROnHiRKlXVlaW9u7dq27duumLL7646HEMw9Do0aP1zTffaOnSpWrQoEGp9R07dpSrq2uph/zu3btXcXFxiomJkSTFxMRo+/btSk5OtvdZvHixfH191bJly7LsHgAAAACcV5lm7TuXDRs2aMiQIdqzZ89F9X/44Yc1d+5cfffdd2rWrJm93c/PTx4eHpKkkSNHauHChZo5c6Z8fX31yCOPSJJWr14tqWT68/bt2ysiIkJTpkxRYmKihg4dqvvuu0+vvPLKRdXBrH0AAAAApIvPBmWebOKsg7m4KD4+/qL7f/jhh5KkHj16lGqfMWOG7rnnHknSW2+9JbPZrEGDBik/P199+vTRBx98YO9rsVi0YMECjRw5UjExMfLy8tLw4cM1adKkS94fAAAAADibMp2R+v7770stG4ahhIQEvffee4qMjNSiRYvKrcDKwBkpAAAAAFIFn5EaOHBgqWWTyaTg4GD17NlTb775ZlmGBAAAAIBqo0xBymazlXcdAAAAAFBtlGnWPgAAAACozcp0RuqvD7O9kKlTp5ZlEwAAAABQZZUpSG3evFmbN29WYWGhfdryP//8UxaLRZdddpm9n8lkKp8qAQAAAKAKKVOQuuGGG+Tj46NZs2apTp06kkoe0nvvvffqqquu0hNPPFGuRQIAAABAVVKm6c/r1q2rX375Ra1atSrVvmPHDl133XUOPUuqKmD6cwAAAADSxWeDMk02kZGRoZSUlDPaU1JSlJmZWZYhAQAAAKDaKFOQuvnmm3Xvvffq66+/1tGjR3X06FH93//9n0aMGKFbbrmlvGsEAAAAgCqlTPdITZ8+XU8++aTuvvtuFRYWlgzk4qIRI0bo9ddfL9cCAQAAAKCqKdM9UqdlZ2crNjZWktSoUSN5eXmVW2GViXukAAAAAEgVfI/UaQkJCUpISFCTJk3k5eWlS8hkAAAAAFBtlClIHT9+XL169VLTpk3Vv39/JSQkSJJGjBjB1OcAAAAAarwyBakxY8bI1dVVcXFx8vT0tLffcccd+umnn8qtOAAAAACoiso02cQvv/yin3/+WfXq1SvV3qRJEx0+fLhcCgOAihYXF6fU1NQKGz8oKEhRUVEVNj4AAHCeMgWp7OzsUmeiTktLS5PVar3kogCgosXFxal5ixbKzcmpsG14eHpqz+7dhCkAAGqgMgWpq666Sp999pleeuklSZLJZJLNZtOUKVN0zTXXlGuBAFARUlNTlZuTo8HjXldoVKNyHz8pLlZzXntKqampBCkAAGqgMgWpKVOmqFevXtqwYYMKCgr09NNPa+fOnUpLS9OqVavKu0YAqDChUY1Ur0krZ5cBAACqmTJNNtG6dWv9+eef6tatm2666SZlZ2frlltu0ebNm9WoUfn/ZRcAAAAAqhKHz0gVFhaqb9++mj59up577rmKqAkAAAAAqjSHz0i5urpq27ZtFVELAAAAAFQLZbq0b8iQIfrkk0/KuxYAAAAAqBbKNNlEUVGRPv30U/3666/q2LGjvLy8Sq2fOnVquRQHAAAAAFWRQ0HqwIEDql+/vnbs2KHLLrtMkvTnn3+W6mMymcqvOgAAAACoghwKUk2aNFFCQoKWLVsmSbrjjjv0zjvvKDQ0tEKKAwAAAICqyKF7pAzDKLW8aNEiZWdnl2tBAAAAAFDVlWmyidP+HqwAAAAAoDZwKEiZTKYz7oHinigAAAAAtY1D90gZhqF77rlHVqtVkpSXl6eHHnrojFn7vv766/KrEAAAAACqGIeC1PDhw0stDxkypFyLAQAAAIDqwKEgNWPGjIqqAwAAAACqjUuabAIAAAAAaiOCFAAAAAA4iCAFAAAAAA4iSAEAAACAgwhSAAAAAOAgghQAAAAAOIggBQAAAAAOIkgBAAAAgIMIUgAAAADgIIIUAAAAADiIIAUAAAAADiJIAQAAAICDCFIAAAAA4CCCFAAAAAA4iCAFAAAAAA4iSAEAAACAg5wapFauXKkbbrhBERERMplM+vbbb0utv+eee2QymUq9+vbtW6pPWlqaBg8eLF9fX/n7+2vEiBHKysqqxL0AAAAAUNs4NUhlZ2erXbt2ev/998/Zp2/fvkpISLC/vvjii1LrBw8erJ07d2rx4sVasGCBVq5cqQceeKCiSwcAAABQi7k4c+P9+vVTv379ztvHarUqLCzsrOt2796tn376SevXr1enTp0kSe+++6769++vN954QxEREWd9X35+vvLz8+3LGRkZZdwDAAAAALVRlb9Havny5QoJCVGzZs00cuRIHT9+3L5uzZo18vf3t4coSerdu7fMZrPWrl17zjEnT54sPz8/+ysyMrJC9wEAAABAzVKlg1Tfvn312WefacmSJXrttde0YsUK9evXT8XFxZKkxMREhYSElHqPi4uLAgIClJiYeM5xx48fr/T0dPvryJEjFbofAAAAAGoWp17adyF33nmn/fs2bdqobdu2atSokZYvX65evXqVeVyr1Sqr1VoeJQIAAACohar0Gam/a9iwoYKCgrR//35JUlhYmJKTk0v1KSoqUlpa2jnvqwIAAACAS1WtgtTRo0d1/PhxhYeHS5JiYmJ08uRJbdy40d5n6dKlstls6tKli7PKBAAAAFDDOfXSvqysLPvZJUk6ePCgtmzZooCAAAUEBGjixIkaNGiQwsLCFBsbq6efflqNGzdWnz59JEktWrRQ3759df/992v69OkqLCzU6NGjdeedd55zxj4AAAAAuFROPSO1YcMGdejQQR06dJAkjR07Vh06dNALL7wgi8Wibdu26cYbb1TTpk01YsQIdezYUb/99lup+5vmzJmj5s2bq1evXurfv7+6deumf//7387aJQAAAAC1gFPPSPXo0UOGYZxz/c8//3zBMQICAjR37tzyLAsAAAAAzqta3SMFAAAAAFUBQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQAAAABwkFNn7QMAZzMMKSOvUInpecrIK1ReoU15hcUqKLLJ3dUiDzeLPF0t8vd0Vbifh9xc+PsTAAAgSAGohXIKirTicK6CbhqnRfGuyj1y6KLeZzJJoT7uqlfHQ83CfBTkbb3wmwAAQI1EkAJQKxiGoXUH0zR/41Et2p6g7IJieTW/SrnFJQEp2NuqAC+3krNQrha5WkzKL7Ipp6BYOQVFSsnMV0ZekRIz8pSYkacNh0+orr+H2tXzU8Ngb1nMJmfvIgAAqEQEKQA1mmEYWrX/uN5e8qfWHzphbw/1smjvL7N14823qXXL5nK1XPiSvYzcQh09masDKVk6kJqtYydzdexkrnzcXdStcZCahHjLZCJQAQBQGxCkANRY6w+l6bVFe7ThcEmAcnMx65YOdXVrx3oyHT+oTi/MVfBdt15UiJIkXw9XtfRwVctwX2XmFWrHsQxtP5auzLwiLdqRqO3+HureLJhL/gAAqAUIUgBqnLTsAk1euFvzNx6VVBKg7r48SiN7NFKor7skaVPaoUvaho+7q2IaBapT/TraePiENhw+oaMnczV3bZw6RtdRpHGpewEAAKoyghSAGsMwDM3feFSTF+7WiZxCSdKdnSM15tqm9gBV3lwtZnVtGKiW4b76bX+q9idnacPhEzpodZHFJ7BCtgkAAJyPIAWgRkjLLtDTX23Tr7uTJEnNw3z08s2t1TE6oFK27+vhqgFtwrUvKVO/7k7W8Xwp/J53tCkhT5dVSgUAAKAyEaQAVHur96dqzH+3KCkjX24Ws564rqn+0a3BRd/7VJ6ahPoo2Meq7zYc1ElPP7382wm5BRzSsJj6lV4LAACoODxZEkC1ZbMZmvrLXg3+ZK2SMvLVKNhL34y6Qg92b+SUEHWav6ebeoQVKXPrzzIkvfDdTk39Za8MgxunAACoKQhSAKqlzLxCPTB7g95Zul+GId11eaR+eKSbWkX4Obs0SZLFJKX99K7ubOUtSXpn6X49+80OFdsIUwAA1ARc2geg2jmUmq37P9ugfclZcnMx69Vb2uiWy+o5u6yzur2Vj1o3qa9/frdDX6yLU2Zeod6+s0O5PMA3Li5Oqamp5VDl2QUFBSkqKqrCxgcAoDojSAGoVtYeOK4HZm9Uem6hQn2t+mhoJ7WP9Hd2Wec1pGu0Ar3c9Oi8zVqwLUGebha9ektbmS8hTMXFxal5ixbKzckpx0pL8/D01J7duwlTAACcBUEKQLWxaHuCHvtyiwqKbGoX6a9/D+1YYdOal7d+bcL1jqRRczfpvxuOysvqoheubymTqWxhKjU1Vbk5ORo87nWFRjUq32IlJcXFas5rTyk1NZUgBQDAWRCkAFQLs9cc0gvf75RhSNe2DNW7d3WQu6vF2WU5pF+bcE25tZ2enL9VM1Ydko+7q8Ze2/SSxgyNaqR6TVqVU4UAAOBiMdkEgCrNMEpm5vvndztPTSoRpQ8HX1btQtRpt3asp4k3lgSfd5bs05y1h51cEQAAKAuCFIAqyzAMvfzjbr2zdL8kaUzvpnrl5tZyceLU5uVh+BX1NaZ3yZmoCd/t1OrYipswAgAAVIzq/dsIgBrLZjP0wnc79Z/fD0qSJt7YSo/1blLme4qqmkd7NdZN7SNUZDM08vNNOpia7eySAACAA7hHCkCVU2wz9OjsNfpx9wmZJD3UyU9t3NO0aVNauW1j9+7d5TZWWZhMJr02qK0OH8/RliMnNWLWen3z8JXy83B1al0AAODiEKQAVCk2m6FHPlujhXtOyLAVK3XhND3z2jI9U0Hby8rKqqCRL8zd1aJ/D+uoge+t0oGUbD36xWbNuKfzJU2LDgAAKgdBCkCVYRiGnv9uhz1ENTcnqPXDj0h6pNy3tXvdCi2a9bby8vLKfWxHhPi46+PhnTTow9Va8WeKPli+X6N7NnFqTQAA4MIIUgCqBMMwNPGHXZq7Nk4mSSkLpqr16McqbGrvpLjYChm3LFpF+OlfA9voyflbNXXxn7osuo6uaBTk7LIAAMB5MNkEAKczDEOTF+3RzNWHJEmjOvspZ/cK5xZVyW7tWE+3dawnmyE9+sUWJWc690wZAAA4P4IUAKf7+LcD+vfKA5KkV25uo54NPJ1ckXNMuqm1moX6KDUrX49+sVnFNsPZJQEAgHMgSAFwqm83H9MrC/dIkp7r30J3d4lyckXO4+Fm0QdDLpOXm0V/HEjT+8v2O7skAABwDgQpAE7z+75UPfXVVknSiG4NdP/VDZ1ckfM1CvbWv25uLUl6e8k+bT1y0rkFAQCAsyJIAXCKHcfS9eDsDSosNnRDuwg917+Fs0uqMga2r6sBbcNVbDM05sstyikocnZJAADgbwhSACrdkbQc3TNjvbILihXTMFBv3NaWZyf9hclk0ssDWyvM110HUrP1ykLnPjwYAACcienPAZRJXFycUlNTHX5fRr5N45ekKjWrWNF+Lnq4rYt2bttaqs/u3QQHf083vXFbOw35ZK0+/yNOvZqH6prmIc4uCwAAnEKQAuCwuLg4NW/RQrk5OQ69z+RqVeidL8sa0VxF6Ula9f5TuvrZtHP2z8rKutRSq7VuTYL0jysb6NNVB/X0/23T4jFXy9/TzdllAQAAEaQAlEFqaqpyc3I0eNzrCo1qdFHvsRnSmhQXJeaZ5WY2dG3zOvJ97T9n7bt73QotmvW28vJ4ltLTfZtpxZ/Jik3J1qQFuzT19vbOLgkAAIggBeAShEY1Ur0mrS7YzzAMLdmTrMS8DFnMJt3UoZ4i/D3O2T8pLrY8y3Sq8rhM8b427np2aba+3nRMLbxy1DHcncsfAQBwMoIUgAr3x8E07YzPkElS/9Zh5w1RNUVGWookaciQIeUyXp1rRsj38ps1ceE+xf/nYRkFJZdV1vbLHwEAcBaCFIAKtf1outYdLLkP6prmIWoY7O3kiipHblaGJGnAg8+pWduOlzxekU36NdFQtk+Qrn7+C3nGLuXyRwAAnIggBaDCxKZkadneZEnS5Q0C1Kaun5MrqnyBEdEXdfnjxegXkquvNh3VoWyLWoc1LZcxAQBA2fAcKQAVIv5krhbtSJQhqVWEr7o2CHB2SdVe3ToealuvJIzuV5hkcXVyRQAA1F4EKQDlLi27QN9vjVexzVCDIC/1bBYik4kH7paHKxoFystqUZ6s8ou5zdnlAABQa3FpH6qlsj4M9mIFBQUpKiqqwsavybLyi/TtlmPKL7IpzNdd/VqHyWwmRJUXq4tF3ZsEa+GORPl1uU05tnM/hwsAAFQcghSqnbI+DNYRHp6e2rN7N2HKQflFxfpuyzFl5hXJ39NVN7aLkKuFE9/lrXGIt+ooSydcvPVngb9iDIMzfgAAVDKCFKqdsjwM1hFJcbGa89pTSk1NJUg5oMhm04KtCUrNKpCnm0UD29eVh5vF2WXVSCaTSY2UqHWF9ZTu6q49iZlqEe7r7LIAAKhVCFKoti72YbCoeIZh6JedSTp6MlduFrNuah8hPw8mQqhI7ipU+qovVKfHvfptX6rqB3nJw5XgCgBAZXHqNTcrV67UDTfcoIiICJlMJn377bel1huGoRdeeEHh4eHy8PBQ7969tW/fvlJ90tLSNHjwYPn6+srf318jRozgAZVAJTIMQyv3pWpfcpbMJmlA23CF+Lg7u6xaIWP9t/IyFSq3sFir9lfcPYMAAOBMTg1S2dnZateund5///2zrp8yZYreeecdTZ8+XWvXrpWXl5f69OlT6gGUgwcP1s6dO7V48WItWLBAK1eu1AMPPFBZuwDUepviTmrLkZOSpGtbhioqwNO5BdUmtmI1cTspSdoZn6FjJ3KdWw8AALWIUy/t69evn/r163fWdYZhaNq0aXr++ed10003SZI+++wzhYaG6ttvv9Wdd96p3bt366efftL69evVqVMnSdK7776r/v3764033lBERESl7QtQG8Vlm7U+ruRMSLfGQWoexn06lc3PUqjWEb7aEZ+hpXuTdfflUbIwSyIAABWuyk6ndfDgQSUmJqp37972Nj8/P3Xp0kVr1qyRJK1Zs0b+/v72ECVJvXv3ltls1tq1a885dn5+vjIyMkq9ADjGPbqdNhwvuSenfaS/Lovyd25BtdiVjYPk4WpRWnaBNsWdcHY5AADUClU2SCUmJkqSQkNDS7WHhoba1yUmJiokJKTUehcXFwUEBNj7nM3kyZPl5+dnf0VGRpZz9UDNduBEoYJvfk6GTGoa4q2rmwQx/bYTubtadHWTIEnS2oNpSs8tdHJFAADUfFU2SFWk8ePHKz093f46cuSIs0sCqo3Dx7P1r9/SZLZ6Kthq07WtQglRVUCzMB9F1vFQsc3Qsr3JMgzD2SUBAFCjVdkgFRYWJklKSkoq1Z6UlGRfFxYWpuTk5FLri4qKlJaWZu9zNlarVb6+vqVeAC4sKSNPQz5Zq5N5NhUkH1RMcJFczFX2x0itYjKZdE3zEFlMJh0+nqPYlGxnlwQAQI1WZX8DatCggcLCwrRkyRJ7W0ZGhtauXauYmBhJUkxMjE6ePKmNGzfa+yxdulQ2m01dunSp9JqBmuxEdoGG/GetjqTlKszboqT//lOuVfYnSO1Ux9NNHaPrSJJW/JmigiKbkysCAKDmcuqvQVlZWdqyZYu2bNkiqWSCiS1btiguLk4mk0mPP/64/vWvf+n777/X9u3bNWzYMEVERGjgwIGSpBYtWqhv3766//77tW7dOq1atUqjR4/WnXfeyYx9QDnKzCvU8BnrtC85S2G+7nqxe4Bs2SedXRbOonP9OvLzcFVWfpHWHjzu7HIAAKixnDr9+YYNG3TNNdfYl8eOHStJGj58uGbOnKmnn35a2dnZeuCBB3Ty5El169ZNP/30k9zd//ewzzlz5mj06NHq1auXzGazBg0apHfeeafS9wVwRFxcnFJTK/YBqkFBQYqKirrkcfIKi3X/Zxu07Wi66ni66vP7LlfG0X0XfiOcwsViVo+mwfpua7w2HzmpFuG+CvK2OrssAABqHKcGqR49epz3hmiTyaRJkyZp0qRJ5+wTEBCguXPnVkR5QIWIi4tT8xYtlJuTU6Hb8fD01J7duy8pTBUW2zR67ib9cSBN3lYXzfrH5Woc4qNNR8uxUJS7+kFeahTspdiUbC3dk6zbOtZjQhAAAMqZU4MUUBulpqYqNydHg8e9rtCoRhWyjaS4WM157SmlpqaWOUjZbIaenL9Vv+5OltXFrP8M76S29fzLt1BUmO5NgxWXlqOE9DztSshQqwg/Z5cEAECNQpACnCQ0qpHqNWnl7DLOyjAMTfh+p77bEi8Xs0kfDrlMXRsGOrssOMDH3VVdGgTq9/2p+n1/qhoGe8vD1eLssgAAqDGYcwtAKYZh6KUFuzX7j8MymaQ3b2+nns1DL/xGVDntI/0V6OWmvEKbVu+v2HvyAACobQhSAOwMw9DEH3bp01UHJUkvD2yjm9rXdXJVKCuLueTZUpK0Iz5DCem5Tq4IAICagyAFQFJJiHrx+52aufqQJGnyLW10d5dLn/UPzlXX30Mtw0seOr50T7JstnNP8AMAAC4eQQqAbDZDz3+7Q7PWlFzON2VQW911OSGqpriycaCsLmalZhVo69GTzi4HAIAagSAF1HIFRTY99uUWzVkbJ5NJem1QW93eOdLZZaEcebq5qFvjIEnSmgPHlZVX5OSKAACo/ghSQC2WU1Ck+z7boB+2xsvVYtI7d3bQ7Z0IUTVRqwhfhfm6q7DY0Mp9Kc4uBwCAao8gBdRSJ7ILNOQ/a7XyzxR5uFr0n+GddUO7CGeXhQpiMpnUs3mITJL2JWfp8PFsZ5cEAEC1RpACaqHYlCzd/MEqbYo7KT8PV31+Xxd1bxrs7LJQwYJ9rGoX6S9JWrY3RUXFNucWBABANUaQAmqZ1bGpuvn9VTp0PEd1/T00/6EYdYyu4+yyUEm6NgyQl5tF6bmF2nD4hLPLAQCg2iJIAbWEYRiauzZOwz5Zp4y8Il0W5a/vRl+ppqE+zi4NlcjqYtHVp84+bjh8QieyC5xcEQAA1RNBCqgFcguK9eT8bXr2m+0qshm6sV2E5t7fVUHeVmeXBidoEuKtqABPFdsM/bonSYbBs6UAAHAUQQqo4U7fD/V/m47KbJKe6tNMb9/ZXu6uFmeXBicxmUzq1TxErhaT4k/mafuxdGeXBABAtUOQAmqwJQdzdOO7v2tPYqaCvK2ac19XjbqmsUwmk7NLg5P5ergqpmGgJGnV/uPKzCt0ckUAAFQvBCmgBsotkoJvnaD316cru6BYXRoEaOGj3RTTKNDZpaEKaRfprzBfdxUU27RsbwqX+AEA4ACCFFCDGIahXQkZWpzgKs9GneVilp7p11xz7uuiEF93Z5eHKsZsMql3ixCZTdLB1Gz9mZTl7JIAAKg2CFJADZGYkaf/bjiqxbuSVGiYlJ/wp968NkgPdW8kFwv/1HF2gd5Wda4fIEla/meysvOLnFwRAADVA79dAdVcZl6hFu9K0pfrjygxI0+uFpNa+xcpcfaTivRzdXZ5qAY61w9QsI9VeYU2LdmTzCV+AABcBIIUUE2l5xZqye4kzVx9SLsSMiRJLcJ8NCymvpr52iTD5uQKUV1YzCZd1zJUFpNJB1Oz7ccTAAA4NxdnFwDg4hmGoYT0kumq9yZl6vSJg3r+HrqicaDC/TwkSSedVyKqqSBvq7o2DNCq2ONa+WeqeoY6uyIAAKo2ghRQDWTnF2lfcpZ2HEvX8ewCe3tUgKcubxCguv4eTqwONcVl0XV0IDVbCel52njcRRLT5AMAcC4EKaAKMgxDJ3MLdTAlW/tTspSQnmdf52I2qWmoj9rU81MYM/GhHJlNJZf4zVkbp5R8s3wvv8XZJQEAUGURpIAqoLDYpuNZBUrKzFP8iVwdO5mr7ILiUn1CfKxqGe6r5mE+srpanFQpajp/Tzd1bxqsJXuS5X/1UO09XqDLnF0UAABVEEEKqATFNkMpmfmKT8/VqiO58ul8s7akWbRlW7yOZxXoZG7hGe+xmEwK93dX42BvNQz2ko87M/ChcrSK8NXeuAQdzXHRW3+cVP9uhfLzqJjjLy4uTqmpqRUytiQFBQUpKiqqwsYHANReBCmgHKTnFir+ZO7/Xul5ij+Zq4STeTp2MldJGXkqsv1vSumAniMUmyUpK9ve5ulmUZC3VRH+7qrr76EwX3ee/wSnMJlMuiygWAfjU5SsMD379Xa9d3cHmUzle89UXFycmrdoodycnHId9688PD21Z/duwhQAoNwRpAAH2GyG9iVnaePhE9qVkK79yVnan5yt1Kz8C77XYjYpzNddvi5F2rBysTp0vUoRYWGq4+WmQC83eVn554iqw9UspX4/RfWGT9WP2xMUszZQQ7pGl+s2UlNTlZuTo8HjXldoVKNyHVuSkuJiNee1p5SamkqQAgCUO35zAy7g2MlcLd6ZqGV7U7Qp7oQy84rO2i/Qy03h/u6K8PNQhL+HIvzdFf6X70N83GUxm7Rp0yZ1fOoNtel7hepF+lfuzgAOKEj4U4Pb+OizbZma9MMutYzw1WVRdcp9O6FRjVSvSatyHxcAgIpEkALOwuzpp693Z+mFVb9px7HSDyf1dLOofaS/2tbzV5MQbzUO8VajEG95c0YJNdCNzbyUVOypn3cm6aHZG/XDI90UymyRAAAQpIDTTj/sdl2qRfUenqnPt2dKkkwmqXN0gK5tGaqYRoFqHubDvUuoNcwmk968vZ0OvL9K+5Kz9NDnGzXvga6yujBzJACgdiNIASq5fG/1/lTFp+dJsshksahJgKtG9Giu3i1DFeRtdXaJgNN4W1308bBOuvG937U57qRe+HanXh3UptwnnwAAoDrhz+qo1VIy8/XdlmP6auNRxafnyWI2qb5XsRJmPqbXegfpzsujCFGApPpBXnr37stkNklfbjiij3874OySAABwKoIUaqXCYpt+25eiL9bF6dDxHJlMUusIXw2PiVbHwGIVJMU6u0SgyuneNFjj+7WQJL2ycI++3XzMyRUBAOA8XNqHWicuLUdL9yQr/dRDcBuHeOuKRoGq4+kmSUp3ZnFAFXffVQ2UkJ6nT1cd1FNfbVWgt5uuahLs7LIAAKh0nJFCrVFks2n53mR9s/mY0nML5W110Q3twjWgTbg9RAE4P5PJpOcHtND1bcNVWGzoodkbteMYf34AANQ+BCnUCum5hZq/4ai2Hi35ha9tXT8N6RqlhkHeTq4MqH7M5pKZ/GIaBiq7oFjDPl2nXfEZF34jAAA1CEEKNV5sSpbmrotTcma+3F3MuqldhK5pHsL0zcAlsLpY9NGwjmpbz09p2QW6+z9/aGc8Z6YAALUHQQo1lmEYWncwTQu2JaigyKZwP3fd3SVK9YO8nF0aUCP4urtq9oguahfpr5M5hbr747Vc5gcAqDUIUqiRim2GFu9O0poDxyVJ7SP9NeiyevJxd3VyZUDN4ufhqtkjLleHKH+l5xbq7o//0MbDac4uCwCACkeQQo2TV1isbzcf0+6ETJlM0jXNgtW9abAsZh4eClQEX3dXffaPy9Uxuo4y8op018drtXB7grPLAgCgQhGkUKNk5Rdp/sajOnoyV24Ws25sF6G29fydXRZQ4/m4l5yZ6t0iRAVFNo2au0kfrzwgwzCcXRoAABWCIIUao2RmviNKyy6Ql9WiWzvWU/1A7ocCKounm4s+GtpJw2OiZRjSywt36/lvdyi/qNjZpQEAUO4IUqgRjmfla/7GI8rIK5Kfh6tu6xipYB+rs8sCah2L2aQXb2yl5we0kMkkzVkbp9unr9HREznOLg0AgHLl4uwCgEuVkpmvrzcfVV6hTYFebrq5Q115WTm0JWn37t3ValzUDCaTSfdd1VCNgr31+JdbtPVouga887veuqOdejYPdXZ5AACUC37bRLWWkpmvrzcdVV6RTaG+Vt3Uvq48XHk+VEZaiiRpyJAhFbqdrKysCh0f1ds1zUP046PdNGrOJm09mq5/zNyge6+sr6f7NJeHG/9OAQDVG0EK1VZ6gUm/b/5fiLq5Q10esntKblaGJGnAg8+pWduO5T7+7nUrtGjW28rLyyv3sVGz1Kvjqf8+FKNXftytWWsOa8aqQ1q+N0Vv3NZOzKMJAKjOCFKollyDorUy2UUFtlMhqj0h6mwCI6JVr0mrch83KS623MdEzWV1sWjiTa3Vo3mInvm/bTqYmq3bpq/WDU29ZHLlXkYAQPVUpSebePHFF2UymUq9mjdvbl+fl5enUaNGKTAwUN7e3ho0aJCSkpKcWDEqQ1x6oULvfFkFNpNCfE6FKC7nA6q8a5qF6JfHu+uWy+rKZkjf7c1WxIgPdSzHxDTpAIBqp0oHKUlq1aqVEhIS7K/ff//dvm7MmDH64YcfNH/+fK1YsULx8fG65ZZbnFgtKtq+pExNWJ4mi5e//F1tJZfzEaKAasPP01VTb2+vT4Z3UrCnRS5+Ifoj1VU/bEvQyZwCZ5cHAMBFq/KX9rm4uCgsLOyM9vT0dH3yySeaO3euevbsKUmaMWOGWrRooT/++ENdu3at7FJRwfYnZ+muj9cqPd+mgqRYXXVZpNwJUUC11KtFqN7pG6z+T70j/5jbdDA1W4ePZ6tdpL8urx/Av20AQJVX5c9I7du3TxEREWrYsKEGDx6suLg4SdLGjRtVWFio3r172/s2b95cUVFRWrNmzXnHzM/PV0ZGRqkXqrbYlCzd9fEfSs3KV31/FyXNe15M+gVUb1YXk06u/Ey9wwsVHeApmyFtjjupWasPacuRkyq2cbkfAKDqqtJBqkuXLpo5c6Z++uknffjhhzp48KCuuuoqZWZmKjExUW5ubvL39y/1ntDQUCUmJp533MmTJ8vPz8/+ioyMrMC9wKWKTcnSXf/+QymZ+Woe5qMXuwfKlpfp7LIAlBNfV2lgh7q6qX2EAr3clFdk04o/U/T5H4cVm5LF/VMAgCqpSl/a169fP/v3bdu2VZcuXRQdHa3//ve/8vDwKPO448eP19ixY+3LGRkZhKkq6sCpEJV8KkTNua+LDv+509llAagA9QO9FFXHUzsTMrQm9rhO5hZqwbYE1fX30FVNghTq6+7sEgEAsKvSZ6T+zt/fX02bNtX+/fsVFhamgoICnTx5slSfpKSks95T9VdWq1W+vr6lXqh6DqZm666PS0JUs9CSEBXozVTJQE1mNpvUpq6fhl8RrU7RdWQxm3TsZK7mrT+in3cmKjOv0NklAgAgqZoFqaysLMXGxio8PFwdO3aUq6urlixZYl+/d+9excXFKSYmxolVojwcTM3Wnf9eo6SMfDUN9dac+wlRQG1idbHoysZBGtY1Ws1CfSRJexIzNWvNYa2OTVVBkc3JFQIAarsqfWnfk08+qRtuuEHR0dGKj4/XhAkTZLFYdNddd8nPz08jRozQ2LFjFRAQIF9fXz3yyCOKiYlhxr5q7lBqtu769x/2EDX3/q4KIkQBtZKvh6v6tg5T+0h//bYvRfHpeVp/6IR2xmeoa8NAtQr3ldlscnaZAIBaqEoHqaNHj+quu+7S8ePHFRwcrG7duumPP/5QcHCwJOmtt96S2WzWoEGDlJ+frz59+uiDDz5wctW4FIdSs3Xnv/9QYkaemoQQogCUCPNz160d6yk2JVu/709Vem6hlu5J1tYjJ3V102BFBXg6u0QAQC1TpYPUvHnzzrve3d1d77//vt5///1KqggV6WBqtu7+mBAF4OxMJpMah3irQZCXth09qbUH03Q8u0DfbD6mJiHeuqpJkHzcXZ1dJgCglqjSQQq1x874dA3/dJ1SswrU+FSICvYhRAE4k8VsUoeoOmoR7qs/DhzXtqPp2pecpUPHs9WlQaDaR/rLwuV+AIAKRpCC060/lKZ/zFyvzLwitQz31ax/XE6IAnBB7q4W9WgWolYRflq2N1kJ6Xn6fX+qdsVnqEezYBGlAAAViSBVBcXFxSk1NbXCxg8KClJUVFSFje+IZXuSNXLORuUV2tS5fh39Z3hn+XlwaQ6AixfsY9VtHetpd2Kmft+XqrScAn29+ZjqeVpk8Q5wdnkAgBqKIFXFxMXFqXmLFsrNyamwbXh4emrP7t1OD1Of/3FYE77fqWKboWuaBeuDwR3l4WZxak0AqieTyaSW4b5qFOSlNacu9zuaY1HEfR/qp/3Zat/eYHY/AEC5IkhVMampqcrNydHgca8rNKpRuY+fFBerOa89pdTUVKcFqWKboZd/3K1PVx2UJN1yWV29NqitXC3V6rFmAKog618u9/tpyyGlyUv/3pShjcfX6NVb2qjJqWdSAQBwqQhSVVRoVCPVa9LK2WWUu8y8Qj02b4uW7kmWJD3Vp5ke7tFIJhN/KQZQfoJ9rOoRWqT/fPqpIvqN1MbDJ9T/nd80skdjjbqmkawunP0GAFwaTgGg0uyKz9CN763S0j3JsrqY9f7dl2nUNY0JUQAqhMkkZW5aoHf6Bqt3i1AVFht6Z8k+9Xv7N607mObs8gAA1RxBChXOMAx9sS5ON3+wSgdTsxXh564vH4zRgLbhzi4NQC0Q5GnRx8M66oPBlynYx6oDKdm6/aM1Gv/1dqXnFjq7PABANUWQQoU6kV2gx+Zt0fivtyu/yKZrmgXrx0evUvtIf2eXBqAWMZlM6t8mXL+O7a67Li+5P/SLdXHqPXWFFm5PkGEYTq4QAFDdEKRQIQzD0MLtCbr2rRX6fmu8LGaTxvdrrk+Gd1YdLzdnlweglvLzcNXkW9roywe6qmGwl1Iy8/XwnE26/7ONij+Z6+zyAADVCJNN1FK7d++usLFt7n76eOMJ/bwzSZLUJMRbU25tqw5RdSpsmwDgiC4NA7Xosav0/rJYfbh8v37dnaQ1sal6qk8zDY2pLwtTpQMALoAgVctkpKVIkoYMGVLuY5usXvLreqt8O90kk4ubXMwmjbqmsR5mhiwAVZDVxaKx1zbVDW3D9czX27Xx8Am9+MMufbslXq8OaqPmYb7OLhEAUIURpGqZ3KwMSdKAB59Ts7Ydy2XMQpt0MMusvRkWFdhK/orbMthNUwd34RcRAFVek1AfzX8wRnPWxWnKoj3acuSkrn/ndz3YvaEe6dlE7q78IQgAcCaCVC0VGBF9yc+pyswr1NYj6dqemK6CIpskycfVptgvXtL/ffkhIQpAtWE2mzS0a7SubRGqCd/v0M87k/T+slgt3J6ol29urSsaBTmttri4OKWmplbY+EFBQU57QDsAVGcEKTikyGbTodQc7UnM0MHUbNlOTXRVx9NVHaPryCfrqHbErq8Rz4aqqPvIKvL+NACXJszPXR8N7aSfdiRqwvc7dDA1W3d/vFY3tovQuH7NVdffo1LriYuLU/MWLZSbk1Nh2/Dw9NSe3bsJUwDgIIIULqio2KYjJ3J1ICVL+5KzlH/q7JMk1fP3UIdofzUI9JLJZNLRfUedWGn5qMj7yP4qKyurQscHUHZ9W4fpisaBmvLTHs1ZG6fvt8br552Juu+qBhrZo7G8rZXzv8/U1FTl5uRo8LjXFRrVqNzHT4qL1ZzXnlJqaipBCgAcRJDCGQzDUGpWgY6dzNXh49k6eiJXRbb/PWPFy2pR81BfNQ/3UZC31YmVVoyKuI/sr3avW6FFs95WXl5euY8NoPz4urvqXwPb6M7OUfrXj7v0x4E0vb8sVl+uP6KRPRprcJeoSrt/KjSq0SVfjg0AKF8EKSi/qFhJGflKzMhTUnqejp3MLXXWSZK8rS5qEOSlxiHeqlfHQ+YacOnehZTHfWRnkxQXW+5jAqg4rev66Yv7u+qXXUmavHC3Dh3P0UsLdunjlQc0umdj3dapHjOTAkAtRJCqZQxJbqGNFF/oqeRdiUpKz1daTsEZ/VwtJkX4eahuHQ81CPJSoJdbjbjvCQDKwmQyqU+rMPVsHqKvNh7Vu0v2KT49T89/u0PvLNmne69soLu7RMnPw9XZpQIAKglBqobLLyxWQnqeEtLzFJ+eq3g1U/g9b2tfoaSETHs/X3cXhfm6K9TPXRF+Hgr2sfJASgD4G1eLWXddHqVbLqureeuO6IPl+5WUka/Xftqj95ft1+2dInV3lyg1DvF2dqkAgApGkKph8gqLdSQtR3EncpRwMk/Hs/9+tsksW16WAj1d1CQqQqF+VoX6uMurkm6cBoCawOpi0fAr6uuuy6P0/dZ4/XtlrP5MytKnqw7q01UHdXn9AN3VJVJ9W4XLw43L/gCgJuK352ru9MQQsSlZOnQ8W0kZ+Wf08fNwVYSfu8L9PXRizx/69u3HdM3Ej9S+URsnVAwANYebi1m3dqynQZfV1fI/UzTnj8NauidZ6w6lad2hND3ntkO9WoTq+rbh6t40mIf7AkANQpCqhgzDUFJGvvYmZSo2JUuZeUWl1gd6uSkywFP16ngo3M9dnm7/+8+8cU++Su6UAoDaoSKf3Xb6YbYmk0nXNAvRNc1ClJCeq/kbjuq/G47o6Ilc/bA1Xj9sjZeXm0VXNg5Sj2Yh6tEsWBGV/EwqAED5IkhVIydzCrQnMVN7EjOVnltob3cxmxQd6KkGQV6KDvCStzv/WQGgMp4Jd7aH2Yb7eejRXk30SM/G2no0XQu2xuvH7QlKSM/TL7uS9MuuJElSwyAvXd4gQF0aBqhz/QDV9fdgUh8AqEb4jbuKK7YZik3J0vZj6Tp6Itfe7mI2qVGwt5qEeisqwFOuFrMTqwSAqqeinwl3oYfZmkwmtY/0V/tIfz3bv4V2JWRo2Z5kLdubrC1HTupAarYOpGZr3vojkqQALze1ruunNnV91aaun1rX9ZNhcAUBAFRVBKkqKq9YWhN7XNuPpSu3sNjeHh3gqebhPmoY5C03F8ITAFxIRT0TzhFms0mtT4WjR3o1UXpOodafuo9q7cE07TyWrrTsAq38M0Ur/0yxv8/HzaSQ2ydp+wmLshIyFOhtVYCXG7OqAkAVQJCqYg6eKFRg/zFaeMxVhtIkSV5uFrWK8FOrCF/58owSAJWoIu8vqsixqzo/T1f1bhmq3i1DJZXMuLonMVPbj6Vrx9F0bT+Wrj+TMpVZYMijwWX6M1P689QlgWZTydmrIG/rqVfJ98y+CgCVi5+6Vcivu5L0xOJUebfpJUNSuJ+7OkT6q2GwN399BFCpKuP+otOysrIqfBtVnburxX4Z4Gl5hcX6bsV63ffki+p868PKc/FSamaBCoptSs0qUGpWgaT/PQ/Qw9WiEF+rQn3dS54L6GstNdkQAKB88RO2CunWJEh13M06ummZrr/mCrVt3cTZJQGopSr6/iJJ2r1uhRbNelt5eXkVMn515+5qUZMAN2Vt/VkdHnhQ9ZpEyjAMZeYVKTUr/1SYyldKVr5O5hQqt7BYh4/n6PDxHPsYvu4u9mAV5ueuUF93/jAHAOWEIFWFuLta9EH/EF0x8XUF9P3a2eUAQIXeX5QUF1sh49ZkJpNJvh6u8vVwVcPg/7UXFtt0PKtAyZl5SszIU1J6vtJyCpSRV6SMvCztSy456+diNinC30P16pS8bMxlAQBlRpCqYqwu/KUQAOAYV4tZYX4lZ53anmrLLypWcka+EjPylJiep/j0XOUV2hSXlqO4tJKzVi4mV4Xc9qK+3ZMlz4gMNQv1YQp2ALhIBCkAAGogq4tFkQGeigzwlFTyMPfj2QU6eiJXR0/k6NiJXOUV2eTRsJM+25apz7b9pnA/d/VoFqzuTUN0ZeNA+bgzwREAnAtBCgCAWsBkMtln+msf6S/DMLRr12799/MZ6j30Ee1MKVRCep6+WHdEX6w7IhezSZ3q19E1zULUq0WIGgV7c7YKAP6CIAUAQC1kMpnk52Yoc+P3ev7fE9SyTTv9ceC4lu9N0Yo/U3QwNVt/HEjTHwfSNHnRHkUFeKpn85JQdXmDAFldLM7eBQBwKoIUAACQu6tFPZqFqEezEEnSodRsLd+brGV7U7Qm9rji0nI0c/UhzVx9SF5uFnVrEqRezUPVo3mwQnzcnVw9AFQ+ghQAADhD/SAv3RPUQPdc2UDZ+UVatT9VS/cka8meZKVk5uvnnUn6eWfJQ4Lb1fNTz+ah6tUiRK0ifLkEEECtQJACAOAS7N69u1qO7Qgvq4uuaxWm61qFyWYztDM+Q0v2JGnpnmRtO5quradeb/36p0J9rerZPEQ9m4fqysaBPBQYQI3FTzcAAMogIy1FkjRkyJAK31ZWVlaFb+Nimc0mtannpzb1/PR476ZKzsjTsr3JWronWb/tS1VSRr59wgo3F7OuaBSons1D1KNpiCIDPDhbBaDGIEgBAFAGuVkZkqQBDz6nZm07Vsg2dq9boUWz3lZeXl6FjF8eQnzddUfnKN3ROUr5RcVaeyBNS/ck69fdSTp6IlfL96Zo+d4USTtV199DVzQK1BWNAxXTMEhhftxbBaD6IkgBAHAJAiOiVa9JqwoZOykutkLGrShWF4uubhqsq5sGa8INLbU/OUtL9iRr6e5kbYo7oWMnczV/41HN33hUktQw2EsxDQPVpWGgOkT6q16d2nnGKi4uTqmpqRU2flBQkKKioipsfKC2IkihQtSGewYAAOdmMpnUJNRH1vwTutzHqtwOIdpzvFDbk/K1PblAB08W6kBKtg6kZGvO2jhJkq/VrCYBrmoS4KqmgW5qEuAqLzfzebdT3UNCXFycmrdoodycnArbhoenp/bs3l2tPyegKiJIoVzV1nsGAKA6q6g/UCUkJOjW225TXm7uGetMVi+5R7aSe3Q7WSNayC20gTLyXbUxIV8bE/Lt/QpPxKswNc7+KkiNU1HaURlFBZKqf0hITU1Vbk6OBo97XaFRjcp9/KS4WM157SmlpqZW289I4qwdqiaCFMoV9wwAQPVRWX/8uvnRSWrQrPV5+xQbhk4WFCot36QTBSalFZiVXWSSa50IudaJkJp0/UtvQ14ukltRtuI2r9RHvx1S55YWhfu5K8LfQ2F+7nK1nP9MVlUTGtWowi4Rre44a4eqiiCFCsE9AwBQ9VX0H79O/+HLJzDsov6fEP33+gqKlZqVr7TsAh3PLjj1NV95hTZlF0nZ8pZPh/6asz1Tc7Zvsb/PZJICvazy93SVn4er/D1KvvqdWvZ1d5Wbi7nkZfnfV1cXs1zNJhmSDEOyGYZshmH/vthmqMhmqLDYpoIimwqLS74vLLapoNimwqK/Lf+lzb5c/Nf325Sema3we9/Vz/GuMicflM2Qim2Gig1DNpshSTKbTDKbSi6X/OtXF7NZLhaTXCwmuVrMcjGf+npq2dVsVk66WT4db9CSgzlKdE2Qp5tF3lYXeVld7F+9rBZZXSzl9x++nHHWDlUVQQoAgFquov74dal/+PJwsygywFORAZ72NsMwlFtYrONZBTp0+LCW/jBfNw/+h/LMnopPz1XCyTwVFNuUmpWv1Kz884xedbiFNFBWkaSionP0MC5hdBcF9H5Q769Pl9ZvOncvs+TuYpKHi1keriZ5uJS83F3N9u9L2s2n2k1yNZvkYjbJxSwF1vFTRFiY3FxMcjGbZT7HpCGGSgKpPVAW21RYVDpg/i90liwfOpIp/x73KsG7qU4W1VGRrWSM06+iUt/bVGwzdCqD2oOw/XtJJp0Kp+aSrzaX5oq4b7oe+u6ovBenyGoxyepiltUiubuY5WYxyWoxydPVJC83s7xcTfI+9bVk2SxvN5PcXUznnSyFywdrHoIUAACoNkwmkzzdXOQZ4CLTcZvSf5+j3g/1U4sWLSS5yWb4KiPfprRcm7ILbMoqtCkr31BWYclyZoGh3EKbCm2GCm1Skc1QUbFUeOoX8iJbyRmtkl+2S7bp5uoqd6ubTCbTqTNXp874WE6dyTp9Bsjlb8unznaVWv7b++MOHdCjox7WbY++qLCoBrKYTDKbTbKYTfYw8tezYn89U3Y6kBQVn9qfYpuKTgWSkv2yKSUxXvu2b5TZzVMmNw+Z3TxOfXUv+epaMgV9kU3KKjCUVVBcxv8yJyQdusT/uufm12WQYrMknTqLWr5Mcg2sp+R8KTn/XGH2wgxbsWx5WbLlZZ/6milbXqaKc0u+txTna/KLz6tRZNips6Vu9rOm1e1SVJQgSAEAgGqpsu7xqsj7Zzblxyvv8FYFuxsK9/Mo9/E3Hv1Da76fcuryzbZnrLcZBSoySoJUyVeTCk8v20z2dYWG6Yw+JaFOyi/IV1pSgho2aSoXVzcVFhsyjHOfRbOcCpZup8Lk6aBZEjr/smwxy+pq1sm045o3Z7a6XDtQdYJCZDkVNF1Off3796dDqEklwft0MDadajNUOpTuWr9Sv3z+oboOHK66DZqqyJCKT+178alX0an9L7BJhTaTCm1Sgf2rZMgkk9kii6efLJ5+59z3V5YelXT0jHZvq0vJ5acervL3LHn5urvK063k0svTXz1cLfKyusjTraTN061k2epiVnJivDJPnrCfJTx9GWh54YzamWpMkHr//ff1+uuvKzExUe3atdO7776ryy+/3NllAQCAClIZExydvn/mt99+O3XWq3xV1iM9KvLe5aP7dmrqa49qwcaNuuyyy8p9/E2bNmn6/TPU6rYbVK9BQLmPf1R5yj+2S9EhddS+bUuH328YJWcz84tsyi8sVt5fvuYVFiu/0KbU1BRtX/e72l9+hQw3T2UV2JRVYFN2Qcnlhln5RcrKL9Kxk2fOcFlWhmGTiotkFBfKKC6SUVwkFRfKsBXLsBVJNltJm61Yhq1YshWVrPt7W3GxZBTLbJL6XXedfH28ZDFJFrNkMZnsX13MKgmyJp0KtyXtrhbJzWIq9bJaSi4NdfvLutCQ4GoX1GpEkPryyy81duxYTZ8+XV26dNG0adPUp08f7d27VyEhIc4uDwAAVKCKDAmVddarJjzSo6JCYVV/fqTJZLKfRfO2nv1X612p27Xsh9f16w9/f7NZZqunzO4+Mnv4yOzuXfK9u7cs7t4yubrL5OZecimmq7vMrh4y2b93/99lmhY3mVxc/1aXWXJxk8nFrdz2dUWCpITschvvr4zioxrZLUXP3FQxfxSpCDUiSE2dOlX333+/7r33XknS9OnT9eOPP+rTTz/VM8884+TqAABAdVVZMxtW50d6EDYvrOKPo+VaNOttDXnx32p9+VX2e+j+OgPk/77/3312ttP33Nn+8v1Z+hzZv0tbf/tFTTr3UJ2QcPtlnSWXSf79q8m+bDMkm/53iWSxzVQyM+Wpl03/u/TQZHFRTnbFhLSKUu2DVEFBgTZu3Kjx48fb28xms3r37q01a9ac9T35+fnKz//fTD7p6emSpIyMiriB0TGnf0gc3bdT+bnl/7yE0zMoJR76U7FenhfoXfXGr4xtVPfxK2Mb1X38ytgG4zt/G9V9/MrYBuNf/DYKC/Ir5P/LhQUlv49U58/o0K7NkqTO/e5QvQZNyn38uD+3a+Ov3+nQnm0VMilDTTqOkg//qSPeju+D+dTrXJLi1il99TwFNKunJpHeZSvyLIxTgSo5Pk7f/ecNXfHt/1WJ38dP13C+e/0kyWRcqEcVFx8fr7p162r16tWKiYmxtz/99NNasWKF1q5de8Z7XnzxRU2cOLEyywQAAABQjRw5ckT16tU75/pqf0aqLMaPH6+xY8fal202m9LS0hQYGFius5uURUZGhiIjI3XkyBH5+vo6tRZUDxwzcBTHDBzFMQNHcczAEVXteDEMQ5mZmYqIiDhvv2ofpIKCgmSxWJSUlFSqPSkpSWFhYWd9j9VqldVqLdXm7+9fUSWWia+vb5U4kFB9cMzAURwzcBTHDBzFMQNHVKXjxc/v3NPYn1btn/7l5uamjh07asmSJfY2m82mJUuWlLrUDwAAAADKS7U/IyVJY8eO1fDhw9WpUyddfvnlmjZtmrKzs+2z+AEAAABAeaoRQeqOO+5QSkqKXnjhBSUmJqp9+/b66aefFBoa6uzSHGa1WjVhwoQzLj0EzoVjBo7imIGjOGbgKI4ZOKK6Hi/VftY+AAAAAKhs1f4eKQAAAACobAQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaSc4P3331f9+vXl7u6uLl26aN26deftP3/+fDVv3lzu7u5q06aNFi5cWEmVoqpw5Jj5+OOPddVVV6lOnTqqU6eOevfufcFjDDWPoz9nTps3b55MJpMGDhxYsQWiSnH0eDl58qRGjRql8PBwWa1WNW3alP831TKOHjPTpk1Ts2bN5OHhocjISI0ZM0Z5eXmVVC2cbeXKlbrhhhsUEREhk8mkb7/99oLvWb58uS677DJZrVY1btxYM2fOrPA6HUWQqmRffvmlxo4dqwkTJmjTpk1q166d+vTpo+Tk5LP2X716te666y6NGDFCmzdv1sCBAzVw4EDt2LGjkiuHszh6zCxfvlx33XWXli1bpjVr1igyMlLXXXedjh07VsmVw1kcPWZOO3TokJ588kldddVVlVQpqgJHj5eCggJde+21OnTokL766ivt3btXH3/8serWrVvJlcNZHD1m5s6dq2eeeUYTJkzQ7t279cknn+jLL7/Us88+W8mVw1mys7PVrl07vf/++xfV/+DBgxowYICuueYabdmyRY8//rjuu+8+/fzzzxVcqYMMVKrLL7/cGDVqlH25uLjYiIiIMCZPnnzW/rfffrsxYMCAUm1dunQxHnzwwQqtE1WHo8fM3xUVFRk+Pj7GrFmzKqpEVDFlOWaKioqMK664wvjPf/5jDB8+3LjpppsqoVJUBY4eLx9++KHRsGFDo6CgoLJKRBXj6DEzatQoo2fPnqXaxo4da1x55ZUVWieqJknGN998c94+Tz/9tNGqVatSbXfccYfRp0+fCqzMcZyRqkQFBQXauHGjevfubW8zm83q3bu31qxZc9b3rFmzplR/SerTp885+6NmKcsx83c5OTkqLCxUQEBARZWJKqSsx8ykSZMUEhKiESNGVEaZqCLKcrx8//33iomJ0ahRoxQaGqrWrVvrlVdeUXFxcWWVDScqyzFzxRVXaOPGjfbL/w4cOKCFCxeqf//+lVIzqp/q8vuvi7MLqE1SU1NVXFys0NDQUu2hoaHas2fPWd+TmJh41v6JiYkVVieqjrIcM383btw4RUREnPEDCTVTWY6Z33//XZ988om2bNlSCRWiKinL8XLgwAEtXbpUgwcP1sKFC7V//349/PDDKiws1IQJEyqjbDhRWY6Zu+++W6mpqerWrZsMw1BRUZEeeughLu3DOZ3r99+MjAzl5ubKw8PDSZWVxhkpoAZ79dVXNW/ePH3zzTdyd3d3djmogjIzMzV06FB9/PHHCgoKcnY5qAZsNptCQkL073//Wx07dtQdd9yh5557TtOnT3d2aaiili9frldeeUUffPCBNm3apK+//lo//vijXnrpJWeXBlwSzkhVoqCgIFksFiUlJZVqT0pKUlhY2FnfExYW5lB/1CxlOWZOe+ONN/Tqq6/q119/Vdu2bSuyTFQhjh4zsbGxOnTokG644QZ7m81mkyS5uLho7969atSoUcUWDacpy8+Y8PBwubq6ymKx2NtatGihxMREFRQUyM3NrUJrhnOV5Zj55z//qaFDh+q+++6TJLVp00bZ2dl64IEH9Nxzz8ls5u/6KO1cv//6+vpWmbNREmekKpWbm5s6duyoJUuW2NtsNpuWLFmimJiYs74nJiamVH9JWrx48Tn7o2YpyzEjSVOmTNFLL72kn376SZ06daqMUlFFOHrMNG/eXNu3b9eWLVvsrxtvvNE+U1JkZGRllo9KVpafMVdeeaX2799vD9yS9Oeffyo8PJwQVQuU5ZjJyck5IyydDuKGYVRcsai2qs3vv86e7aK2mTdvnmG1Wo2ZM2cau3btMh544AHD39/fSExMNAzDMIYOHWo888wz9v6rVq0yXFxcjDfeeMPYvXu3MWHCBMPV1dXYvn27s3YBlczRY+bVV1813NzcjK+++spISEiwvzIzM521C6hkjh4zf8esfbWLo8dLXFyc4ePjY4wePdrYu3evsWDBAiMkJMT417/+5axdQCVz9JiZMGGC4ePjY3zxxRfGgQMHjF9++cVo1KiRcfvttztrF1DJMjMzjc2bNxubN282JBlTp041Nm/ebBw+fNgwDMN45plnjKFDh9r7HzhwwPD09DSeeuopY/fu3cb7779vWCwW46effnLWLpwVQcoJ3n33XSMqKspwc3MzLr/8cuOPP/6wr+vevbsxfPjwUv3/+9//Gk2bNjXc3NyMVq1aGT/++GMlVwxnc+SYiY6ONiSd8ZowYULlFw6ncfTnzF8RpGofR4+X1atXG126dDGsVqvRsGFD4+WXXzaKiooquWo4kyPHTGFhofHiiy8ajRo1Mtzd3Y3IyEjj4YcfNk6cOFH5hcMpli1bdtbfTU4fJ8OHDze6d+9+xnvat29vuLm5GQ0bNjRmzJhR6XVfiMkwOKcKAAAAAI7gHikAAAAAcBBBCgAAAAAcRJACAAAAAAcRpAAAAADAQQQpAAAAAHAQQQoAAAAAHESQAgAAAAAHEaQAAAAAwEEEKQBAlXHPPfdo4MCBzi4DAIALIkgBAAAAgIMIUgAAAADgIIIUAKBc2Ww2TZkyRY0bN5bValVUVJRefvllSdL27dvVs2dPeXh4KDAwUA888ICysrLOOVb9+vU1bdq0Um3t27fXiy++aF82mUz66KOPdP3118vT01MtWrTQmjVrtH//fvXo0UNeXl664oorFBsba3/Piy++qPbt22v27NmqX7++/Pz8dOeddyozM/Oi9rFHjx569NFH9fTTTysgIEBhYWGlapKkqVOnqk2bNvLy8lJkZKQefvjhUvs6c+ZM+fv7a8GCBWrWrJk8PT116623KicnR7NmzVL9+vVVp04dPfrooyouLra/Lz8/X08++aTq1q0rLy8vdenSRcuXL7+ougEA5YcgBQAoV+PHj9err76qf/7zn9q1a5fmzp2r0NBQZWdnq0+fPqpTp47Wr1+v+fPn69dff9Xo0aMveZsvvfSShg0bpi1btqh58+a6++679eCDD2r8+PHasGGDDMM4YzuxsbH69ttvtWDBAi1YsEArVqzQq6++etHbnDVrlry8vLR27VpNmTJFkyZN0uLFi+3rzWaz3nnnHe3cuVOzZs3S0qVL9fTTT5caIycnR++8847mzZunn376ScuXL9fNN9+shQsXauHChZo9e7Y++ugjffXVV/b3jB49WmvWrNG8efO0bds23Xbbberbt6/27dtXxk8PAFAmBgAA5SQjI8OwWq3Gxx9/fMa6f//730adOnWMrKwse9uPP/5omM1mIzEx0TAMwxg+fLhx00032ddHR0cbb731Vqlx2rVrZ0yYMMG+LMl4/vnn7ctr1qwxJBmffPKJve2LL74w3N3d7csTJkwwPD09jYyMDHvbU089ZXTp0uWi9rN79+5Gt27dSrV17tzZGDdu3DnfM3/+fCMwMNC+PGPGDEOSsX//fnvbgw8+aHh6ehqZmZn2tj59+hgPPvigYRiGcfjwYcNisRjHjh0rNXavXr2M8ePHX1TtAIDy4eLkHAcAqEF2796t/Px89erV66zr2rVrJy8vL3vblVdeKZvNpr179yo0NLTM223btq39+9PjtGnTplRbXl6eMjIy5OvrK6nkskEfHx97n/DwcCUnJ5dpm2d7/6+//qrJkydrz549ysjIUFFRkfLy8pSTkyNPT09Jkqenpxo1alSqzvr168vb27tU2+lxt2/fruLiYjVt2rTUtvPz8xUYGHjRtQMALh1BCgBQbjw8PMp1PLPZLMMwSrUVFhae0c/V1dX+vclkOmebzWY763tO9/nr+gs53/sPHTqk66+/XiNHjtTLL7+sgIAA/f777xoxYoQKCgrsQepsY5xv3KysLFksFm3cuFEWi6VUv7+GLwBAxeMeKQBAuWnSpIk8PDy0ZMmSM9a1aNFCW7duVXZ2tr1t1apVMpvNatas2VnHCw4OVkJCgn05IyNDBw8eLP/Cy9nGjRtls9n05ptvqmvXrmratKni4+MvedwOHTqouLhYycnJaty4calXWFhYOVQOALhYBCkAQLlxd3fXuHHj9PTTT+uzzz5TbGys/vjjD33yyScaPHiw3N3dNXz4cO3YsUPLli3TI488oqFDh57zsr6ePXtq9uzZ+u2337R9+3YNHz78jDMxVVHjxo1VWFiod999VwcOHNDs2bM1ffr0Sx63adOmGjx4sIYNG6avv/5aBw8e1Lp16zR58mT9+OOP5VA5AOBiEaQAAOXqn//8p5544gm98MILatGihe644w4lJyfL09NTP//8s9LS0tS5c2fdeuut6tWrl957771zjjV+/Hh1795d119/vQYMGKCBAweWuqeoqmrXrp2mTp2q1157Ta1bt9acOXM0efLkchl7xowZGjZsmJ544gk1a9ZMAwcO1Pr16xUVFVUu4wMALo7J+PvF5wAAAACA8+KMFAAAAAA4iCAFAMBfxMXFydvb+5yvuLg4Z5cIAKgCuLQPAIC/KCoq0qFDh865vn79+nJx4ekhAFDbEaQAAAAAwEFc2gcAAAAADiJIAQAAAICDCFIAAAAA4CCCFAAAAAA4iCAFAAAAAA4iSAEAAACAgwhSAAAAAOCg/wezHdm55w8BFwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 假設你要繪製 df['column_name'] 的分布\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(full_table.drop(columns=\"sales\")[\"medianRevenue\"], kde=True)  # kde=True 可以添加密度估計曲線\n",
    "plt.title('Distribution of column_name')\n",
    "plt.xlabel('column_name')\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(0, 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>averageRevenue</th>\n",
       "      <th>medianRevenue</th>\n",
       "      <th>publishedGames</th>\n",
       "      <th>price</th>\n",
       "      <th>earlyAccess</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Casual</th>\n",
       "      <th>Early Access</th>\n",
       "      <th>...</th>\n",
       "      <th>Steam Cloud</th>\n",
       "      <th>Steam Leaderboards</th>\n",
       "      <th>Steam Trading Cards</th>\n",
       "      <th>Steam Turn Notifications</th>\n",
       "      <th>Steam Workshop</th>\n",
       "      <th>SteamVR Collectibles</th>\n",
       "      <th>Tracked Controller Support</th>\n",
       "      <th>VR Only</th>\n",
       "      <th>VR Supported</th>\n",
       "      <th>Valve Anti-Cheat enabled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011630</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.696133</td>\n",
       "      <td>0.074884</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.949297</td>\n",
       "      <td>0.011630</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.696133</td>\n",
       "      <td>0.074884</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.098669</td>\n",
       "      <td>0.100861</td>\n",
       "      <td>0.093901</td>\n",
       "      <td>0.038674</td>\n",
       "      <td>0.249906</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.910367</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.499937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.895628</td>\n",
       "      <td>0.027177</td>\n",
       "      <td>0.010831</td>\n",
       "      <td>0.187845</td>\n",
       "      <td>0.249906</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>13.865232</td>\n",
       "      <td>0.036138</td>\n",
       "      <td>0.025433</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.099887</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>8.429891</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.270718</td>\n",
       "      <td>0.749969</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>11.482043</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.265193</td>\n",
       "      <td>0.249906</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>7.859799</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.270718</td>\n",
       "      <td>0.499937</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>7.204893</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.480663</td>\n",
       "      <td>0.062383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1244 rows × 466 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sales  averageRevenue  medianRevenue  publishedGames     price  \\\n",
       "0      0.000000        0.011630       0.002322        0.696133  0.074884   \n",
       "1     10.949297        0.011630       0.002322        0.696133  0.074884   \n",
       "2     12.098669        0.100861       0.093901        0.038674  0.249906   \n",
       "3     13.910367        0.054163       0.009294        0.055249  0.499937   \n",
       "4     10.895628        0.027177       0.010831        0.187845  0.249906   \n",
       "...         ...             ...            ...             ...       ...   \n",
       "1239  13.865232        0.036138       0.025433        0.033149  0.099887   \n",
       "1240   8.429891        0.004767       0.001744        0.270718  0.749969   \n",
       "1241  11.482043        0.014553       0.003005        0.265193  0.249906   \n",
       "1242   7.859799        0.004767       0.001744        0.270718  0.499937   \n",
       "1243   7.204893        0.004249       0.001196        0.480663  0.062383   \n",
       "\n",
       "      earlyAccess  Action  Adventure  Casual  Early Access  ...  Steam Cloud  \\\n",
       "0               0       1          0       0             0  ...            0   \n",
       "1               0       1          0       0             0  ...            0   \n",
       "2               0       1          1       0             0  ...            1   \n",
       "3               0       0          0       0             0  ...            1   \n",
       "4               0       1          1       0             0  ...            0   \n",
       "...           ...     ...        ...     ...           ...  ...          ...   \n",
       "1239            0       1          1       0             0  ...            1   \n",
       "1240            0       1          1       0             0  ...            1   \n",
       "1241            0       1          0       0             0  ...            1   \n",
       "1242            0       1          1       0             0  ...            1   \n",
       "1243            0       0          1       1             0  ...            1   \n",
       "\n",
       "      Steam Leaderboards  Steam Trading Cards  Steam Turn Notifications  \\\n",
       "0                      0                    0                         0   \n",
       "1                      0                    0                         0   \n",
       "2                      1                    1                         0   \n",
       "3                      0                    1                         0   \n",
       "4                      0                    1                         0   \n",
       "...                  ...                  ...                       ...   \n",
       "1239                   0                    0                         0   \n",
       "1240                   0                    1                         0   \n",
       "1241                   0                    1                         0   \n",
       "1242                   0                    1                         0   \n",
       "1243                   0                    1                         0   \n",
       "\n",
       "      Steam Workshop  SteamVR Collectibles  Tracked Controller Support  \\\n",
       "0                  0                     0                           0   \n",
       "1                  0                     0                           0   \n",
       "2                  0                     0                           0   \n",
       "3                  1                     0                           0   \n",
       "4                  0                     0                           0   \n",
       "...              ...                   ...                         ...   \n",
       "1239               0                     0                           0   \n",
       "1240               0                     0                           0   \n",
       "1241               0                     0                           0   \n",
       "1242               0                     0                           0   \n",
       "1243               0                     0                           0   \n",
       "\n",
       "      VR Only  VR Supported  Valve Anti-Cheat enabled  \n",
       "0           0             0                         0  \n",
       "1           0             0                         0  \n",
       "2           0             0                         0  \n",
       "3           0             0                         0  \n",
       "4           0             0                         0  \n",
       "...       ...           ...                       ...  \n",
       "1239        0             0                         0  \n",
       "1240        0             0                         0  \n",
       "1241        0             0                         0  \n",
       "1242        0             0                         0  \n",
       "1243        0             0                         0  \n",
       "\n",
       "[1244 rows x 466 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev310-mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
